{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Code to Extract Data From Template and Transfer to PostGRE SQL\n",
    "#### Authors : Aaron Liu, Rahul Venkatesh, Jessica Bonsu, Myeongyeon Lee \n",
    "##### Date Edited : 06-07-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Required Packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2 as pg\n",
    "\n",
    "import os\n",
    "from psycopg2.extras import Json\n",
    "from psycopg2.extensions import AsIs\n",
    "import functools\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import requests\n",
    "# import bibtexparser\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Required Functions To Extract Information from Template\n",
    "\n",
    "# Function to remove rows that have no value (NaN) in the second column\n",
    "def remove_emptyrows(df):\n",
    "    nan_mask = ~df.iloc[:,1].isna() \n",
    "    return df[nan_mask]\n",
    "\n",
    "# Function to convert a sheet into dictionary data type\n",
    "def read_sheet(filepath, sheet_name, ordering=False, usecols=\"A,B,D\", meas=False):\n",
    "\n",
    "    ## NOTE: ADD AN ARGUMENT TO DECIDE WHETHER OR NOT TO BRACKET THE SHEET\n",
    "    ## NOTE : The argument \"ordering\" is used for sheets like solution processing or substrate pretreatmant where the order of the processing step matters\n",
    "    ## NOTE : The argument \"usecols\" is to store information from particular columns in the excel sheet\n",
    "    ## NOTE : The argument \"meas\" is used to \n",
    "    \n",
    "    ## Read Sheet Information\n",
    "    df = pd.read_excel(\n",
    "        filepath,\n",
    "        sheet_name=sheet_name,\n",
    "        usecols=usecols\n",
    "    )\n",
    "    \n",
    "    # Call Function To Remove empty rows\n",
    "    df_ = remove_emptyrows(df)\n",
    "    \n",
    "    # Create an empty dictionary\n",
    "    sheet_dict = dict()\n",
    "\n",
    "    # To account for sheets where processing order is important\n",
    "    if ordering==True:\n",
    "        df_list = split_df(df_) #calls function split_df\n",
    "        for i, df in enumerate(df_list):\n",
    "            sheet_dict[i] = table_to_dict(df) #adds each table to the dictionary\n",
    "    else:\n",
    "        sheet_dict = table_to_dict(df_)\n",
    "    \n",
    "    return sheet_dict #returns a dataframe\n",
    "\n",
    "def split_df(df_):\n",
    "    #For sheets where processing order is important, this function finds tables with '#' in the name of the first column title and turns it into a df\n",
    "    \n",
    "    split_idx_mask = df_.iloc[:,0].str.contains('#') #Find the object splits\n",
    "    w = df_[split_idx_mask].index.values\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "    for i in range(len(w)-1):\n",
    "        next_df = df_.loc[w[i]+1:w[i+1]-1,:]\n",
    "        df_list.append(next_df)    \n",
    "    \n",
    "    return df_list\n",
    "\n",
    "def table_to_dict(df_):\n",
    "    \n",
    "    main_mask = pd.isna(df_.JSON) # it flags rows that dont have a value for JSON column\n",
    "    step_dict = dict(df_[main_mask].iloc[:,:2].values) # Stores rows that have \"NaN\" for JSON column in df_ as dict\n",
    "\n",
    "    \n",
    "    \n",
    "    for json_field in pd.unique(df_.JSON): #read through unique JSON types (e.g. NaN, meta or data)\n",
    "\n",
    "        if pd.isna(json_field): #ignore fields with JSON type as NaN\n",
    "            continue\n",
    "            \n",
    "        # dictionary to store information with JSON type \"data\"\n",
    "        elif json_field=='data':\n",
    "            data_mask = df_.JSON=='data'\n",
    "            \n",
    "            # lump key:value pairs into a second nested data dict\n",
    "            step_dict['data'] = dict()\n",
    "            \n",
    "            for i, s in df_[data_mask].iterrows():\n",
    "                step_dict['data'][s[s.index[0]]] = s['value':'error_type'].dropna().to_dict()\n",
    "        else:\n",
    "            json_mask = df_.JSON==json_field\n",
    "            step_dict[json_field] = dict(df_[json_mask].iloc[:,:2].values) # creates a new key for JSON types like meta and params and adds its corresponding values to it \n",
    "\n",
    "    return step_dict\n",
    "\n",
    "# f = pd.ExcelFile(fpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Extracting Data From Sheets in Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\Anaconda3\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:296: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#Reading Data From Sheets in Template\n",
    "\n",
    "#fpath = r'..\\db_feed\\v6_example_1_real.xlsx' #Add path for template file\n",
    "fpath = r'..\\db_feed\\v6_example.xlsx' #Add path for template file\n",
    "\n",
    "#Storing each sheet in the template file as a dictionary\n",
    "exp_info = read_sheet(fpath, 'Data Origin')\n",
    "solution_makeup = read_sheet(fpath, 'Solution Makeup', ordering=True)\n",
    "solution_processing = read_sheet(fpath, 'Solution Treatment', ordering=True)\n",
    "device_fab = read_sheet(fpath, 'Device Fabrication')\n",
    "substrate_pretreat = read_sheet(fpath, 'Substrate Pretreat', ordering=True)\n",
    "coating_process = read_sheet(fpath, 'Coating Process')\n",
    "post_process = read_sheet(fpath, 'Post-Processing', ordering=True)\n",
    "device_meas = read_sheet(fpath, 'Device Measurement', usecols=\"A:G\", ordering=True)\n",
    "other_meas = read_sheet(fpath, 'Other Measurements', usecols=\"A:G\", ordering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this code block to check how each sheet has been converted to a dictionary\n",
    "substrate_pretreat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transferring Information From Template To PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postgres python\n",
    "from psycopg2.extras import Json \n",
    "\n",
    "# Adapters necessary for converting python data types to PostgreSQL compatible data types \n",
    "def addapt_numpy_float64(numpy_float64):\n",
    "    return AsIs(numpy_float64)\n",
    "\n",
    "def addapt_numpy_int64(numpy_int64):\n",
    "    return AsIs(numpy_int64)\n",
    "\n",
    "def nan_to_null(f,\n",
    "        _NULL=AsIs('NULL'),\n",
    "        _Float=pg.extensions.Float):\n",
    "    if not np.isnan(f):\n",
    "        return _Float(f)\n",
    "    return _NULL\n",
    "\n",
    "pg.extensions.register_adapter(np.float64, addapt_numpy_float64)\n",
    "pg.extensions.register_adapter(np.int64, addapt_numpy_int64)\n",
    "pg.extensions.register_adapter(float, nan_to_null)\n",
    "\n",
    "param_dict = {\n",
    "    \"host\"      : \"127.0.0.1\",\n",
    "    \"database\"  : \"ofetdb_testenv_RV\",\n",
    "    \"user\"      : \"postgres\",\n",
    "    \"password\"  : \"Rahul2411!\",\n",
    "    \"port\"      : \"5432\",\n",
    "}\n",
    "\n",
    "def connect(params_dict):\n",
    "    \"\"\" Connect to the PostgreSQL database server \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        # connect to the PostgreSQL server\n",
    "        print('Connecting to the PostgreSQL database...')\n",
    "        conn = pg.connect(**params_dict)\n",
    "    except (Exception, pg.DatabaseError) as error:\n",
    "        print(error)\n",
    "        sys.exit(1) \n",
    "    print(\"Connection successful\")\n",
    "    return conn\n",
    "\n",
    "def pg_query(sql, tup):\n",
    "    \n",
    "    try:\n",
    "        # Database connection\n",
    "        conn = connect(param_dict)\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Pass SQL query, using string and placeholders\n",
    "        cur.execute(sql, tup)\n",
    "        \n",
    "        # Fetch result\n",
    "        fetched = cur.fetchone()[0]\n",
    "        \n",
    "        # Commit result\n",
    "        conn.commit()\n",
    "        print(\"Operation Successful\")\n",
    "\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "    except (Exception, pg.DatabaseError) as error:\n",
    "        # If database connection unsuccessful, then close connection \n",
    "        print(\"Error: %s\" % error)\n",
    "        conn.rollback()\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    \n",
    "    return fetched #return query result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2.extras import Json\n",
    "\n",
    "def convert_entry(entry_dict):\n",
    "    \n",
    "    #This function reads a dictionary and extracts the column names and values from it\n",
    "    \n",
    "    pg_entry = entry_dict\n",
    "    for key in pg_entry.keys():\n",
    "        if type(pg_entry[key])==dict:\n",
    "            pg_entry[key]=Json(pg_entry[key])\n",
    "    columns = pg_entry.keys()\n",
    "    values = [pg_entry[column] for column in columns]\n",
    "    \n",
    "    return pg_entry, columns, values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Doubt 1 : \n",
    "\n",
    "I made a new database. we were not able to add any new records to the old database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Checking and Storing Experiment Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import _json\n",
    "\n",
    "exp_pg_entry, exp_columns, exp_values = convert_entry(exp_info)\n",
    "\n",
    "#print(type(pg_entry))\n",
    "#print(type(columns))\n",
    "#print(columns)\n",
    "#print(type(values))\n",
    "#print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "    INSERT INTO experiment_info (%s) \n",
    "    VALUES %s\n",
    "    ON CONFLICT (citation_type, meta) DO UPDATE\n",
    "    SET (%s) = %s\n",
    "    RETURNING exp_id\n",
    "    \n",
    "    '''\n",
    "tup = (AsIs(','.join(exp_columns)), tuple(exp_values), AsIs(','.join(exp_columns)), tuple(exp_values))\n",
    "\n",
    "\n",
    "\n",
    "exp_id = pg_query(sql, tup)\n",
    "exp_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Dont forget to assign the exp_id to sample table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Checking and Storing Solution Information (Polymer, Solvent, Solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import _json\n",
    "\n",
    "pg_entry_solution_makeup, columns_solution_makeup, values_solution_makeup = convert_entry(solution_makeup)\n",
    "\n",
    "#print(values_solution_makeup)\n",
    "#print(type(values_solution_makeup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Solution data\n",
    "\n",
    "solution_data = values_solution_makeup[0].adapted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Solvent data - accounting for multiple solvents\n",
    "solvent_data_filtered = [json_obj for json_obj in values_solution_makeup if json_obj.adapted.get(\"entity_type\") == \"solvent\"]\n",
    "\n",
    "# Convert psycopg2._json.Json objects to JSON strings\n",
    "solvent_data = [json_obj.adapted for json_obj in solvent_data_filtered]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Polymer data - accounting for multiple polymers\n",
    "polymer_data_filtered = [json_obj for json_obj in values_solution_makeup if json_obj.adapted.get(\"entity_type\") == \"polymer\"]\n",
    "\n",
    "# Convert psycopg2._json.Json objects to JSON strings\n",
    "polymer_data = [json_obj.adapted for json_obj in polymer_data_filtered]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.1 Storing Solvent Information in SOLVENT table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting only the required information for the SOLVENT table\n",
    "\n",
    "SOLVENT_list = []\n",
    "for d in solvent_data:\n",
    "    SOLVENT_dict = {}\n",
    "    if 'pubchem_cid' in d:\n",
    "        SOLVENT_dict['pubchem_cid'] = d['pubchem_cid']\n",
    "    if 'iupac_name' in d:\n",
    "        SOLVENT_dict['iupac_name'] = d['iupac_name']\n",
    "    if 'meta' in d:\n",
    "        SOLVENT_dict['meta'] = d['meta']\n",
    "    SOLVENT_list.append(SOLVENT_dict)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting key and values information for the POLYMER table and inserting into it\n",
    "\n",
    "for i in SOLVENT_list:\n",
    "    pg_entry_solvent, solvent_columns, solvent_values = convert_entry(i)\n",
    "    \n",
    "    sql = '''\n",
    "    INSERT INTO SOLVENT (%s) \n",
    "    VALUES %s\n",
    "    ON CONFLICT (iupac_name, meta) DO UPDATE\n",
    "    SET (%s) = %s\n",
    "    RETURNING pubchem_cid\n",
    "    \n",
    "    '''\n",
    "\n",
    "    tup = (AsIs(','.join(solvent_columns)), tuple(solvent_values), AsIs(','.join(solvent_columns)), tuple(solvent_values))\n",
    "\n",
    "\n",
    "    pubchem_cid = pg_query(sql, tup)\n",
    "    print(pubchem_cid)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DOUBT 2 \n",
    "\n",
    "Currently the SOLVENT table has a UNIQUE value assigned to (iupac,meta). Which means there can only be one chloroform. But what if we have two chlorforms from different vendors having diff meta information. Shouldnt we store both and assign each one an ID?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.2 Storing polymer Information in POLYMER table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting only the required information for the POLYMER table\n",
    "\n",
    "POLYMER_list = []\n",
    "for d in polymer_data:\n",
    "    POLYMER_dict = {}\n",
    "    if 'common_name' in d:\n",
    "        POLYMER_dict['common_name'] = d['common_name']\n",
    "    if 'iupac_name' in d:\n",
    "        POLYMER_dict['iupac_name'] = d['iupac_name']\n",
    "    if 'mw' in d:\n",
    "        POLYMER_dict['mw'] = d['mw']\n",
    "    if 'mn' in d:\n",
    "        POLYMER_dict['mn'] = d['mn']\n",
    "    if 'dispersity' in d:\n",
    "        POLYMER_dict['dispersity'] = d['dispersity']\n",
    "    if 'meta' in d:\n",
    "        POLYMER_dict['meta'] = d['meta']\n",
    "    POLYMER_list.append(POLYMER_dict)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting key and values information for the POLYMER table and inserting into it\n",
    "\n",
    "for i in POLYMER_list:\n",
    "    pg_entry_polymer, polymer_columns, polymer_values = convert_entry(i)\n",
    "    \n",
    "    sql = '''\n",
    "        INSERT INTO POLYMER (%s) VALUES %s\n",
    "        ON CONFLICT(common_name,iupac_name,Mn,Mw,dispersity,meta) DO UPDATE\n",
    "        SET (%s) = %s\n",
    "            RETURNING polymer_id\n",
    "    '''\n",
    "\n",
    "    tup = (AsIs(','.join(polymer_columns)), tuple(polymer_values), AsIs(','.join(polymer_columns)), tuple(polymer_values))\n",
    "\n",
    "\n",
    "    polymer_id = pg_query(sql, tup)\n",
    "    print(polymer_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.3 Storing solution Information in SOLUTION table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting only the required information for the SOLUTION table\n",
    "\n",
    "solution_desired_keys = ['concentration']\n",
    "\n",
    "SOLUTION_data = {key: solution_data[key] for key in solution_desired_keys if key in solution_data}\n",
    "\n",
    "print(SOLUTION_data)\n",
    "print(type(SOLUTION_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting column and values information for the SOLUTION table\n",
    "\n",
    "pg_entry_solution, solution_columns, solution_values = convert_entry(SOLUTION_data)\n",
    "\n",
    "print(solution_columns)\n",
    "print(type(solution_columns))\n",
    "print(solution_values)\n",
    "print(type(solution_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert into SOLUTION table\n",
    "\n",
    "## DOUBT : currently no constraints applied to SQL statement. Which ones to apply\n",
    "\n",
    "sql = '''\n",
    "INSERT INTO SOLUTION (%s) VALUES %s\n",
    "RETURNING solution_id\n",
    "'''\n",
    "\n",
    "tup = (AsIs(','.join(solution_columns)), tuple(solution_values))\n",
    "\n",
    "\n",
    "solution_id = pg_query(sql, tup)\n",
    "solution_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.4 Storing Solvent Information in SOLUTION_MAKEUP_SOLVENT table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the volume fraction information for the SOLUTION_MAKEUP_SOLVENT table\n",
    "\n",
    "SOLUTION_MAKEUP_SOLVENT_list = []\n",
    "for d in solvent_data:\n",
    "    SOLUTION_MAKEUP_SOLVENT_dict = {}\n",
    "    if 'vol_frac' in d:\n",
    "        SOLUTION_MAKEUP_SOLVENT_dict['vol_frac'] = d['vol_frac']\n",
    "    SOLUTION_MAKEUP_SOLVENT_list.append(SOLUTION_MAKEUP_SOLVENT_dict)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting key and values information for the SOLUTION_MAKEUP_SOLVENT table and inserting into it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.5 Storing polymer Information in SOLUTION_MAKEUP_POLYMER table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the volume fraction information for the SOLUTION_MAKEUP_POLYMER table\n",
    "\n",
    "SOLUTION_MAKEUP_POLYMER_list = []\n",
    "for d in polymer_data:\n",
    "    SOLUTION_MAKEUP_POLYMER_dict = {}\n",
    "    if 'wt_frac' in d:\n",
    "        SOLUTION_MAKEUP_POLYMER_dict['wt_frac'] = d['wt_frac']\n",
    "    SOLUTION_MAKEUP_POLYMER_list.append(SOLUTION_MAKEUP_POLYMER_dict)\n",
    "\n",
    "#SOLUTION_MAKEUP_POLYMER_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Checking and Storing Device Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import _json\n",
    "\n",
    "device_fab_pg_entry, device_fab_columns, device_fab_values = convert_entry(device_fab)\n",
    "\n",
    "#print(type(device_fab_pg_entry))\n",
    "#print(type(device_fab_columns))\n",
    "print(device_fab_columns)\n",
    "print(type(device_fab_values))\n",
    "print(device_fab_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "    INSERT INTO DEVICE_FABRICATION (%s) \n",
    "    VALUES %s\n",
    "    ON CONFLICT (params, meta) DO UPDATE\n",
    "    SET (%s) = %s\n",
    "    RETURNING device_fab_id\n",
    "    \n",
    "    '''\n",
    "tup = (AsIs(','.join(device_fab_columns)), tuple(device_fab_values), AsIs(','.join(device_fab_columns)), tuple(device_fab_values))\n",
    "\n",
    "\n",
    "\n",
    "device_fab_id = pg_query(sql, tup)\n",
    "device_fab_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Checking and Storing Film Deposition Information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import _json\n",
    "\n",
    "coating_process_pg_entry, coating_process_columns, coating_process_values = convert_entry(coating_process)\n",
    "\n",
    "#print(type(coating_process_pg_entry))\n",
    "#print(type(coating_process_columns))\n",
    "print(coating_process_columns)\n",
    "print(type(coating_process_values))\n",
    "print(coating_process_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Doubt : In the create OFET DB code why is film_deposition_id in unique?\n",
    "\n",
    "sql = '''\n",
    "    INSERT INTO FILM_DEPOSITION (%s) \n",
    "    VALUES %s\n",
    "    ON CONFLICT (film_deposition_id,deposition_type, params, meta) DO UPDATE\n",
    "    SET (%s) = %s\n",
    "    RETURNING film_deposition_id\n",
    "    \n",
    "    '''\n",
    "tup = (AsIs(','.join(coating_process_columns)), tuple(coating_process_values), AsIs(','.join(coating_process_columns)), tuple(coating_process_values))\n",
    "\n",
    "\n",
    "\n",
    "film_deposition_id = pg_query(sql, tup)\n",
    "film_deposition_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Checking and Storing the subprocess recipes (Solution Treatment, Substrate Pretreatment, Post Process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5.1 SOLUTION TREATMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5.2 SUBSTRATE PRETREATMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'treatment_type': 'chemical_treat',\n",
       "  'process_step': 1,\n",
       "  'params': {'environment': 'air',\n",
       "   'iupac_name': 'methanol',\n",
       "   'temperature': 25,\n",
       "   'time': 15}},\n",
       " 1: {'treatment_type': 'uv_ozone',\n",
       "  'process_step': 2,\n",
       "  'params': {'time': 30},\n",
       "  'meta': {'equipment_model': 'Entela T20'}},\n",
       " 2: {'treatment_type': 'sam',\n",
       "  'process_step': 3,\n",
       "  'params': {'sam_name': 'OTS-8',\n",
       "   'iupac_name': 'octyltrichlorosilane',\n",
       "   'pubchem_cid': 21354}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substrate_pretreat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import json\n",
    "\n",
    "# Establish a connection to the PostgreSQL database\n",
    "conn = pg.connect(**param_dict)\n",
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cur = conn.cursor()\n",
    "\n",
    "\n",
    "# Generate substrate_pretreat_id and insert records into SUBSTRATE_PRETREAT, SUBSTRATE_PRETREAT_STEP, and SUBSTRATE_PRETREAT_ORDER tables\n",
    "for key, value in substrate_pretreat.items():\n",
    "    # Insert into SUBSTRATE_PRETREAT table\n",
    "    insert_pretreat_query = '''\n",
    "        INSERT INTO SUBSTRATE_PRETREAT DEFAULT VALUES\n",
    "        RETURNING substrate_pretreat_id\n",
    "    '''\n",
    "    cur.execute(insert_pretreat_query)\n",
    "    substrate_pretreat_id = cur.fetchone()[0]\n",
    "\n",
    "    # Insert into SUBSTRATE_PRETREAT_STEP table\n",
    "    treatment_type = value['treatment_type']\n",
    "    params = json.dumps(value['params'])\n",
    "    meta = json.dumps(value.get('meta', {}))\n",
    "\n",
    "    insert_step_query = '''\n",
    "        INSERT INTO SUBSTRATE_PRETREAT_STEP (treatment_type, params, meta)\n",
    "        VALUES (%s, %s, %s)\n",
    "        RETURNING substrate_pretreat_step_id\n",
    "    '''\n",
    "    cur.execute(insert_step_query, (treatment_type, params, meta))\n",
    "    substrate_pretreat_step_id = cur.fetchone()[0]\n",
    "\n",
    "    # Insert into SUBSTRATE_PRETREAT_ORDER table\n",
    "    process_order = value['process_step']\n",
    "    insert_order_query = '''\n",
    "        INSERT INTO SUBSTRATE_PRETREAT_ORDER (substrate_pretreat_id, process_order, substrate_pretreat_step_id)\n",
    "        VALUES (%s, %s, %s)\n",
    "    '''\n",
    "    cur.execute(insert_order_query, (substrate_pretreat_id, process_order, substrate_pretreat_step_id))\n",
    "\n",
    "# Commit the changes to the database\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5.3 POST PROCESSING TREATMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Checking and Storing information to the OFET_PROCESS TABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Checking and Storing information to the SAMPLE TABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Checking and Storing the measurement information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f5a8b522aac8123589db0e64c73ca2530e0ffae51a117df4f813e361992c41db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
