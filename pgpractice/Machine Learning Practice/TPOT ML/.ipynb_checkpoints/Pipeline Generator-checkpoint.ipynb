{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\Anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\Aaron\\Anaconda3\\lib\\site-packages\\tpot\\builtins\\__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "\n",
    "#imp links:\n",
    "#https://towardsdatascience.com/tpot-automated-machine-learning-in-python-4c063b3e5de9\n",
    "#http://epistasislab.github.io/tpot/api/\n",
    "#how to use TPOT : https://epistasislab.github.io/tpot/using/#:~:text=%2Dp-,POPULATION_SIZE,time)%20to%20optimize%20the%20pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Import libraries\n",
    "import pylab as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('max_columns', 200)\n",
    "pd.set_option('max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>solution_concentration</th>\n",
       "      <th>polymer_mw</th>\n",
       "      <th>polymer_mn</th>\n",
       "      <th>polymer_dispersity</th>\n",
       "      <th>solution_treatment</th>\n",
       "      <th>substrate_pretreatment</th>\n",
       "      <th>post_process</th>\n",
       "      <th>channel_width</th>\n",
       "      <th>channel_length</th>\n",
       "      <th>film_deposition_type_drop</th>\n",
       "      <th>film_deposition_type_spin</th>\n",
       "      <th>dielectric_material_SiO2</th>\n",
       "      <th>electrode_configuration_BGBC</th>\n",
       "      <th>electrode_configuration_BGTC</th>\n",
       "      <th>film_deposition_type_MGC</th>\n",
       "      <th>solvent_boiling_point</th>\n",
       "      <th>hole_mobility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>11.10</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.004960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0000</td>\n",
       "      <td>13.80</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.009480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0000</td>\n",
       "      <td>23.30</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.005290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.0</td>\n",
       "      <td>52.0000</td>\n",
       "      <td>22.60</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.005540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.002990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.0</td>\n",
       "      <td>270.0000</td>\n",
       "      <td>117.40</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>11.10</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.001490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0000</td>\n",
       "      <td>13.80</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.030500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0000</td>\n",
       "      <td>23.30</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.0</td>\n",
       "      <td>52.0000</td>\n",
       "      <td>22.60</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.098200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.0</td>\n",
       "      <td>270.0000</td>\n",
       "      <td>117.40</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.143500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.009940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>11.10</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0000</td>\n",
       "      <td>13.80</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0000</td>\n",
       "      <td>23.30</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10.0</td>\n",
       "      <td>52.0000</td>\n",
       "      <td>22.60</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.066200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.065200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10.0</td>\n",
       "      <td>270.0000</td>\n",
       "      <td>117.40</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.081300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.005440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.003630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.002960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.018300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.017100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.009930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.007570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.068000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.065000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.007620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.003810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.002550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.137800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.125300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.004230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.007930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.010700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.016500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.008920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.004060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.002140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.002280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.002790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0.000556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0.000413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0.000426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0.000254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0.000309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0.000377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.0000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.035220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.0000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.051820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.0000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.054680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.0000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.103430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.0000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.0000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.020930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.0000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.011230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.4000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10.8000</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>5.0</td>\n",
       "      <td>13.9200</td>\n",
       "      <td>11.60</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>5.0</td>\n",
       "      <td>50.7000</td>\n",
       "      <td>33.80</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.007480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.9800</td>\n",
       "      <td>4.60</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>5.0</td>\n",
       "      <td>24.1200</td>\n",
       "      <td>13.40</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>5.0</td>\n",
       "      <td>56.6200</td>\n",
       "      <td>29.80</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.004190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>5.0</td>\n",
       "      <td>62.0500</td>\n",
       "      <td>36.50</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.009080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.4800</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>5.0</td>\n",
       "      <td>19.4400</td>\n",
       "      <td>16.20</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.007850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.5400</td>\n",
       "      <td>31.10</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.007670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2.0</td>\n",
       "      <td>48.9600</td>\n",
       "      <td>24.00</td>\n",
       "      <td>2.040000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0.023900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2.0</td>\n",
       "      <td>48.9600</td>\n",
       "      <td>24.00</td>\n",
       "      <td>2.040000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.030500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2.0</td>\n",
       "      <td>48.9600</td>\n",
       "      <td>24.00</td>\n",
       "      <td>2.040000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>0.033500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2.0</td>\n",
       "      <td>48.9600</td>\n",
       "      <td>24.00</td>\n",
       "      <td>2.040000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>0.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>15.0</td>\n",
       "      <td>5.1000</td>\n",
       "      <td>5.10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>15.0</td>\n",
       "      <td>11.3000</td>\n",
       "      <td>11.30</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>15.0</td>\n",
       "      <td>18.6000</td>\n",
       "      <td>18.60</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>15.0</td>\n",
       "      <td>6.6080</td>\n",
       "      <td>5.60</td>\n",
       "      <td>1.180000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>15.0</td>\n",
       "      <td>20.4240</td>\n",
       "      <td>13.80</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>15.0</td>\n",
       "      <td>25.6500</td>\n",
       "      <td>19.00</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>213</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>138</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.1</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.1</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.1</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.1</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.1</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.7550</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.5920</td>\n",
       "      <td>5.60</td>\n",
       "      <td>2.070000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2.0</td>\n",
       "      <td>21.3840</td>\n",
       "      <td>10.80</td>\n",
       "      <td>1.980000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.003990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2.0</td>\n",
       "      <td>60.7500</td>\n",
       "      <td>27.00</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.7550</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.0</td>\n",
       "      <td>11.5920</td>\n",
       "      <td>5.60</td>\n",
       "      <td>2.070000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1.0</td>\n",
       "      <td>21.3840</td>\n",
       "      <td>10.80</td>\n",
       "      <td>1.980000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.006870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1.0</td>\n",
       "      <td>60.7500</td>\n",
       "      <td>27.00</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.310000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2.7550</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.5</td>\n",
       "      <td>11.5920</td>\n",
       "      <td>5.60</td>\n",
       "      <td>2.070000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.5</td>\n",
       "      <td>21.3840</td>\n",
       "      <td>10.80</td>\n",
       "      <td>1.980000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.004140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.5</td>\n",
       "      <td>60.7500</td>\n",
       "      <td>27.00</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.036400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>4.0</td>\n",
       "      <td>47.7000</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>4.0</td>\n",
       "      <td>47.7000</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.007190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>4.0</td>\n",
       "      <td>47.7000</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>4.0</td>\n",
       "      <td>47.7000</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>4.0</td>\n",
       "      <td>47.7000</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.030600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>4.0</td>\n",
       "      <td>47.7000</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.036100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>4.0</td>\n",
       "      <td>47.7000</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.015300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>3.5</td>\n",
       "      <td>47.7000</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>3.5</td>\n",
       "      <td>47.7000</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.049400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>3.5</td>\n",
       "      <td>47.7000</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>3.5</td>\n",
       "      <td>47.7000</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.003450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>3.5</td>\n",
       "      <td>23.0000</td>\n",
       "      <td>12.70</td>\n",
       "      <td>1.811024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>3.5</td>\n",
       "      <td>23.0000</td>\n",
       "      <td>12.70</td>\n",
       "      <td>1.811024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.043600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>3.5</td>\n",
       "      <td>23.0000</td>\n",
       "      <td>12.70</td>\n",
       "      <td>1.811024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>3.5</td>\n",
       "      <td>23.0000</td>\n",
       "      <td>12.70</td>\n",
       "      <td>1.811024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.014800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>3.5</td>\n",
       "      <td>70.0000</td>\n",
       "      <td>35.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.003680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>3.5</td>\n",
       "      <td>70.0000</td>\n",
       "      <td>35.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.001350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>3.5</td>\n",
       "      <td>70.0000</td>\n",
       "      <td>35.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.000629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>3.5</td>\n",
       "      <td>70.0000</td>\n",
       "      <td>35.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.000871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>3.5</td>\n",
       "      <td>70.0000</td>\n",
       "      <td>35.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.001020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>3.5</td>\n",
       "      <td>163.2000</td>\n",
       "      <td>68.00</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.001020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>3.5</td>\n",
       "      <td>163.2000</td>\n",
       "      <td>68.00</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.003050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>3.5</td>\n",
       "      <td>163.2000</td>\n",
       "      <td>68.00</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.008030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>3.5</td>\n",
       "      <td>163.2000</td>\n",
       "      <td>68.00</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.009530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>3.5</td>\n",
       "      <td>163.2000</td>\n",
       "      <td>68.00</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.009150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.008397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.5</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.002720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.002395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>3.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.004360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>4.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.004459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>10.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.006890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.006050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.012700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>3.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.008115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>5.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.103900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>8.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>1.651400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>10.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.175180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>12.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>15.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>10.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.011071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>10.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.120523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>5.0</td>\n",
       "      <td>51.0000</td>\n",
       "      <td>24.00</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.058170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>5.0</td>\n",
       "      <td>51.0000</td>\n",
       "      <td>24.00</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>5.0</td>\n",
       "      <td>77.5000</td>\n",
       "      <td>26.00</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>5.0</td>\n",
       "      <td>71.0000</td>\n",
       "      <td>32.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>5.0</td>\n",
       "      <td>71.0000</td>\n",
       "      <td>32.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>5.0</td>\n",
       "      <td>71.0000</td>\n",
       "      <td>32.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>5.0</td>\n",
       "      <td>71.0000</td>\n",
       "      <td>32.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.111300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>5.0</td>\n",
       "      <td>71.0000</td>\n",
       "      <td>32.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>5.0</td>\n",
       "      <td>71.0000</td>\n",
       "      <td>32.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.101600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>5.0</td>\n",
       "      <td>71.0000</td>\n",
       "      <td>32.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>5.0</td>\n",
       "      <td>71.0000</td>\n",
       "      <td>32.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.109100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>5.0</td>\n",
       "      <td>71.0000</td>\n",
       "      <td>32.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.089400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>5.0</td>\n",
       "      <td>71.0000</td>\n",
       "      <td>32.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>5.0</td>\n",
       "      <td>104.0000</td>\n",
       "      <td>47.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.138900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>5.0</td>\n",
       "      <td>104.0000</td>\n",
       "      <td>47.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.108900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>5.0</td>\n",
       "      <td>104.0000</td>\n",
       "      <td>47.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.064400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>5.0</td>\n",
       "      <td>104.0000</td>\n",
       "      <td>47.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.048700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>5.0</td>\n",
       "      <td>41.0000</td>\n",
       "      <td>20.50</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.042790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>5.0</td>\n",
       "      <td>41.0000</td>\n",
       "      <td>20.50</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.101700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>5.0</td>\n",
       "      <td>41.0000</td>\n",
       "      <td>20.50</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.083050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>5.0</td>\n",
       "      <td>41.0000</td>\n",
       "      <td>20.50</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.064800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>5.0</td>\n",
       "      <td>66.0000</td>\n",
       "      <td>33.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.090670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>5.0</td>\n",
       "      <td>66.0000</td>\n",
       "      <td>33.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>5.0</td>\n",
       "      <td>66.0000</td>\n",
       "      <td>33.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.104200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>5.0</td>\n",
       "      <td>66.0000</td>\n",
       "      <td>33.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>5.0</td>\n",
       "      <td>129.6000</td>\n",
       "      <td>54.00</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.081000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.101000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.015700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.058400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.065200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>10.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>10.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>10.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.079000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     solution_concentration  polymer_mw  polymer_mn  polymer_dispersity  \\\n",
       "0                      10.0     15.4000       10.30            1.500000   \n",
       "1                      10.0     20.0000       11.10            1.800000   \n",
       "2                      10.0     22.0000       13.80            1.600000   \n",
       "3                      10.0     29.0000       19.30            1.500000   \n",
       "4                      10.0     42.0000       23.30            1.800000   \n",
       "5                      10.0     52.0000       22.60            2.300000   \n",
       "6                      10.0     76.0000       44.70            1.700000   \n",
       "7                      10.0    270.0000      117.40            2.300000   \n",
       "8                      10.0     15.4000       10.30            1.500000   \n",
       "9                      10.0     20.0000       11.10            1.800000   \n",
       "10                     10.0     22.0000       13.80            1.600000   \n",
       "11                     10.0     29.0000       19.30            1.500000   \n",
       "12                     10.0     42.0000       23.30            1.800000   \n",
       "13                     10.0     52.0000       22.60            2.300000   \n",
       "14                     10.0     76.0000       44.70            1.700000   \n",
       "15                     10.0    270.0000      117.40            2.300000   \n",
       "16                     10.0     15.4000       10.30            1.500000   \n",
       "17                     10.0     20.0000       11.10            1.800000   \n",
       "18                     10.0     22.0000       13.80            1.600000   \n",
       "19                     10.0     29.0000       19.30            1.500000   \n",
       "20                     10.0     42.0000       23.30            1.800000   \n",
       "21                     10.0     52.0000       22.60            2.300000   \n",
       "22                     10.0     76.0000       44.70            1.700000   \n",
       "23                     10.0    270.0000      117.40            2.300000   \n",
       "24                     10.0     76.0000       44.70            1.700000   \n",
       "25                     10.0     76.0000       44.70            1.700000   \n",
       "26                     10.0     76.0000       44.70            1.700000   \n",
       "27                     10.0     76.0000       44.70            1.700000   \n",
       "28                     10.0     29.0000       19.30            1.500000   \n",
       "29                     10.0     29.0000       19.30            1.500000   \n",
       "30                     10.0     29.0000       19.30            1.500000   \n",
       "31                     10.0     15.4000       10.30            1.500000   \n",
       "32                     10.0     15.4000       10.30            1.500000   \n",
       "33                     10.0     15.4000       10.30            1.500000   \n",
       "34                     10.0     15.4000       10.30            1.500000   \n",
       "35                     10.0     29.0000       19.30            1.500000   \n",
       "36                     10.0     29.0000       19.30            1.500000   \n",
       "37                     10.0     29.0000       19.30            1.500000   \n",
       "38                     10.0     15.4000       10.30            1.500000   \n",
       "39                     10.0     15.4000       10.30            1.500000   \n",
       "40                     10.0     15.4000       10.30            1.500000   \n",
       "41                     10.0     76.0000       44.70            1.700000   \n",
       "42                     10.0     76.0000       44.70            1.700000   \n",
       "43                     10.0     76.0000       44.70            1.700000   \n",
       "44                      5.0     91.4810       40.30            2.270000   \n",
       "45                      5.0     91.4810       40.30            2.270000   \n",
       "46                      5.0     91.4810       40.30            2.270000   \n",
       "47                      5.0     91.4810       40.30            2.270000   \n",
       "48                      5.0     91.4810       40.30            2.270000   \n",
       "49                      5.0     91.4810       40.30            2.270000   \n",
       "50                      5.0     91.4810       40.30            2.270000   \n",
       "51                      5.0     91.4810       40.30            2.270000   \n",
       "52                      5.0     91.4810       40.30            2.270000   \n",
       "53                      5.0     91.4810       40.30            2.270000   \n",
       "54                      5.0     91.4810       40.30            2.270000   \n",
       "55                      5.0     91.4810       40.30            2.270000   \n",
       "56                      5.0     91.4810       40.30            2.270000   \n",
       "57                      5.0     91.4810       40.30            2.270000   \n",
       "58                      5.0     91.4810       40.30            2.270000   \n",
       "59                      5.0     91.4810       40.30            2.270000   \n",
       "60                      5.0     91.4810       40.30            2.270000   \n",
       "61                      5.0    207.0000       90.00            2.300000   \n",
       "62                      5.0    207.0000       90.00            2.300000   \n",
       "63                      5.0    207.0000       90.00            2.300000   \n",
       "64                      5.0    207.0000       90.00            2.300000   \n",
       "65                      5.0    207.0000       90.00            2.300000   \n",
       "66                      5.0    207.0000       90.00            2.300000   \n",
       "67                      5.0    207.0000       90.00            2.300000   \n",
       "68                      5.0      4.4000        4.00            1.100000   \n",
       "69                      5.0     10.8000        9.00            1.200000   \n",
       "70                      5.0     13.9200       11.60            1.200000   \n",
       "71                      5.0     50.7000       33.80            1.500000   \n",
       "72                      5.0      5.9800        4.60            1.300000   \n",
       "73                      5.0     24.1200       13.40            1.800000   \n",
       "74                      5.0     56.6200       29.80            1.900000   \n",
       "75                      5.0     62.0500       36.50            1.700000   \n",
       "76                      5.0      4.4800        3.20            1.400000   \n",
       "77                      5.0     19.4400       16.20            1.200000   \n",
       "78                      5.0     43.5400       31.10            1.400000   \n",
       "79                      2.0     48.9600       24.00            2.040000   \n",
       "80                      2.0     48.9600       24.00            2.040000   \n",
       "81                      2.0     48.9600       24.00            2.040000   \n",
       "82                      2.0     48.9600       24.00            2.040000   \n",
       "83                     15.0      5.1000        5.10            1.000000   \n",
       "84                     15.0     11.3000       11.30            1.000000   \n",
       "85                     15.0     18.6000       18.60            1.000000   \n",
       "86                     15.0      6.6080        5.60            1.180000   \n",
       "87                     15.0     20.4240       13.80            1.480000   \n",
       "88                     15.0     25.6500       19.00            1.350000   \n",
       "89                      1.0     37.7400       25.50            1.480000   \n",
       "90                      1.0     37.7400       25.50            1.480000   \n",
       "91                      1.0     37.7400       25.50            1.480000   \n",
       "92                      1.0     37.7400       25.50            1.480000   \n",
       "93                      1.0     37.7400       25.50            1.480000   \n",
       "94                      1.0     37.7400       25.50            1.480000   \n",
       "95                      1.0     37.7400       25.50            1.480000   \n",
       "96                      1.0     37.7400       25.50            1.480000   \n",
       "97                      0.1     37.7400       25.50            1.480000   \n",
       "98                      0.1     37.7400       25.50            1.480000   \n",
       "99                      0.1     37.7400       25.50            1.480000   \n",
       "100                     0.1     37.7400       25.50            1.480000   \n",
       "101                     0.1     37.7400       25.50            1.480000   \n",
       "102                     2.0      2.7550        1.90            1.450000   \n",
       "103                     2.0     11.5920        5.60            2.070000   \n",
       "104                     2.0     21.3840       10.80            1.980000   \n",
       "105                     2.0     60.7500       27.00            2.250000   \n",
       "106                     1.0      2.7550        1.90            1.450000   \n",
       "107                     1.0     11.5920        5.60            2.070000   \n",
       "108                     1.0     21.3840       10.80            1.980000   \n",
       "109                     1.0     60.7500       27.00            2.250000   \n",
       "110                     0.5      1.4279        1.09            1.310000   \n",
       "111                     0.5      2.7550        1.90            1.450000   \n",
       "112                     0.5     11.5920        5.60            2.070000   \n",
       "113                     0.5     21.3840       10.80            1.980000   \n",
       "114                     0.5     60.7500       27.00            2.250000   \n",
       "115                     4.0     47.7000       24.00            1.987500   \n",
       "116                     4.0     47.7000       24.00            1.987500   \n",
       "117                     4.0     47.7000       24.00            1.987500   \n",
       "118                     4.0     47.7000       24.00            1.987500   \n",
       "119                     4.0     47.7000       24.00            1.987500   \n",
       "120                     4.0     47.7000       24.00            1.987500   \n",
       "121                     4.0     47.7000       24.00            1.987500   \n",
       "122                     3.5     47.7000       24.00            1.987500   \n",
       "123                     3.5     47.7000       24.00            1.987500   \n",
       "124                     3.5     47.7000       24.00            1.987500   \n",
       "125                     3.5     47.7000       24.00            1.987500   \n",
       "126                     3.5     23.0000       12.70            1.811024   \n",
       "127                     3.5     23.0000       12.70            1.811024   \n",
       "128                     3.5     23.0000       12.70            1.811024   \n",
       "129                     3.5     23.0000       12.70            1.811024   \n",
       "130                     3.5     70.0000       35.00            2.000000   \n",
       "131                     3.5     70.0000       35.00            2.000000   \n",
       "132                     3.5     70.0000       35.00            2.000000   \n",
       "133                     3.5     70.0000       35.00            2.000000   \n",
       "134                     3.5     70.0000       35.00            2.000000   \n",
       "135                     3.5    163.2000       68.00            2.400000   \n",
       "136                     3.5    163.2000       68.00            2.400000   \n",
       "137                     3.5    163.2000       68.00            2.400000   \n",
       "138                     3.5    163.2000       68.00            2.400000   \n",
       "139                     3.5    163.2000       68.00            2.400000   \n",
       "140                     6.0     74.0000       33.63            2.200000   \n",
       "141                     0.5     74.0000       33.63            2.200000   \n",
       "142                     1.0     74.0000       33.63            2.200000   \n",
       "143                     2.0     74.0000       33.63            2.200000   \n",
       "144                     3.0     74.0000       33.63            2.200000   \n",
       "145                     4.0     74.0000       33.63            2.200000   \n",
       "146                    10.0     74.0000       33.63            2.200000   \n",
       "147                     1.0     74.0000       33.63            2.200000   \n",
       "148                     2.0     74.0000       33.63            2.200000   \n",
       "149                     3.0     74.0000       33.63            2.200000   \n",
       "150                     5.0     74.0000       33.63            2.200000   \n",
       "151                     8.0     74.0000       33.63            2.200000   \n",
       "152                    10.0     74.0000       33.63            2.200000   \n",
       "153                    12.0     74.0000       33.63            2.200000   \n",
       "154                    15.0     74.0000       33.63            2.200000   \n",
       "155                    10.0     74.0000       33.63            2.200000   \n",
       "156                    10.0     74.0000       33.63            2.200000   \n",
       "157                     5.0     51.0000       24.00            2.100000   \n",
       "158                     5.0     51.0000       24.00            2.100000   \n",
       "159                     5.0     77.5000       26.00            2.950000   \n",
       "160                     5.0     71.0000       32.27            2.200000   \n",
       "161                     5.0     71.0000       32.27            2.200000   \n",
       "162                     5.0     71.0000       32.27            2.200000   \n",
       "163                     5.0     71.0000       32.27            2.200000   \n",
       "164                     5.0     71.0000       32.27            2.200000   \n",
       "165                     5.0     71.0000       32.27            2.200000   \n",
       "166                     5.0     71.0000       32.27            2.200000   \n",
       "167                     5.0     71.0000       32.27            2.200000   \n",
       "168                     5.0     71.0000       32.27            2.200000   \n",
       "169                     5.0     71.0000       32.27            2.200000   \n",
       "170                     5.0    104.0000       47.27            2.200000   \n",
       "171                     5.0    104.0000       47.27            2.200000   \n",
       "172                     5.0    104.0000       47.27            2.200000   \n",
       "173                     5.0    104.0000       47.27            2.200000   \n",
       "174                     5.0     41.0000       20.50            2.000000   \n",
       "175                     5.0     41.0000       20.50            2.000000   \n",
       "176                     5.0     41.0000       20.50            2.000000   \n",
       "177                     5.0     41.0000       20.50            2.000000   \n",
       "178                     5.0     66.0000       33.00            2.000000   \n",
       "179                     5.0     66.0000       33.00            2.000000   \n",
       "180                     5.0     66.0000       33.00            2.000000   \n",
       "181                     5.0     66.0000       33.00            2.000000   \n",
       "182                     5.0    129.6000       54.00            2.400000   \n",
       "183                     5.0     69.0000       34.60            2.000000   \n",
       "184                     5.0     69.0000       34.60            2.000000   \n",
       "185                     5.0     69.0000       34.60            2.000000   \n",
       "186                     5.0     69.0000       34.60            2.000000   \n",
       "187                     5.0     69.0000       34.60            2.000000   \n",
       "188                     5.0     69.0000       34.60            2.000000   \n",
       "189                     5.0     69.0000       34.60            2.000000   \n",
       "190                     5.0     69.0000       34.60            2.000000   \n",
       "191                     5.0     69.0000       34.60            2.000000   \n",
       "192                     5.0     69.0000       34.60            2.000000   \n",
       "193                     5.0     69.0000       34.60            2.000000   \n",
       "194                     5.0     69.0000       34.60            2.000000   \n",
       "195                     5.0     69.0000       34.60            2.000000   \n",
       "196                     5.0     69.0000       34.60            2.000000   \n",
       "197                     5.0     69.0000       34.60            2.000000   \n",
       "198                     5.0     69.0000       34.60            2.000000   \n",
       "199                     5.0     43.7000       19.70            2.220000   \n",
       "200                     5.0     43.7000       19.70            2.220000   \n",
       "201                     5.0     43.7000       19.70            2.220000   \n",
       "202                     5.0     43.7000       19.70            2.220000   \n",
       "203                     5.0     43.7000       19.70            2.220000   \n",
       "204                     5.0     43.7000       19.70            2.220000   \n",
       "205                     5.0     43.7000       19.70            2.220000   \n",
       "206                     5.0     43.7000       19.70            2.220000   \n",
       "207                     5.0     43.7000       19.70            2.220000   \n",
       "208                     5.0     43.7000       19.70            2.220000   \n",
       "209                     5.0     43.7000       19.70            2.220000   \n",
       "210                     5.0     43.7000       19.70            2.220000   \n",
       "211                     5.0     43.7000       19.70            2.220000   \n",
       "212                     5.0     43.7000       19.70            2.220000   \n",
       "213                     5.0     43.7000       19.70            2.220000   \n",
       "214                     5.0     43.7000       19.70            2.220000   \n",
       "215                     5.0     43.7000       19.70            2.220000   \n",
       "216                     5.0     43.7000       19.70            2.220000   \n",
       "217                     5.0     43.7000       19.70            2.220000   \n",
       "218                     5.0     43.7000       19.70            2.220000   \n",
       "219                     5.0     43.7000       19.70            2.220000   \n",
       "220                     5.0     43.7000       19.70            2.220000   \n",
       "221                    10.0     43.7000       19.70            2.220000   \n",
       "222                    10.0     43.7000       19.70            2.220000   \n",
       "223                    10.0     43.7000       19.70            2.220000   \n",
       "\n",
       "     solution_treatment  substrate_pretreatment  post_process  channel_width  \\\n",
       "0                     1                       1             1          10000   \n",
       "1                     1                       1             1          10000   \n",
       "2                     1                       1             1          10000   \n",
       "3                     1                       1             1          10000   \n",
       "4                     1                       1             1          10000   \n",
       "5                     1                       1             1          10000   \n",
       "6                     1                       1             1          10000   \n",
       "7                     1                       1             1          10000   \n",
       "8                     1                       1             1          10000   \n",
       "9                     1                       1             1          10000   \n",
       "10                    1                       1             1          10000   \n",
       "11                    1                       1             1          10000   \n",
       "12                    1                       1             1          10000   \n",
       "13                    1                       1             1          10000   \n",
       "14                    1                       1             1          10000   \n",
       "15                    1                       1             1          10000   \n",
       "16                    1                       1             1          10000   \n",
       "17                    1                       1             1          10000   \n",
       "18                    1                       1             1          10000   \n",
       "19                    1                       1             1          10000   \n",
       "20                    1                       1             1          10000   \n",
       "21                    1                       1             1          10000   \n",
       "22                    1                       1             1          10000   \n",
       "23                    1                       1             1          10000   \n",
       "24                    1                       1             1          10000   \n",
       "25                    1                       1             1          10000   \n",
       "26                    1                       1             1          10000   \n",
       "27                    1                       1             1          10000   \n",
       "28                    1                       1             1          10000   \n",
       "29                    1                       1             1          10000   \n",
       "30                    1                       1             1          10000   \n",
       "31                    1                       1             1          10000   \n",
       "32                    1                       1             1          10000   \n",
       "33                    1                       1             1          10000   \n",
       "34                    1                       1             1          10000   \n",
       "35                    1                       1             1          10000   \n",
       "36                    1                       1             1          10000   \n",
       "37                    1                       1             1          10000   \n",
       "38                    1                       1             1          10000   \n",
       "39                    1                       1             1          10000   \n",
       "40                    1                       1             1          10000   \n",
       "41                    1                       1             1          10000   \n",
       "42                    1                       1             1          10000   \n",
       "43                    1                       1             1          10000   \n",
       "44                    0                       0             0           2000   \n",
       "45                    1                       0             0           2000   \n",
       "46                    1                       0             0           2000   \n",
       "47                    1                       0             0           2000   \n",
       "48                    1                       0             0           2000   \n",
       "49                    1                       0             0           2000   \n",
       "50                    1                       0             0           2000   \n",
       "51                    1                       0             0           2000   \n",
       "52                    1                       0             0           2000   \n",
       "53                    1                       0             0           2000   \n",
       "54                    1                       0             0           2000   \n",
       "55                    0                       0             0           2000   \n",
       "56                    1                       0             0           2000   \n",
       "57                    1                       0             0           2000   \n",
       "58                    1                       0             0           2000   \n",
       "59                    1                       0             0           2000   \n",
       "60                    1                       0             0           2000   \n",
       "61                    0                       0             0           2000   \n",
       "62                    1                       0             0           2000   \n",
       "63                    1                       0             0           2000   \n",
       "64                    1                       0             0           2000   \n",
       "65                    1                       0             0           2000   \n",
       "66                    1                       0             0           2000   \n",
       "67                    1                       0             0           2000   \n",
       "68                    0                       1             0          40000   \n",
       "69                    0                       1             0          40000   \n",
       "70                    0                       1             0          40000   \n",
       "71                    0                       1             0          40000   \n",
       "72                    0                       1             0          40000   \n",
       "73                    0                       1             0          40000   \n",
       "74                    0                       1             0          40000   \n",
       "75                    0                       1             0          40000   \n",
       "76                    0                       1             0          40000   \n",
       "77                    0                       1             0          40000   \n",
       "78                    0                       1             0          40000   \n",
       "79                    1                       1             0            800   \n",
       "80                    1                       1             0            800   \n",
       "81                    1                       1             0            800   \n",
       "82                    1                       1             0            800   \n",
       "83                    0                       1             0         148500   \n",
       "84                    0                       1             0         148500   \n",
       "85                    0                       1             0         148500   \n",
       "86                    0                       1             0         148500   \n",
       "87                    0                       1             0         148500   \n",
       "88                    0                       1             0         148500   \n",
       "89                    0                       0             0           1000   \n",
       "90                    0                       0             0           1000   \n",
       "91                    0                       0             0           1000   \n",
       "92                    0                       1             0           1000   \n",
       "93                    0                       0             0           1000   \n",
       "94                    0                       0             0           1000   \n",
       "95                    0                       0             0           1000   \n",
       "96                    0                       1             0           1000   \n",
       "97                    0                       0             0           1000   \n",
       "98                    0                       0             0           1000   \n",
       "99                    0                       0             0           1000   \n",
       "100                   0                       1             0           1000   \n",
       "101                   0                       1             1           1000   \n",
       "102                   0                       1             0           9000   \n",
       "103                   0                       1             0           9000   \n",
       "104                   0                       1             0           9000   \n",
       "105                   0                       1             0           9000   \n",
       "106                   0                       0             0           9000   \n",
       "107                   0                       0             0           9000   \n",
       "108                   0                       0             0           9000   \n",
       "109                   0                       0             0           9000   \n",
       "110                   1                       1             0           9000   \n",
       "111                   1                       1             0           9000   \n",
       "112                   1                       1             0           9000   \n",
       "113                   1                       1             0           9000   \n",
       "114                   1                       1             0           9000   \n",
       "115                   1                       1             0           2000   \n",
       "116                   1                       1             0           2000   \n",
       "117                   1                       1             0           2000   \n",
       "118                   1                       1             0           2000   \n",
       "119                   1                       1             0           2000   \n",
       "120                   1                       1             0           2000   \n",
       "121                   1                       1             0           2000   \n",
       "122                   1                       1             1           2000   \n",
       "123                   1                       1             1           2000   \n",
       "124                   1                       1             1           2000   \n",
       "125                   1                       1             1           2000   \n",
       "126                   1                       1             1           2000   \n",
       "127                   1                       1             1           2000   \n",
       "128                   1                       1             1           2000   \n",
       "129                   1                       1             1           2000   \n",
       "130                   1                       1             0            500   \n",
       "131                   1                       1             0            500   \n",
       "132                   1                       1             0            500   \n",
       "133                   1                       1             0            500   \n",
       "134                   1                       1             0            500   \n",
       "135                   1                       1             0            500   \n",
       "136                   1                       1             0            500   \n",
       "137                   1                       1             0            500   \n",
       "138                   1                       1             0            500   \n",
       "139                   1                       1             0            500   \n",
       "140                   1                       1             0             50   \n",
       "141                   1                       1             0             50   \n",
       "142                   1                       1             0             50   \n",
       "143                   1                       1             0             50   \n",
       "144                   1                       1             0             50   \n",
       "145                   1                       1             0             50   \n",
       "146                   1                       1             0             50   \n",
       "147                   1                       1             0             50   \n",
       "148                   1                       1             0             50   \n",
       "149                   1                       1             0             50   \n",
       "150                   1                       1             0             50   \n",
       "151                   1                       1             0             50   \n",
       "152                   1                       1             0             50   \n",
       "153                   1                       1             0             50   \n",
       "154                   1                       1             0             50   \n",
       "155                   1                       1             0             50   \n",
       "156                   1                       1             0             50   \n",
       "157                   1                       1             0           1000   \n",
       "158                   1                       1             0           1000   \n",
       "159                   1                       1             0           1000   \n",
       "160                   1                       1             0             50   \n",
       "161                   1                       1             0             50   \n",
       "162                   1                       1             0             50   \n",
       "163                   1                       1             0             50   \n",
       "164                   1                       1             0             50   \n",
       "165                   1                       1             0             50   \n",
       "166                   1                       1             0             50   \n",
       "167                   1                       1             0             50   \n",
       "168                   1                       1             0             50   \n",
       "169                   1                       1             0             50   \n",
       "170                   1                       1             1             50   \n",
       "171                   1                       1             1             50   \n",
       "172                   1                       1             1             50   \n",
       "173                   1                       1             1             50   \n",
       "174                   1                       1             1             50   \n",
       "175                   1                       1             1             50   \n",
       "176                   1                       1             1             50   \n",
       "177                   1                       1             1             50   \n",
       "178                   1                       1             1             50   \n",
       "179                   1                       1             1             50   \n",
       "180                   1                       1             1             50   \n",
       "181                   1                       1             1             50   \n",
       "182                   0                       1             1           1000   \n",
       "183                   1                       1             1           2000   \n",
       "184                   1                       1             1           2000   \n",
       "185                   1                       1             1           2000   \n",
       "186                   1                       1             1           2000   \n",
       "187                   1                       1             1           2000   \n",
       "188                   1                       1             1           2000   \n",
       "189                   1                       1             1           2000   \n",
       "190                   1                       1             1           2000   \n",
       "191                   1                       1             1           2000   \n",
       "192                   1                       1             1           2000   \n",
       "193                   1                       1             1           2000   \n",
       "194                   1                       1             1           2000   \n",
       "195                   1                       1             1           2000   \n",
       "196                   1                       1             1           2000   \n",
       "197                   1                       1             1           2000   \n",
       "198                   1                       1             1           2000   \n",
       "199                   1                       1             1           2000   \n",
       "200                   1                       1             1           2000   \n",
       "201                   1                       1             1           2000   \n",
       "202                   1                       1             1           2000   \n",
       "203                   1                       1             1           2000   \n",
       "204                   1                       1             1           2000   \n",
       "205                   1                       1             1           2000   \n",
       "206                   1                       1             1           2000   \n",
       "207                   1                       1             1           2000   \n",
       "208                   1                       1             1           2000   \n",
       "209                   1                       1             1           2000   \n",
       "210                   1                       1             1           2000   \n",
       "211                   1                       1             1           2000   \n",
       "212                   1                       1             1           2000   \n",
       "213                   1                       1             1           2000   \n",
       "214                   1                       1             1           2000   \n",
       "215                   1                       1             1           2000   \n",
       "216                   1                       1             1           2000   \n",
       "217                   1                       1             1           2000   \n",
       "218                   1                       1             1           2000   \n",
       "219                   1                       1             1           2000   \n",
       "220                   1                       1             1           2000   \n",
       "221                   1                       1             1           2000   \n",
       "222                   1                       1             1           2000   \n",
       "223                   1                       1             1           2000   \n",
       "\n",
       "     channel_length  film_deposition_type_drop  film_deposition_type_spin  \\\n",
       "0              20.0                          0                          1   \n",
       "1              20.0                          0                          1   \n",
       "2              20.0                          0                          1   \n",
       "3              20.0                          0                          1   \n",
       "4              20.0                          0                          1   \n",
       "5              20.0                          0                          1   \n",
       "6              20.0                          0                          1   \n",
       "7              20.0                          0                          1   \n",
       "8              20.0                          0                          1   \n",
       "9              20.0                          0                          1   \n",
       "10             20.0                          0                          1   \n",
       "11             20.0                          0                          1   \n",
       "12             20.0                          0                          1   \n",
       "13             20.0                          0                          1   \n",
       "14             20.0                          0                          1   \n",
       "15             20.0                          0                          1   \n",
       "16             20.0                          1                          0   \n",
       "17             20.0                          1                          0   \n",
       "18             20.0                          1                          0   \n",
       "19             20.0                          1                          0   \n",
       "20             20.0                          1                          0   \n",
       "21             20.0                          1                          0   \n",
       "22             20.0                          1                          0   \n",
       "23             20.0                          1                          0   \n",
       "24              2.0                          0                          1   \n",
       "25              5.0                          0                          1   \n",
       "26             10.0                          0                          1   \n",
       "27             20.0                          0                          1   \n",
       "28              2.0                          0                          1   \n",
       "29              5.0                          0                          1   \n",
       "30             10.0                          0                          1   \n",
       "31              2.0                          0                          1   \n",
       "32              5.0                          0                          1   \n",
       "33             10.0                          0                          1   \n",
       "34             20.0                          0                          1   \n",
       "35              2.0                          0                          1   \n",
       "36              5.0                          0                          1   \n",
       "37             10.0                          0                          1   \n",
       "38              2.0                          0                          1   \n",
       "39              5.0                          0                          1   \n",
       "40             10.0                          0                          1   \n",
       "41              2.0                          0                          1   \n",
       "42              5.0                          0                          1   \n",
       "43             10.0                          0                          1   \n",
       "44             50.0                          0                          1   \n",
       "45             50.0                          0                          1   \n",
       "46             50.0                          0                          1   \n",
       "47             50.0                          0                          1   \n",
       "48             50.0                          0                          1   \n",
       "49             50.0                          0                          1   \n",
       "50             50.0                          0                          1   \n",
       "51             50.0                          0                          1   \n",
       "52             50.0                          0                          1   \n",
       "53             50.0                          0                          1   \n",
       "54             50.0                          0                          1   \n",
       "55             50.0                          0                          1   \n",
       "56             50.0                          0                          1   \n",
       "57             50.0                          0                          1   \n",
       "58             50.0                          0                          1   \n",
       "59             50.0                          0                          1   \n",
       "60             50.0                          0                          1   \n",
       "61             50.0                          0                          1   \n",
       "62             50.0                          0                          1   \n",
       "63             50.0                          0                          1   \n",
       "64             50.0                          0                          1   \n",
       "65             50.0                          0                          1   \n",
       "66             50.0                          0                          1   \n",
       "67             50.0                          0                          1   \n",
       "68              5.0                          0                          1   \n",
       "69              5.0                          0                          1   \n",
       "70              5.0                          0                          1   \n",
       "71              5.0                          0                          1   \n",
       "72              5.0                          0                          1   \n",
       "73              5.0                          0                          1   \n",
       "74              5.0                          0                          1   \n",
       "75              5.0                          0                          1   \n",
       "76              5.0                          0                          1   \n",
       "77              5.0                          0                          1   \n",
       "78              5.0                          0                          1   \n",
       "79             20.0                          1                          0   \n",
       "80             20.0                          1                          0   \n",
       "81             20.0                          1                          0   \n",
       "82             20.0                          1                          0   \n",
       "83            100.0                          0                          1   \n",
       "84            100.0                          0                          1   \n",
       "85            100.0                          0                          1   \n",
       "86            100.0                          0                          1   \n",
       "87            100.0                          0                          0   \n",
       "88            100.0                          0                          0   \n",
       "89              7.5                          0                          1   \n",
       "90              7.5                          0                          1   \n",
       "91              7.5                          0                          1   \n",
       "92              7.5                          0                          1   \n",
       "93              7.5                          0                          0   \n",
       "94              7.5                          0                          0   \n",
       "95              7.5                          0                          0   \n",
       "96              7.5                          0                          0   \n",
       "97              7.5                          1                          0   \n",
       "98              7.5                          1                          0   \n",
       "99              7.5                          1                          0   \n",
       "100             7.5                          1                          0   \n",
       "101             7.5                          1                          0   \n",
       "102            20.0                          0                          1   \n",
       "103            20.0                          0                          1   \n",
       "104            20.0                          0                          1   \n",
       "105            20.0                          0                          1   \n",
       "106            20.0                          0                          0   \n",
       "107            20.0                          0                          0   \n",
       "108            20.0                          0                          0   \n",
       "109            20.0                          0                          0   \n",
       "110            20.0                          1                          0   \n",
       "111            20.0                          1                          0   \n",
       "112            20.0                          1                          0   \n",
       "113            20.0                          1                          0   \n",
       "114            20.0                          1                          0   \n",
       "115            50.0                          0                          1   \n",
       "116            50.0                          0                          1   \n",
       "117            50.0                          0                          1   \n",
       "118            50.0                          0                          1   \n",
       "119            50.0                          0                          1   \n",
       "120            50.0                          0                          1   \n",
       "121            50.0                          0                          1   \n",
       "122            50.0                          0                          1   \n",
       "123            50.0                          0                          0   \n",
       "124            50.0                          0                          1   \n",
       "125            50.0                          0                          0   \n",
       "126            50.0                          0                          1   \n",
       "127            50.0                          0                          0   \n",
       "128            50.0                          0                          1   \n",
       "129            50.0                          0                          0   \n",
       "130            30.0                          0                          1   \n",
       "131            30.0                          0                          1   \n",
       "132            30.0                          0                          1   \n",
       "133            30.0                          0                          1   \n",
       "134            30.0                          0                          1   \n",
       "135            30.0                          0                          1   \n",
       "136            30.0                          0                          1   \n",
       "137            30.0                          0                          1   \n",
       "138            30.0                          0                          1   \n",
       "139            30.0                          0                          1   \n",
       "140          2000.0                          0                          0   \n",
       "141          2000.0                          0                          0   \n",
       "142          2000.0                          0                          0   \n",
       "143          2000.0                          0                          0   \n",
       "144          2000.0                          0                          0   \n",
       "145          2000.0                          0                          0   \n",
       "146          2000.0                          0                          0   \n",
       "147          2000.0                          0                          0   \n",
       "148          2000.0                          0                          0   \n",
       "149          2000.0                          0                          0   \n",
       "150          2000.0                          0                          0   \n",
       "151          2000.0                          0                          0   \n",
       "152          2000.0                          0                          0   \n",
       "153          2000.0                          0                          0   \n",
       "154          2000.0                          0                          0   \n",
       "155          2000.0                          0                          0   \n",
       "156          2000.0                          0                          0   \n",
       "157            30.0                          0                          1   \n",
       "158            30.0                          0                          1   \n",
       "159            30.0                          0                          1   \n",
       "160          2000.0                          0                          1   \n",
       "161          2000.0                          0                          1   \n",
       "162          2000.0                          0                          1   \n",
       "163          2000.0                          0                          1   \n",
       "164          2000.0                          0                          1   \n",
       "165          2000.0                          0                          1   \n",
       "166          2000.0                          0                          1   \n",
       "167          2000.0                          0                          1   \n",
       "168          2000.0                          0                          1   \n",
       "169          2000.0                          0                          1   \n",
       "170          2000.0                          0                          0   \n",
       "171          2000.0                          0                          0   \n",
       "172          2000.0                          0                          0   \n",
       "173          2000.0                          0                          0   \n",
       "174          2000.0                          0                          0   \n",
       "175          2000.0                          0                          0   \n",
       "176          2000.0                          0                          0   \n",
       "177          2000.0                          0                          0   \n",
       "178          2000.0                          0                          0   \n",
       "179          2000.0                          0                          0   \n",
       "180          2000.0                          0                          0   \n",
       "181          2000.0                          0                          0   \n",
       "182           100.0                          0                          1   \n",
       "183            50.0                          0                          1   \n",
       "184            50.0                          0                          1   \n",
       "185            50.0                          0                          1   \n",
       "186            50.0                          0                          1   \n",
       "187            50.0                          0                          1   \n",
       "188            50.0                          0                          1   \n",
       "189            50.0                          0                          1   \n",
       "190            50.0                          0                          1   \n",
       "191            50.0                          0                          1   \n",
       "192            50.0                          0                          1   \n",
       "193            50.0                          0                          1   \n",
       "194            50.0                          0                          1   \n",
       "195            50.0                          0                          1   \n",
       "196            50.0                          0                          1   \n",
       "197            50.0                          0                          1   \n",
       "198            50.0                          0                          1   \n",
       "199            50.0                          0                          1   \n",
       "200            50.0                          0                          1   \n",
       "201            50.0                          0                          1   \n",
       "202            50.0                          0                          1   \n",
       "203            50.0                          0                          1   \n",
       "204            50.0                          0                          1   \n",
       "205            50.0                          0                          1   \n",
       "206            50.0                          0                          1   \n",
       "207            50.0                          0                          1   \n",
       "208            50.0                          0                          1   \n",
       "209            50.0                          0                          1   \n",
       "210            50.0                          0                          1   \n",
       "211            50.0                          0                          1   \n",
       "212            50.0                          0                          1   \n",
       "213            50.0                          0                          1   \n",
       "214            50.0                          0                          1   \n",
       "215            50.0                          0                          1   \n",
       "216            50.0                          0                          1   \n",
       "217            50.0                          0                          1   \n",
       "218            50.0                          0                          1   \n",
       "219            50.0                          0                          1   \n",
       "220            50.0                          0                          1   \n",
       "221            50.0                          0                          1   \n",
       "222            50.0                          0                          1   \n",
       "223            50.0                          0                          1   \n",
       "\n",
       "     dielectric_material_SiO2  electrode_configuration_BGBC  \\\n",
       "0                           1                             1   \n",
       "1                           1                             1   \n",
       "2                           1                             1   \n",
       "3                           1                             1   \n",
       "4                           1                             1   \n",
       "5                           1                             1   \n",
       "6                           1                             1   \n",
       "7                           1                             1   \n",
       "8                           1                             1   \n",
       "9                           1                             1   \n",
       "10                          1                             1   \n",
       "11                          1                             1   \n",
       "12                          1                             1   \n",
       "13                          1                             1   \n",
       "14                          1                             1   \n",
       "15                          1                             1   \n",
       "16                          1                             1   \n",
       "17                          1                             1   \n",
       "18                          1                             1   \n",
       "19                          1                             1   \n",
       "20                          1                             1   \n",
       "21                          1                             1   \n",
       "22                          1                             1   \n",
       "23                          1                             1   \n",
       "24                          1                             1   \n",
       "25                          1                             1   \n",
       "26                          1                             1   \n",
       "27                          1                             1   \n",
       "28                          1                             1   \n",
       "29                          1                             1   \n",
       "30                          1                             1   \n",
       "31                          1                             1   \n",
       "32                          1                             1   \n",
       "33                          1                             1   \n",
       "34                          1                             1   \n",
       "35                          1                             1   \n",
       "36                          1                             1   \n",
       "37                          1                             1   \n",
       "38                          1                             1   \n",
       "39                          1                             1   \n",
       "40                          1                             1   \n",
       "41                          1                             1   \n",
       "42                          1                             1   \n",
       "43                          1                             1   \n",
       "44                          1                             1   \n",
       "45                          1                             1   \n",
       "46                          1                             1   \n",
       "47                          1                             1   \n",
       "48                          1                             1   \n",
       "49                          1                             1   \n",
       "50                          1                             1   \n",
       "51                          1                             1   \n",
       "52                          1                             1   \n",
       "53                          1                             1   \n",
       "54                          1                             1   \n",
       "55                          1                             1   \n",
       "56                          1                             1   \n",
       "57                          1                             1   \n",
       "58                          1                             1   \n",
       "59                          1                             1   \n",
       "60                          1                             1   \n",
       "61                          1                             1   \n",
       "62                          1                             1   \n",
       "63                          1                             1   \n",
       "64                          1                             1   \n",
       "65                          1                             1   \n",
       "66                          1                             1   \n",
       "67                          1                             1   \n",
       "68                          1                             1   \n",
       "69                          1                             1   \n",
       "70                          1                             1   \n",
       "71                          1                             1   \n",
       "72                          1                             1   \n",
       "73                          1                             1   \n",
       "74                          1                             1   \n",
       "75                          1                             1   \n",
       "76                          1                             1   \n",
       "77                          1                             1   \n",
       "78                          1                             1   \n",
       "79                          1                             1   \n",
       "80                          1                             1   \n",
       "81                          1                             1   \n",
       "82                          1                             1   \n",
       "83                          1                             0   \n",
       "84                          1                             0   \n",
       "85                          1                             0   \n",
       "86                          1                             0   \n",
       "87                          1                             0   \n",
       "88                          1                             0   \n",
       "89                          1                             1   \n",
       "90                          1                             1   \n",
       "91                          1                             1   \n",
       "92                          1                             1   \n",
       "93                          1                             1   \n",
       "94                          1                             1   \n",
       "95                          1                             1   \n",
       "96                          1                             1   \n",
       "97                          1                             1   \n",
       "98                          1                             1   \n",
       "99                          1                             1   \n",
       "100                         1                             1   \n",
       "101                         1                             1   \n",
       "102                         1                             1   \n",
       "103                         1                             1   \n",
       "104                         1                             1   \n",
       "105                         1                             1   \n",
       "106                         1                             1   \n",
       "107                         1                             1   \n",
       "108                         1                             1   \n",
       "109                         1                             1   \n",
       "110                         1                             1   \n",
       "111                         1                             1   \n",
       "112                         1                             1   \n",
       "113                         1                             1   \n",
       "114                         1                             1   \n",
       "115                         0                             1   \n",
       "116                         0                             1   \n",
       "117                         0                             1   \n",
       "118                         0                             1   \n",
       "119                         0                             1   \n",
       "120                         0                             1   \n",
       "121                         0                             1   \n",
       "122                         0                             1   \n",
       "123                         0                             1   \n",
       "124                         0                             1   \n",
       "125                         0                             1   \n",
       "126                         0                             1   \n",
       "127                         0                             1   \n",
       "128                         0                             1   \n",
       "129                         0                             1   \n",
       "130                         0                             0   \n",
       "131                         0                             0   \n",
       "132                         0                             0   \n",
       "133                         0                             0   \n",
       "134                         0                             0   \n",
       "135                         0                             0   \n",
       "136                         0                             0   \n",
       "137                         0                             0   \n",
       "138                         0                             0   \n",
       "139                         0                             0   \n",
       "140                         0                             1   \n",
       "141                         0                             1   \n",
       "142                         0                             1   \n",
       "143                         0                             1   \n",
       "144                         0                             1   \n",
       "145                         0                             1   \n",
       "146                         0                             1   \n",
       "147                         0                             1   \n",
       "148                         0                             1   \n",
       "149                         0                             1   \n",
       "150                         0                             1   \n",
       "151                         0                             1   \n",
       "152                         0                             1   \n",
       "153                         0                             1   \n",
       "154                         0                             1   \n",
       "155                         0                             1   \n",
       "156                         0                             1   \n",
       "157                         0                             0   \n",
       "158                         0                             0   \n",
       "159                         0                             0   \n",
       "160                         0                             1   \n",
       "161                         0                             1   \n",
       "162                         0                             1   \n",
       "163                         0                             1   \n",
       "164                         0                             1   \n",
       "165                         0                             1   \n",
       "166                         0                             1   \n",
       "167                         0                             1   \n",
       "168                         0                             1   \n",
       "169                         0                             1   \n",
       "170                         0                             1   \n",
       "171                         0                             1   \n",
       "172                         0                             1   \n",
       "173                         0                             1   \n",
       "174                         0                             1   \n",
       "175                         0                             1   \n",
       "176                         0                             1   \n",
       "177                         0                             1   \n",
       "178                         0                             1   \n",
       "179                         0                             1   \n",
       "180                         0                             1   \n",
       "181                         0                             1   \n",
       "182                         0                             0   \n",
       "183                         0                             1   \n",
       "184                         0                             1   \n",
       "185                         0                             1   \n",
       "186                         0                             1   \n",
       "187                         0                             1   \n",
       "188                         0                             1   \n",
       "189                         0                             1   \n",
       "190                         0                             1   \n",
       "191                         0                             1   \n",
       "192                         0                             1   \n",
       "193                         0                             1   \n",
       "194                         0                             1   \n",
       "195                         0                             1   \n",
       "196                         0                             1   \n",
       "197                         0                             1   \n",
       "198                         0                             1   \n",
       "199                         0                             1   \n",
       "200                         0                             1   \n",
       "201                         0                             1   \n",
       "202                         0                             1   \n",
       "203                         0                             1   \n",
       "204                         0                             1   \n",
       "205                         0                             1   \n",
       "206                         0                             1   \n",
       "207                         0                             1   \n",
       "208                         0                             1   \n",
       "209                         0                             1   \n",
       "210                         0                             1   \n",
       "211                         0                             1   \n",
       "212                         0                             1   \n",
       "213                         0                             1   \n",
       "214                         0                             1   \n",
       "215                         0                             1   \n",
       "216                         0                             1   \n",
       "217                         0                             1   \n",
       "218                         0                             1   \n",
       "219                         0                             1   \n",
       "220                         0                             1   \n",
       "221                         0                             1   \n",
       "222                         0                             1   \n",
       "223                         0                             1   \n",
       "\n",
       "     electrode_configuration_BGTC  film_deposition_type_MGC  \\\n",
       "0                               0                         0   \n",
       "1                               0                         0   \n",
       "2                               0                         0   \n",
       "3                               0                         0   \n",
       "4                               0                         0   \n",
       "5                               0                         0   \n",
       "6                               0                         0   \n",
       "7                               0                         0   \n",
       "8                               0                         0   \n",
       "9                               0                         0   \n",
       "10                              0                         0   \n",
       "11                              0                         0   \n",
       "12                              0                         0   \n",
       "13                              0                         0   \n",
       "14                              0                         0   \n",
       "15                              0                         0   \n",
       "16                              0                         0   \n",
       "17                              0                         0   \n",
       "18                              0                         0   \n",
       "19                              0                         0   \n",
       "20                              0                         0   \n",
       "21                              0                         0   \n",
       "22                              0                         0   \n",
       "23                              0                         0   \n",
       "24                              0                         0   \n",
       "25                              0                         0   \n",
       "26                              0                         0   \n",
       "27                              0                         0   \n",
       "28                              0                         0   \n",
       "29                              0                         0   \n",
       "30                              0                         0   \n",
       "31                              0                         0   \n",
       "32                              0                         0   \n",
       "33                              0                         0   \n",
       "34                              0                         0   \n",
       "35                              0                         0   \n",
       "36                              0                         0   \n",
       "37                              0                         0   \n",
       "38                              0                         0   \n",
       "39                              0                         0   \n",
       "40                              0                         0   \n",
       "41                              0                         0   \n",
       "42                              0                         0   \n",
       "43                              0                         0   \n",
       "44                              0                         0   \n",
       "45                              0                         0   \n",
       "46                              0                         0   \n",
       "47                              0                         0   \n",
       "48                              0                         0   \n",
       "49                              0                         0   \n",
       "50                              0                         0   \n",
       "51                              0                         0   \n",
       "52                              0                         0   \n",
       "53                              0                         0   \n",
       "54                              0                         0   \n",
       "55                              0                         0   \n",
       "56                              0                         0   \n",
       "57                              0                         0   \n",
       "58                              0                         0   \n",
       "59                              0                         0   \n",
       "60                              0                         0   \n",
       "61                              0                         0   \n",
       "62                              0                         0   \n",
       "63                              0                         0   \n",
       "64                              0                         0   \n",
       "65                              0                         0   \n",
       "66                              0                         0   \n",
       "67                              0                         0   \n",
       "68                              0                         0   \n",
       "69                              0                         0   \n",
       "70                              0                         0   \n",
       "71                              0                         0   \n",
       "72                              0                         0   \n",
       "73                              0                         0   \n",
       "74                              0                         0   \n",
       "75                              0                         0   \n",
       "76                              0                         0   \n",
       "77                              0                         0   \n",
       "78                              0                         0   \n",
       "79                              0                         0   \n",
       "80                              0                         0   \n",
       "81                              0                         0   \n",
       "82                              0                         0   \n",
       "83                              1                         0   \n",
       "84                              1                         0   \n",
       "85                              1                         0   \n",
       "86                              1                         0   \n",
       "87                              1                         1   \n",
       "88                              1                         1   \n",
       "89                              0                         0   \n",
       "90                              0                         0   \n",
       "91                              0                         0   \n",
       "92                              0                         0   \n",
       "93                              0                         1   \n",
       "94                              0                         1   \n",
       "95                              0                         1   \n",
       "96                              0                         1   \n",
       "97                              0                         0   \n",
       "98                              0                         0   \n",
       "99                              0                         0   \n",
       "100                             0                         0   \n",
       "101                             0                         0   \n",
       "102                             0                         0   \n",
       "103                             0                         0   \n",
       "104                             0                         0   \n",
       "105                             0                         0   \n",
       "106                             0                         1   \n",
       "107                             0                         1   \n",
       "108                             0                         1   \n",
       "109                             0                         1   \n",
       "110                             0                         0   \n",
       "111                             0                         0   \n",
       "112                             0                         0   \n",
       "113                             0                         0   \n",
       "114                             0                         0   \n",
       "115                             0                         0   \n",
       "116                             0                         0   \n",
       "117                             0                         0   \n",
       "118                             0                         0   \n",
       "119                             0                         0   \n",
       "120                             0                         0   \n",
       "121                             0                         0   \n",
       "122                             0                         0   \n",
       "123                             0                         1   \n",
       "124                             0                         0   \n",
       "125                             0                         1   \n",
       "126                             0                         0   \n",
       "127                             0                         1   \n",
       "128                             0                         0   \n",
       "129                             0                         1   \n",
       "130                             1                         0   \n",
       "131                             1                         0   \n",
       "132                             1                         0   \n",
       "133                             1                         0   \n",
       "134                             1                         0   \n",
       "135                             1                         0   \n",
       "136                             1                         0   \n",
       "137                             1                         0   \n",
       "138                             1                         0   \n",
       "139                             1                         0   \n",
       "140                             0                         1   \n",
       "141                             0                         1   \n",
       "142                             0                         1   \n",
       "143                             0                         1   \n",
       "144                             0                         1   \n",
       "145                             0                         1   \n",
       "146                             0                         1   \n",
       "147                             0                         1   \n",
       "148                             0                         1   \n",
       "149                             0                         1   \n",
       "150                             0                         1   \n",
       "151                             0                         1   \n",
       "152                             0                         1   \n",
       "153                             0                         1   \n",
       "154                             0                         1   \n",
       "155                             0                         1   \n",
       "156                             0                         1   \n",
       "157                             0                         0   \n",
       "158                             0                         0   \n",
       "159                             0                         0   \n",
       "160                             0                         0   \n",
       "161                             0                         0   \n",
       "162                             0                         0   \n",
       "163                             0                         0   \n",
       "164                             0                         0   \n",
       "165                             0                         0   \n",
       "166                             0                         0   \n",
       "167                             0                         0   \n",
       "168                             0                         0   \n",
       "169                             0                         0   \n",
       "170                             0                         1   \n",
       "171                             0                         1   \n",
       "172                             0                         1   \n",
       "173                             0                         1   \n",
       "174                             0                         1   \n",
       "175                             0                         1   \n",
       "176                             0                         1   \n",
       "177                             0                         1   \n",
       "178                             0                         1   \n",
       "179                             0                         1   \n",
       "180                             0                         1   \n",
       "181                             0                         1   \n",
       "182                             1                         0   \n",
       "183                             0                         0   \n",
       "184                             0                         0   \n",
       "185                             0                         0   \n",
       "186                             0                         0   \n",
       "187                             0                         0   \n",
       "188                             0                         0   \n",
       "189                             0                         0   \n",
       "190                             0                         0   \n",
       "191                             0                         0   \n",
       "192                             0                         0   \n",
       "193                             0                         0   \n",
       "194                             0                         0   \n",
       "195                             0                         0   \n",
       "196                             0                         0   \n",
       "197                             0                         0   \n",
       "198                             0                         0   \n",
       "199                             0                         0   \n",
       "200                             0                         0   \n",
       "201                             0                         0   \n",
       "202                             0                         0   \n",
       "203                             0                         0   \n",
       "204                             0                         0   \n",
       "205                             0                         0   \n",
       "206                             0                         0   \n",
       "207                             0                         0   \n",
       "208                             0                         0   \n",
       "209                             0                         0   \n",
       "210                             0                         0   \n",
       "211                             0                         0   \n",
       "212                             0                         0   \n",
       "213                             0                         0   \n",
       "214                             0                         0   \n",
       "215                             0                         0   \n",
       "216                             0                         0   \n",
       "217                             0                         0   \n",
       "218                             0                         0   \n",
       "219                             0                         0   \n",
       "220                             0                         0   \n",
       "221                             0                         0   \n",
       "222                             0                         0   \n",
       "223                             0                         0   \n",
       "\n",
       "     solvent_boiling_point  hole_mobility  \n",
       "0                       62       0.007600  \n",
       "1                       62       0.004960  \n",
       "2                       62       0.009480  \n",
       "3                       62       0.012000  \n",
       "4                       62       0.005290  \n",
       "5                       62       0.005540  \n",
       "6                       62       0.002990  \n",
       "7                       62       0.008100  \n",
       "8                      213       0.001700  \n",
       "9                      213       0.001490  \n",
       "10                     213       0.030500  \n",
       "11                     213       0.063100  \n",
       "12                     213       0.077500  \n",
       "13                     213       0.098200  \n",
       "14                     213       0.122600  \n",
       "15                     213       0.143500  \n",
       "16                      62       0.009940  \n",
       "17                      62       0.018700  \n",
       "18                      62       0.027300  \n",
       "19                      62       0.053900  \n",
       "20                      62       0.061200  \n",
       "21                      62       0.066200  \n",
       "22                      62       0.065200  \n",
       "23                      62       0.081300  \n",
       "24                      62       0.014500  \n",
       "25                      62       0.005440  \n",
       "26                      62       0.003630  \n",
       "27                      62       0.002960  \n",
       "28                      62       0.022900  \n",
       "29                      62       0.018300  \n",
       "30                      62       0.014000  \n",
       "31                      62       0.017100  \n",
       "32                      62       0.012000  \n",
       "33                      62       0.009930  \n",
       "34                      62       0.007570  \n",
       "35                     213       0.077000  \n",
       "36                     213       0.068000  \n",
       "37                     213       0.065000  \n",
       "38                     213       0.007620  \n",
       "39                     213       0.003810  \n",
       "40                     213       0.002550  \n",
       "41                     213       0.137800  \n",
       "42                     213       0.135000  \n",
       "43                     213       0.125300  \n",
       "44                      62       0.004230  \n",
       "45                      62       0.007930  \n",
       "46                      62       0.010700  \n",
       "47                      62       0.016500  \n",
       "48                      62       0.010100  \n",
       "49                      62       0.008920  \n",
       "50                      62       0.004700  \n",
       "51                      62       0.004060  \n",
       "52                      62       0.002140  \n",
       "53                      62       0.002280  \n",
       "54                      62       0.002790  \n",
       "55                     132       0.000556  \n",
       "56                     132       0.000413  \n",
       "57                     132       0.000426  \n",
       "58                     132       0.000254  \n",
       "59                     132       0.000309  \n",
       "60                     132       0.000377  \n",
       "61                      62       0.035220  \n",
       "62                      62       0.051820  \n",
       "63                      62       0.054680  \n",
       "64                      62       0.103430  \n",
       "65                      62       0.053900  \n",
       "66                      62       0.020930  \n",
       "67                      62       0.011230  \n",
       "68                      62       0.000005  \n",
       "69                      62       0.000356  \n",
       "70                      62       0.000735  \n",
       "71                      62       0.007480  \n",
       "72                      62       0.000016  \n",
       "73                      62       0.000537  \n",
       "74                      62       0.004190  \n",
       "75                      62       0.009080  \n",
       "76                      62       0.000002  \n",
       "77                      62       0.007850  \n",
       "78                      62       0.007670  \n",
       "79                     138       0.023900  \n",
       "80                     111       0.030500  \n",
       "81                     145       0.033500  \n",
       "82                      80       0.014500  \n",
       "83                      62       0.000046  \n",
       "84                      62       0.019000  \n",
       "85                      62       0.016000  \n",
       "86                      62       0.000001  \n",
       "87                      62       0.000609  \n",
       "88                      62       0.010600  \n",
       "89                     213       0.023000  \n",
       "90                     138       0.015000  \n",
       "91                      62       0.001100  \n",
       "92                      62       0.001600  \n",
       "93                     213       0.002400  \n",
       "94                     138       0.015000  \n",
       "95                      62       0.085000  \n",
       "96                      62       0.150000  \n",
       "97                     213       0.000150  \n",
       "98                     138       0.001400  \n",
       "99                      62       0.021000  \n",
       "100                     62       0.002300  \n",
       "101                     62       0.250000  \n",
       "102                     62       0.000049  \n",
       "103                     62       0.000551  \n",
       "104                     62       0.003990  \n",
       "105                     62       0.022900  \n",
       "106                     62       0.000114  \n",
       "107                     62       0.000912  \n",
       "108                     62       0.006870  \n",
       "109                     62       0.017000  \n",
       "110                     62       0.000015  \n",
       "111                     62       0.000034  \n",
       "112                     62       0.000709  \n",
       "113                     62       0.004140  \n",
       "114                     62       0.036400  \n",
       "115                     62       0.000202  \n",
       "116                     62       0.007190  \n",
       "117                     62       0.029400  \n",
       "118                     62       0.050600  \n",
       "119                     62       0.030600  \n",
       "120                     62       0.036100  \n",
       "121                     62       0.015300  \n",
       "122                     62       0.027300  \n",
       "123                     62       0.049400  \n",
       "124                     62       0.001160  \n",
       "125                     62       0.003450  \n",
       "126                     62       0.060800  \n",
       "127                     62       0.043600  \n",
       "128                     62       0.020500  \n",
       "129                     62       0.014800  \n",
       "130                    111       0.003680  \n",
       "131                    111       0.001350  \n",
       "132                    111       0.000629  \n",
       "133                    111       0.000871  \n",
       "134                    111       0.001020  \n",
       "135                    111       0.001020  \n",
       "136                    111       0.003050  \n",
       "137                    111       0.008030  \n",
       "138                    111       0.009530  \n",
       "139                    111       0.009150  \n",
       "140                     62       0.008397  \n",
       "141                     62       0.002720  \n",
       "142                     62       0.002300  \n",
       "143                     62       0.002395  \n",
       "144                     62       0.004360  \n",
       "145                     62       0.004459  \n",
       "146                     62       0.006890  \n",
       "147                     62       0.006050  \n",
       "148                     62       0.012700  \n",
       "149                     62       0.008115  \n",
       "150                     62       0.103900  \n",
       "151                     62       1.651400  \n",
       "152                     62       0.175180  \n",
       "153                     62       0.187000  \n",
       "154                     62       0.150000  \n",
       "155                     62       0.011071  \n",
       "156                     62       0.120523  \n",
       "157                     62       0.058170  \n",
       "158                     62       0.026300  \n",
       "159                    132       0.000000  \n",
       "160                     62       0.013000  \n",
       "161                     62       0.086700  \n",
       "162                     62       0.149000  \n",
       "163                     62       0.111300  \n",
       "164                     62       0.022500  \n",
       "165                     62       0.101600  \n",
       "166                     62       0.090500  \n",
       "167                     62       0.109100  \n",
       "168                     62       0.089400  \n",
       "169                     62       0.013000  \n",
       "170                     62       0.138900  \n",
       "171                     62       0.108900  \n",
       "172                     62       0.064400  \n",
       "173                     62       0.048700  \n",
       "174                     62       0.042790  \n",
       "175                     62       0.101700  \n",
       "176                     62       0.083050  \n",
       "177                     62       0.064800  \n",
       "178                     62       0.090670  \n",
       "179                     62       0.135000  \n",
       "180                     62       0.104200  \n",
       "181                     62       0.125000  \n",
       "182                     62       0.030000  \n",
       "183                     62       0.004000  \n",
       "184                     62       0.010000  \n",
       "185                     62       0.022000  \n",
       "186                     62       0.020000  \n",
       "187                     62       0.020000  \n",
       "188                     62       0.035000  \n",
       "189                     62       0.044000  \n",
       "190                     62       0.042000  \n",
       "191                     62       0.019000  \n",
       "192                     62       0.023000  \n",
       "193                     62       0.030000  \n",
       "194                     62       0.032000  \n",
       "195                     62       0.018000  \n",
       "196                     62       0.026000  \n",
       "197                     62       0.021000  \n",
       "198                     62       0.019000  \n",
       "199                     62       0.023000  \n",
       "200                     62       0.069000  \n",
       "201                     62       0.081000  \n",
       "202                     62       0.091000  \n",
       "203                     62       0.085000  \n",
       "204                     62       0.090000  \n",
       "205                     62       0.102000  \n",
       "206                     62       0.120000  \n",
       "207                     62       0.072000  \n",
       "208                     62       0.094000  \n",
       "209                     62       0.101000  \n",
       "210                     62       0.111000  \n",
       "211                     62       0.051000  \n",
       "212                     62       0.050000  \n",
       "213                     62       0.059000  \n",
       "214                     62       0.077000  \n",
       "215                     62       0.015700  \n",
       "216                     62       0.046700  \n",
       "217                     62       0.058400  \n",
       "218                     62       0.065200  \n",
       "219                     62       0.082400  \n",
       "220                     62       0.080400  \n",
       "221                     62       0.018000  \n",
       "222                     62       0.002100  \n",
       "223                     62       0.079000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned = pd.read_csv('P3HT_cleaned.csv')\n",
    "print(df_cleaned.shape)\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 16)\n",
      "(224, 1)\n"
     ]
    }
   ],
   "source": [
    "df_X = df_cleaned.copy().drop(['hole_mobility'], axis=1)\n",
    "df_Y = df_cleaned[['hole_mobility']] \n",
    "\n",
    "print(df_X.shape)\n",
    "print(df_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape of X = (168, 16) and Y = (168, 1) : \n",
      "Test data shape of X = (56, 16) and Y = (56, 1) : \n"
     ]
    }
   ],
   "source": [
    "# Train-Test Split of the data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_X_train, df_X_test, df_Y_train, df_Y_test = train_test_split(\n",
    "    df_X, df_Y,\n",
    "    test_size = 0.25,random_state=1234)\n",
    " \n",
    "print(\"Train data shape of X = % s and Y = % s : \"%(\n",
    "   df_X_train.shape, df_Y_train.shape))\n",
    " \n",
    "print(\"Test data shape of X = % s and Y = % s : \"%(\n",
    "   df_X_test.shape, df_Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>solution_concentration</th>\n",
       "      <th>polymer_mw</th>\n",
       "      <th>polymer_mn</th>\n",
       "      <th>polymer_dispersity</th>\n",
       "      <th>solution_treatment</th>\n",
       "      <th>substrate_pretreatment</th>\n",
       "      <th>post_process</th>\n",
       "      <th>channel_width</th>\n",
       "      <th>channel_length</th>\n",
       "      <th>film_deposition_type_drop</th>\n",
       "      <th>film_deposition_type_spin</th>\n",
       "      <th>dielectric_material_SiO2</th>\n",
       "      <th>electrode_configuration_BGBC</th>\n",
       "      <th>electrode_configuration_BGTC</th>\n",
       "      <th>film_deposition_type_MGC</th>\n",
       "      <th>solvent_boiling_point</th>\n",
       "      <th>hole_mobility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.400</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.003810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>5.0</td>\n",
       "      <td>104.000</td>\n",
       "      <td>47.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.064400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.005440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>8.0</td>\n",
       "      <td>74.000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>1.651400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>5.0</td>\n",
       "      <td>51.000</td>\n",
       "      <td>24.00</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.058170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>3.5</td>\n",
       "      <td>47.700</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.003450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.0</td>\n",
       "      <td>20.000</td>\n",
       "      <td>11.10</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>3.5</td>\n",
       "      <td>47.700</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.049400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>3.5</td>\n",
       "      <td>47.700</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>3.0</td>\n",
       "      <td>74.000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.008115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>5.0</td>\n",
       "      <td>71.000</td>\n",
       "      <td>32.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.111300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.740</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1.0</td>\n",
       "      <td>21.384</td>\n",
       "      <td>10.80</td>\n",
       "      <td>1.980000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.006870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.002960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>4.0</td>\n",
       "      <td>74.000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.004459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1.0</td>\n",
       "      <td>74.000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.481</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.008920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.1</td>\n",
       "      <td>37.740</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.035220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>10.0</td>\n",
       "      <td>74.000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.006890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.0</td>\n",
       "      <td>42.000</td>\n",
       "      <td>23.30</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.755</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2.755</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>5.0</td>\n",
       "      <td>66.000</td>\n",
       "      <td>33.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.090670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.700</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>5.0</td>\n",
       "      <td>71.000</td>\n",
       "      <td>32.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.700</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.700</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.015700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.755</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.400</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.009930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>15.0</td>\n",
       "      <td>25.650</td>\n",
       "      <td>19.00</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.068000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>3.0</td>\n",
       "      <td>74.000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.004360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.481</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0.000426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>3.5</td>\n",
       "      <td>163.200</td>\n",
       "      <td>68.00</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.008030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.481</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.010700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.011230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2.0</td>\n",
       "      <td>48.960</td>\n",
       "      <td>24.00</td>\n",
       "      <td>2.040000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0.023900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2.0</td>\n",
       "      <td>21.384</td>\n",
       "      <td>10.80</td>\n",
       "      <td>1.980000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.003990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>5.0</td>\n",
       "      <td>71.000</td>\n",
       "      <td>32.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.109100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>5.0</td>\n",
       "      <td>71.000</td>\n",
       "      <td>32.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.101600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.0</td>\n",
       "      <td>270.000</td>\n",
       "      <td>117.40</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.143500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>3.5</td>\n",
       "      <td>70.000</td>\n",
       "      <td>35.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.003680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.740</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>138</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.481</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.004230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.400</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.5</td>\n",
       "      <td>74.000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.002720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.481</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0.000413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.065200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.103430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.1</td>\n",
       "      <td>37.740</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.400</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>5.0</td>\n",
       "      <td>66.000</td>\n",
       "      <td>33.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.104200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.700</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.700</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>5.0</td>\n",
       "      <td>24.120</td>\n",
       "      <td>13.40</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>4.0</td>\n",
       "      <td>47.700</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.0</td>\n",
       "      <td>42.000</td>\n",
       "      <td>23.30</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.700</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>20.000</td>\n",
       "      <td>11.10</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.004960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>5.0</td>\n",
       "      <td>66.000</td>\n",
       "      <td>33.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>5.0</td>\n",
       "      <td>104.000</td>\n",
       "      <td>47.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.048700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.700</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>4.0</td>\n",
       "      <td>47.700</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.030600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10.0</td>\n",
       "      <td>52.000</td>\n",
       "      <td>22.60</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.066200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>3.5</td>\n",
       "      <td>163.200</td>\n",
       "      <td>68.00</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.009530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.5</td>\n",
       "      <td>60.750</td>\n",
       "      <td>27.00</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.036400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.740</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.1</td>\n",
       "      <td>37.740</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>4.0</td>\n",
       "      <td>47.700</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.015300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.400</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2.0</td>\n",
       "      <td>60.750</td>\n",
       "      <td>27.00</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>5.0</td>\n",
       "      <td>71.000</td>\n",
       "      <td>32.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.051820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.540</td>\n",
       "      <td>31.10</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.007670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.481</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0.000556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>5.0</td>\n",
       "      <td>104.000</td>\n",
       "      <td>47.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.138900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.592</td>\n",
       "      <td>5.60</td>\n",
       "      <td>2.070000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2.0</td>\n",
       "      <td>48.960</td>\n",
       "      <td>24.00</td>\n",
       "      <td>2.040000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>0.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>42.000</td>\n",
       "      <td>23.30</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.005290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>22.000</td>\n",
       "      <td>13.80</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.030500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.700</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.700</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>5.0</td>\n",
       "      <td>71.000</td>\n",
       "      <td>32.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>3.5</td>\n",
       "      <td>70.000</td>\n",
       "      <td>35.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.000871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>3.5</td>\n",
       "      <td>163.200</td>\n",
       "      <td>68.00</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.001020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>10.0</td>\n",
       "      <td>43.700</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1.0</td>\n",
       "      <td>60.750</td>\n",
       "      <td>27.00</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>15.0</td>\n",
       "      <td>18.600</td>\n",
       "      <td>18.60</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.0</td>\n",
       "      <td>22.000</td>\n",
       "      <td>13.80</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>10.0</td>\n",
       "      <td>43.700</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.079000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>15.0</td>\n",
       "      <td>6.608</td>\n",
       "      <td>5.60</td>\n",
       "      <td>1.180000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>5.0</td>\n",
       "      <td>13.920</td>\n",
       "      <td>11.60</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.980</td>\n",
       "      <td>4.60</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.137800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.400</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.009940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.481</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.007930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>15.0</td>\n",
       "      <td>11.300</td>\n",
       "      <td>11.30</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.740</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>5.0</td>\n",
       "      <td>77.500</td>\n",
       "      <td>26.00</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.5</td>\n",
       "      <td>21.384</td>\n",
       "      <td>10.80</td>\n",
       "      <td>1.980000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.004140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.700</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.740</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.700</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.065200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.0</td>\n",
       "      <td>52.000</td>\n",
       "      <td>22.60</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.098200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>15.0</td>\n",
       "      <td>20.424</td>\n",
       "      <td>13.80</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2.0</td>\n",
       "      <td>48.960</td>\n",
       "      <td>24.00</td>\n",
       "      <td>2.040000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>0.033500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>10.0</td>\n",
       "      <td>74.000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.120523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>5.0</td>\n",
       "      <td>41.000</td>\n",
       "      <td>20.50</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.042790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.481</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0.000377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.0</td>\n",
       "      <td>11.592</td>\n",
       "      <td>5.60</td>\n",
       "      <td>2.070000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>4.0</td>\n",
       "      <td>47.700</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.0</td>\n",
       "      <td>74.000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.008397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>22.000</td>\n",
       "      <td>13.80</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.009480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.700</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>5.0</td>\n",
       "      <td>62.050</td>\n",
       "      <td>36.50</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.009080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>3.5</td>\n",
       "      <td>23.000</td>\n",
       "      <td>12.70</td>\n",
       "      <td>1.811024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>3.5</td>\n",
       "      <td>163.200</td>\n",
       "      <td>68.00</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.009150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>5.0</td>\n",
       "      <td>71.000</td>\n",
       "      <td>32.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.400</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.007570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.065000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.480</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.481</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>5.0</td>\n",
       "      <td>41.000</td>\n",
       "      <td>20.50</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.101700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>3.5</td>\n",
       "      <td>23.000</td>\n",
       "      <td>12.70</td>\n",
       "      <td>1.811024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.043600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.700</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.081000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2.0</td>\n",
       "      <td>48.960</td>\n",
       "      <td>24.00</td>\n",
       "      <td>2.040000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.030500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10.800</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.5</td>\n",
       "      <td>11.592</td>\n",
       "      <td>5.60</td>\n",
       "      <td>2.070000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>4.0</td>\n",
       "      <td>47.700</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.036100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>4.0</td>\n",
       "      <td>47.700</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.003630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>3.5</td>\n",
       "      <td>47.700</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>5.0</td>\n",
       "      <td>51.000</td>\n",
       "      <td>24.00</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>5.0</td>\n",
       "      <td>104.000</td>\n",
       "      <td>47.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.108900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>15.0</td>\n",
       "      <td>74.000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10.0</td>\n",
       "      <td>270.000</td>\n",
       "      <td>117.40</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.081300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>5.0</td>\n",
       "      <td>41.000</td>\n",
       "      <td>20.50</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.064800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2.0</td>\n",
       "      <td>74.000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.002395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>10.0</td>\n",
       "      <td>74.000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.175180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>4.0</td>\n",
       "      <td>47.700</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.007190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.700</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.481</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.002280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.400</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.007620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.700</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.481</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.016500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     solution_concentration  polymer_mw  polymer_mn  polymer_dispersity  \\\n",
       "39                     10.0      15.400       10.30            1.500000   \n",
       "172                     5.0     104.000       47.27            2.200000   \n",
       "25                     10.0      76.000       44.70            1.700000   \n",
       "151                     8.0      74.000       33.63            2.200000   \n",
       "157                     5.0      51.000       24.00            2.100000   \n",
       "125                     3.5      47.700       24.00            1.987500   \n",
       "17                     10.0      20.000       11.10            1.800000   \n",
       "123                     3.5      47.700       24.00            1.987500   \n",
       "122                     3.5      47.700       24.00            1.987500   \n",
       "11                     10.0      29.000       19.30            1.500000   \n",
       "192                     5.0      69.000       34.60            2.000000   \n",
       "149                     3.0      74.000       33.63            2.200000   \n",
       "163                     5.0      71.000       32.27            2.200000   \n",
       "95                      1.0      37.740       25.50            1.480000   \n",
       "108                     1.0      21.384       10.80            1.980000   \n",
       "27                     10.0      76.000       44.70            1.700000   \n",
       "145                     4.0      74.000       33.63            2.200000   \n",
       "142                     1.0      74.000       33.63            2.200000   \n",
       "49                      5.0      91.481       40.30            2.270000   \n",
       "97                      0.1      37.740       25.50            1.480000   \n",
       "61                      5.0     207.000       90.00            2.300000   \n",
       "185                     5.0      69.000       34.60            2.000000   \n",
       "146                    10.0      74.000       33.63            2.200000   \n",
       "20                     10.0      42.000       23.30            1.800000   \n",
       "106                     1.0       2.755        1.90            1.450000   \n",
       "191                     5.0      69.000       34.60            2.000000   \n",
       "111                     0.5       2.755        1.90            1.450000   \n",
       "190                     5.0      69.000       34.60            2.000000   \n",
       "178                     5.0      66.000       33.00            2.000000   \n",
       "203                     5.0      43.700       19.70            2.220000   \n",
       "197                     5.0      69.000       34.60            2.000000   \n",
       "161                     5.0      71.000       32.27            2.200000   \n",
       "35                     10.0      29.000       19.30            1.500000   \n",
       "219                     5.0      43.700       19.70            2.220000   \n",
       "215                     5.0      43.700       19.70            2.220000   \n",
       "102                     2.0       2.755        1.90            1.450000   \n",
       "42                     10.0      76.000       44.70            1.700000   \n",
       "33                     10.0      15.400       10.30            1.500000   \n",
       "88                     15.0      25.650       19.00            1.350000   \n",
       "36                     10.0      29.000       19.30            1.500000   \n",
       "144                     3.0      74.000       33.63            2.200000   \n",
       "57                      5.0      91.481       40.30            2.270000   \n",
       "137                     3.5     163.200       68.00            2.400000   \n",
       "46                      5.0      91.481       40.30            2.270000   \n",
       "67                      5.0     207.000       90.00            2.300000   \n",
       "79                      2.0      48.960       24.00            2.040000   \n",
       "104                     2.0      21.384       10.80            1.980000   \n",
       "167                     5.0      71.000       32.27            2.200000   \n",
       "188                     5.0      69.000       34.60            2.000000   \n",
       "165                     5.0      71.000       32.27            2.200000   \n",
       "15                     10.0     270.000      117.40            2.300000   \n",
       "130                     3.5      70.000       35.00            2.000000   \n",
       "94                      1.0      37.740       25.50            1.480000   \n",
       "24                     10.0      76.000       44.70            1.700000   \n",
       "44                      5.0      91.481       40.30            2.270000   \n",
       "8                      10.0      15.400       10.30            1.500000   \n",
       "141                     0.5      74.000       33.63            2.200000   \n",
       "56                      5.0      91.481       40.30            2.270000   \n",
       "22                     10.0      76.000       44.70            1.700000   \n",
       "64                      5.0     207.000       90.00            2.300000   \n",
       "100                     0.1      37.740       25.50            1.480000   \n",
       "68                      5.0       4.400        4.00            1.100000   \n",
       "180                     5.0      66.000       33.00            2.000000   \n",
       "208                     5.0      43.700       19.70            2.220000   \n",
       "187                     5.0      69.000       34.60            2.000000   \n",
       "210                     5.0      43.700       19.70            2.220000   \n",
       "73                      5.0      24.120       13.40            1.800000   \n",
       "117                     4.0      47.700       24.00            1.987500   \n",
       "12                     10.0      42.000       23.30            1.800000   \n",
       "212                     5.0      43.700       19.70            2.220000   \n",
       "189                     5.0      69.000       34.60            2.000000   \n",
       "183                     5.0      69.000       34.60            2.000000   \n",
       "1                      10.0      20.000       11.10            1.800000   \n",
       "179                     5.0      66.000       33.00            2.000000   \n",
       "173                     5.0     104.000       47.27            2.200000   \n",
       "199                     5.0      43.700       19.70            2.220000   \n",
       "119                     4.0      47.700       24.00            1.987500   \n",
       "21                     10.0      52.000       22.60            2.300000   \n",
       "138                     3.5     163.200       68.00            2.400000   \n",
       "114                     0.5      60.750       27.00            2.250000   \n",
       "90                      1.0      37.740       25.50            1.480000   \n",
       "98                      0.1      37.740       25.50            1.480000   \n",
       "121                     4.0      47.700       24.00            1.987500   \n",
       "0                      10.0      15.400       10.30            1.500000   \n",
       "105                     2.0      60.750       27.00            2.250000   \n",
       "169                     5.0      71.000       32.27            2.200000   \n",
       "62                      5.0     207.000       90.00            2.300000   \n",
       "78                      5.0      43.540       31.10            1.400000   \n",
       "55                      5.0      91.481       40.30            2.270000   \n",
       "170                     5.0     104.000       47.27            2.200000   \n",
       "103                     2.0      11.592        5.60            2.070000   \n",
       "82                      2.0      48.960       24.00            2.040000   \n",
       "4                      10.0      42.000       23.30            1.800000   \n",
       "28                     10.0      29.000       19.30            1.500000   \n",
       "10                     10.0      22.000       13.80            1.600000   \n",
       "195                     5.0      69.000       34.60            2.000000   \n",
       "216                     5.0      43.700       19.70            2.220000   \n",
       "207                     5.0      43.700       19.70            2.220000   \n",
       "162                     5.0      71.000       32.27            2.200000   \n",
       "133                     3.5      70.000       35.00            2.000000   \n",
       "135                     3.5     163.200       68.00            2.400000   \n",
       "221                    10.0      43.700       19.70            2.220000   \n",
       "109                     1.0      60.750       27.00            2.250000   \n",
       "196                     5.0      69.000       34.60            2.000000   \n",
       "85                     15.0      18.600       18.60            1.000000   \n",
       "18                     10.0      22.000       13.80            1.600000   \n",
       "223                    10.0      43.700       19.70            2.220000   \n",
       "86                     15.0       6.608        5.60            1.180000   \n",
       "70                      5.0      13.920       11.60            1.200000   \n",
       "72                      5.0       5.980        4.60            1.300000   \n",
       "41                     10.0      76.000       44.70            1.700000   \n",
       "16                     10.0      15.400       10.30            1.500000   \n",
       "45                      5.0      91.481       40.30            2.270000   \n",
       "84                     15.0      11.300       11.30            1.000000   \n",
       "89                      1.0      37.740       25.50            1.480000   \n",
       "159                     5.0      77.500       26.00            2.950000   \n",
       "113                     0.5      21.384       10.80            1.980000   \n",
       "206                     5.0      43.700       19.70            2.220000   \n",
       "96                      1.0      37.740       25.50            1.480000   \n",
       "218                     5.0      43.700       19.70            2.220000   \n",
       "13                     10.0      52.000       22.60            2.300000   \n",
       "87                     15.0      20.424       13.80            1.480000   \n",
       "81                      2.0      48.960       24.00            2.040000   \n",
       "156                    10.0      74.000       33.63            2.200000   \n",
       "174                     5.0      41.000       20.50            2.000000   \n",
       "184                     5.0      69.000       34.60            2.000000   \n",
       "60                      5.0      91.481       40.30            2.270000   \n",
       "14                     10.0      76.000       44.70            1.700000   \n",
       "107                     1.0      11.592        5.60            2.070000   \n",
       "115                     4.0      47.700       24.00            1.987500   \n",
       "140                     6.0      74.000       33.63            2.200000   \n",
       "19                     10.0      29.000       19.30            1.500000   \n",
       "2                      10.0      22.000       13.80            1.600000   \n",
       "3                      10.0      29.000       19.30            1.500000   \n",
       "202                     5.0      43.700       19.70            2.220000   \n",
       "75                      5.0      62.050       36.50            1.700000   \n",
       "128                     3.5      23.000       12.70            1.811024   \n",
       "139                     3.5     163.200       68.00            2.400000   \n",
       "166                     5.0      71.000       32.27            2.200000   \n",
       "34                     10.0      15.400       10.30            1.500000   \n",
       "37                     10.0      29.000       19.30            1.500000   \n",
       "76                      5.0       4.480        3.20            1.400000   \n",
       "50                      5.0      91.481       40.30            2.270000   \n",
       "175                     5.0      41.000       20.50            2.000000   \n",
       "127                     3.5      23.000       12.70            1.811024   \n",
       "201                     5.0      43.700       19.70            2.220000   \n",
       "80                      2.0      48.960       24.00            2.040000   \n",
       "69                      5.0      10.800        9.00            1.200000   \n",
       "112                     0.5      11.592        5.60            2.070000   \n",
       "120                     4.0      47.700       24.00            1.987500   \n",
       "186                     5.0      69.000       34.60            2.000000   \n",
       "118                     4.0      47.700       24.00            1.987500   \n",
       "26                     10.0      76.000       44.70            1.700000   \n",
       "124                     3.5      47.700       24.00            1.987500   \n",
       "158                     5.0      51.000       24.00            2.100000   \n",
       "171                     5.0     104.000       47.27            2.200000   \n",
       "30                     10.0      29.000       19.30            1.500000   \n",
       "154                    15.0      74.000       33.63            2.200000   \n",
       "23                     10.0     270.000      117.40            2.300000   \n",
       "177                     5.0      41.000       20.50            2.000000   \n",
       "143                     2.0      74.000       33.63            2.200000   \n",
       "152                    10.0      74.000       33.63            2.200000   \n",
       "116                     4.0      47.700       24.00            1.987500   \n",
       "204                     5.0      43.700       19.70            2.220000   \n",
       "53                      5.0      91.481       40.30            2.270000   \n",
       "38                     10.0      15.400       10.30            1.500000   \n",
       "211                     5.0      43.700       19.70            2.220000   \n",
       "47                      5.0      91.481       40.30            2.270000   \n",
       "\n",
       "     solution_treatment  substrate_pretreatment  post_process  channel_width  \\\n",
       "39                    1                       1             1          10000   \n",
       "172                   1                       1             1             50   \n",
       "25                    1                       1             1          10000   \n",
       "151                   1                       1             0             50   \n",
       "157                   1                       1             0           1000   \n",
       "125                   1                       1             1           2000   \n",
       "17                    1                       1             1          10000   \n",
       "123                   1                       1             1           2000   \n",
       "122                   1                       1             1           2000   \n",
       "11                    1                       1             1          10000   \n",
       "192                   1                       1             1           2000   \n",
       "149                   1                       1             0             50   \n",
       "163                   1                       1             0             50   \n",
       "95                    0                       0             0           1000   \n",
       "108                   0                       0             0           9000   \n",
       "27                    1                       1             1          10000   \n",
       "145                   1                       1             0             50   \n",
       "142                   1                       1             0             50   \n",
       "49                    1                       0             0           2000   \n",
       "97                    0                       0             0           1000   \n",
       "61                    0                       0             0           2000   \n",
       "185                   1                       1             1           2000   \n",
       "146                   1                       1             0             50   \n",
       "20                    1                       1             1          10000   \n",
       "106                   0                       0             0           9000   \n",
       "191                   1                       1             1           2000   \n",
       "111                   1                       1             0           9000   \n",
       "190                   1                       1             1           2000   \n",
       "178                   1                       1             1             50   \n",
       "203                   1                       1             1           2000   \n",
       "197                   1                       1             1           2000   \n",
       "161                   1                       1             0             50   \n",
       "35                    1                       1             1          10000   \n",
       "219                   1                       1             1           2000   \n",
       "215                   1                       1             1           2000   \n",
       "102                   0                       1             0           9000   \n",
       "42                    1                       1             1          10000   \n",
       "33                    1                       1             1          10000   \n",
       "88                    0                       1             0         148500   \n",
       "36                    1                       1             1          10000   \n",
       "144                   1                       1             0             50   \n",
       "57                    1                       0             0           2000   \n",
       "137                   1                       1             0            500   \n",
       "46                    1                       0             0           2000   \n",
       "67                    1                       0             0           2000   \n",
       "79                    1                       1             0            800   \n",
       "104                   0                       1             0           9000   \n",
       "167                   1                       1             0             50   \n",
       "188                   1                       1             1           2000   \n",
       "165                   1                       1             0             50   \n",
       "15                    1                       1             1          10000   \n",
       "130                   1                       1             0            500   \n",
       "94                    0                       0             0           1000   \n",
       "24                    1                       1             1          10000   \n",
       "44                    0                       0             0           2000   \n",
       "8                     1                       1             1          10000   \n",
       "141                   1                       1             0             50   \n",
       "56                    1                       0             0           2000   \n",
       "22                    1                       1             1          10000   \n",
       "64                    1                       0             0           2000   \n",
       "100                   0                       1             0           1000   \n",
       "68                    0                       1             0          40000   \n",
       "180                   1                       1             1             50   \n",
       "208                   1                       1             1           2000   \n",
       "187                   1                       1             1           2000   \n",
       "210                   1                       1             1           2000   \n",
       "73                    0                       1             0          40000   \n",
       "117                   1                       1             0           2000   \n",
       "12                    1                       1             1          10000   \n",
       "212                   1                       1             1           2000   \n",
       "189                   1                       1             1           2000   \n",
       "183                   1                       1             1           2000   \n",
       "1                     1                       1             1          10000   \n",
       "179                   1                       1             1             50   \n",
       "173                   1                       1             1             50   \n",
       "199                   1                       1             1           2000   \n",
       "119                   1                       1             0           2000   \n",
       "21                    1                       1             1          10000   \n",
       "138                   1                       1             0            500   \n",
       "114                   1                       1             0           9000   \n",
       "90                    0                       0             0           1000   \n",
       "98                    0                       0             0           1000   \n",
       "121                   1                       1             0           2000   \n",
       "0                     1                       1             1          10000   \n",
       "105                   0                       1             0           9000   \n",
       "169                   1                       1             0             50   \n",
       "62                    1                       0             0           2000   \n",
       "78                    0                       1             0          40000   \n",
       "55                    0                       0             0           2000   \n",
       "170                   1                       1             1             50   \n",
       "103                   0                       1             0           9000   \n",
       "82                    1                       1             0            800   \n",
       "4                     1                       1             1          10000   \n",
       "28                    1                       1             1          10000   \n",
       "10                    1                       1             1          10000   \n",
       "195                   1                       1             1           2000   \n",
       "216                   1                       1             1           2000   \n",
       "207                   1                       1             1           2000   \n",
       "162                   1                       1             0             50   \n",
       "133                   1                       1             0            500   \n",
       "135                   1                       1             0            500   \n",
       "221                   1                       1             1           2000   \n",
       "109                   0                       0             0           9000   \n",
       "196                   1                       1             1           2000   \n",
       "85                    0                       1             0         148500   \n",
       "18                    1                       1             1          10000   \n",
       "223                   1                       1             1           2000   \n",
       "86                    0                       1             0         148500   \n",
       "70                    0                       1             0          40000   \n",
       "72                    0                       1             0          40000   \n",
       "41                    1                       1             1          10000   \n",
       "16                    1                       1             1          10000   \n",
       "45                    1                       0             0           2000   \n",
       "84                    0                       1             0         148500   \n",
       "89                    0                       0             0           1000   \n",
       "159                   1                       1             0           1000   \n",
       "113                   1                       1             0           9000   \n",
       "206                   1                       1             1           2000   \n",
       "96                    0                       1             0           1000   \n",
       "218                   1                       1             1           2000   \n",
       "13                    1                       1             1          10000   \n",
       "87                    0                       1             0         148500   \n",
       "81                    1                       1             0            800   \n",
       "156                   1                       1             0             50   \n",
       "174                   1                       1             1             50   \n",
       "184                   1                       1             1           2000   \n",
       "60                    1                       0             0           2000   \n",
       "14                    1                       1             1          10000   \n",
       "107                   0                       0             0           9000   \n",
       "115                   1                       1             0           2000   \n",
       "140                   1                       1             0             50   \n",
       "19                    1                       1             1          10000   \n",
       "2                     1                       1             1          10000   \n",
       "3                     1                       1             1          10000   \n",
       "202                   1                       1             1           2000   \n",
       "75                    0                       1             0          40000   \n",
       "128                   1                       1             1           2000   \n",
       "139                   1                       1             0            500   \n",
       "166                   1                       1             0             50   \n",
       "34                    1                       1             1          10000   \n",
       "37                    1                       1             1          10000   \n",
       "76                    0                       1             0          40000   \n",
       "50                    1                       0             0           2000   \n",
       "175                   1                       1             1             50   \n",
       "127                   1                       1             1           2000   \n",
       "201                   1                       1             1           2000   \n",
       "80                    1                       1             0            800   \n",
       "69                    0                       1             0          40000   \n",
       "112                   1                       1             0           9000   \n",
       "120                   1                       1             0           2000   \n",
       "186                   1                       1             1           2000   \n",
       "118                   1                       1             0           2000   \n",
       "26                    1                       1             1          10000   \n",
       "124                   1                       1             1           2000   \n",
       "158                   1                       1             0           1000   \n",
       "171                   1                       1             1             50   \n",
       "30                    1                       1             1          10000   \n",
       "154                   1                       1             0             50   \n",
       "23                    1                       1             1          10000   \n",
       "177                   1                       1             1             50   \n",
       "143                   1                       1             0             50   \n",
       "152                   1                       1             0             50   \n",
       "116                   1                       1             0           2000   \n",
       "204                   1                       1             1           2000   \n",
       "53                    1                       0             0           2000   \n",
       "38                    1                       1             1          10000   \n",
       "211                   1                       1             1           2000   \n",
       "47                    1                       0             0           2000   \n",
       "\n",
       "     channel_length  film_deposition_type_drop  film_deposition_type_spin  \\\n",
       "39              5.0                          0                          1   \n",
       "172          2000.0                          0                          0   \n",
       "25              5.0                          0                          1   \n",
       "151          2000.0                          0                          0   \n",
       "157            30.0                          0                          1   \n",
       "125            50.0                          0                          0   \n",
       "17             20.0                          1                          0   \n",
       "123            50.0                          0                          0   \n",
       "122            50.0                          0                          1   \n",
       "11             20.0                          0                          1   \n",
       "192            50.0                          0                          1   \n",
       "149          2000.0                          0                          0   \n",
       "163          2000.0                          0                          1   \n",
       "95              7.5                          0                          0   \n",
       "108            20.0                          0                          0   \n",
       "27             20.0                          0                          1   \n",
       "145          2000.0                          0                          0   \n",
       "142          2000.0                          0                          0   \n",
       "49             50.0                          0                          1   \n",
       "97              7.5                          1                          0   \n",
       "61             50.0                          0                          1   \n",
       "185            50.0                          0                          1   \n",
       "146          2000.0                          0                          0   \n",
       "20             20.0                          1                          0   \n",
       "106            20.0                          0                          0   \n",
       "191            50.0                          0                          1   \n",
       "111            20.0                          1                          0   \n",
       "190            50.0                          0                          1   \n",
       "178          2000.0                          0                          0   \n",
       "203            50.0                          0                          1   \n",
       "197            50.0                          0                          1   \n",
       "161          2000.0                          0                          1   \n",
       "35              2.0                          0                          1   \n",
       "219            50.0                          0                          1   \n",
       "215            50.0                          0                          1   \n",
       "102            20.0                          0                          1   \n",
       "42              5.0                          0                          1   \n",
       "33             10.0                          0                          1   \n",
       "88            100.0                          0                          0   \n",
       "36              5.0                          0                          1   \n",
       "144          2000.0                          0                          0   \n",
       "57             50.0                          0                          1   \n",
       "137            30.0                          0                          1   \n",
       "46             50.0                          0                          1   \n",
       "67             50.0                          0                          1   \n",
       "79             20.0                          1                          0   \n",
       "104            20.0                          0                          1   \n",
       "167          2000.0                          0                          1   \n",
       "188            50.0                          0                          1   \n",
       "165          2000.0                          0                          1   \n",
       "15             20.0                          0                          1   \n",
       "130            30.0                          0                          1   \n",
       "94              7.5                          0                          0   \n",
       "24              2.0                          0                          1   \n",
       "44             50.0                          0                          1   \n",
       "8              20.0                          0                          1   \n",
       "141          2000.0                          0                          0   \n",
       "56             50.0                          0                          1   \n",
       "22             20.0                          1                          0   \n",
       "64             50.0                          0                          1   \n",
       "100             7.5                          1                          0   \n",
       "68              5.0                          0                          1   \n",
       "180          2000.0                          0                          0   \n",
       "208            50.0                          0                          1   \n",
       "187            50.0                          0                          1   \n",
       "210            50.0                          0                          1   \n",
       "73              5.0                          0                          1   \n",
       "117            50.0                          0                          1   \n",
       "12             20.0                          0                          1   \n",
       "212            50.0                          0                          1   \n",
       "189            50.0                          0                          1   \n",
       "183            50.0                          0                          1   \n",
       "1              20.0                          0                          1   \n",
       "179          2000.0                          0                          0   \n",
       "173          2000.0                          0                          0   \n",
       "199            50.0                          0                          1   \n",
       "119            50.0                          0                          1   \n",
       "21             20.0                          1                          0   \n",
       "138            30.0                          0                          1   \n",
       "114            20.0                          1                          0   \n",
       "90              7.5                          0                          1   \n",
       "98              7.5                          1                          0   \n",
       "121            50.0                          0                          1   \n",
       "0              20.0                          0                          1   \n",
       "105            20.0                          0                          1   \n",
       "169          2000.0                          0                          1   \n",
       "62             50.0                          0                          1   \n",
       "78              5.0                          0                          1   \n",
       "55             50.0                          0                          1   \n",
       "170          2000.0                          0                          0   \n",
       "103            20.0                          0                          1   \n",
       "82             20.0                          1                          0   \n",
       "4              20.0                          0                          1   \n",
       "28              2.0                          0                          1   \n",
       "10             20.0                          0                          1   \n",
       "195            50.0                          0                          1   \n",
       "216            50.0                          0                          1   \n",
       "207            50.0                          0                          1   \n",
       "162          2000.0                          0                          1   \n",
       "133            30.0                          0                          1   \n",
       "135            30.0                          0                          1   \n",
       "221            50.0                          0                          1   \n",
       "109            20.0                          0                          0   \n",
       "196            50.0                          0                          1   \n",
       "85            100.0                          0                          1   \n",
       "18             20.0                          1                          0   \n",
       "223            50.0                          0                          1   \n",
       "86            100.0                          0                          1   \n",
       "70              5.0                          0                          1   \n",
       "72              5.0                          0                          1   \n",
       "41              2.0                          0                          1   \n",
       "16             20.0                          1                          0   \n",
       "45             50.0                          0                          1   \n",
       "84            100.0                          0                          1   \n",
       "89              7.5                          0                          1   \n",
       "159            30.0                          0                          1   \n",
       "113            20.0                          1                          0   \n",
       "206            50.0                          0                          1   \n",
       "96              7.5                          0                          0   \n",
       "218            50.0                          0                          1   \n",
       "13             20.0                          0                          1   \n",
       "87            100.0                          0                          0   \n",
       "81             20.0                          1                          0   \n",
       "156          2000.0                          0                          0   \n",
       "174          2000.0                          0                          0   \n",
       "184            50.0                          0                          1   \n",
       "60             50.0                          0                          1   \n",
       "14             20.0                          0                          1   \n",
       "107            20.0                          0                          0   \n",
       "115            50.0                          0                          1   \n",
       "140          2000.0                          0                          0   \n",
       "19             20.0                          1                          0   \n",
       "2              20.0                          0                          1   \n",
       "3              20.0                          0                          1   \n",
       "202            50.0                          0                          1   \n",
       "75              5.0                          0                          1   \n",
       "128            50.0                          0                          1   \n",
       "139            30.0                          0                          1   \n",
       "166          2000.0                          0                          1   \n",
       "34             20.0                          0                          1   \n",
       "37             10.0                          0                          1   \n",
       "76              5.0                          0                          1   \n",
       "50             50.0                          0                          1   \n",
       "175          2000.0                          0                          0   \n",
       "127            50.0                          0                          0   \n",
       "201            50.0                          0                          1   \n",
       "80             20.0                          1                          0   \n",
       "69              5.0                          0                          1   \n",
       "112            20.0                          1                          0   \n",
       "120            50.0                          0                          1   \n",
       "186            50.0                          0                          1   \n",
       "118            50.0                          0                          1   \n",
       "26             10.0                          0                          1   \n",
       "124            50.0                          0                          1   \n",
       "158            30.0                          0                          1   \n",
       "171          2000.0                          0                          0   \n",
       "30             10.0                          0                          1   \n",
       "154          2000.0                          0                          0   \n",
       "23             20.0                          1                          0   \n",
       "177          2000.0                          0                          0   \n",
       "143          2000.0                          0                          0   \n",
       "152          2000.0                          0                          0   \n",
       "116            50.0                          0                          1   \n",
       "204            50.0                          0                          1   \n",
       "53             50.0                          0                          1   \n",
       "38              2.0                          0                          1   \n",
       "211            50.0                          0                          1   \n",
       "47             50.0                          0                          1   \n",
       "\n",
       "     dielectric_material_SiO2  electrode_configuration_BGBC  \\\n",
       "39                          1                             1   \n",
       "172                         0                             1   \n",
       "25                          1                             1   \n",
       "151                         0                             1   \n",
       "157                         0                             0   \n",
       "125                         0                             1   \n",
       "17                          1                             1   \n",
       "123                         0                             1   \n",
       "122                         0                             1   \n",
       "11                          1                             1   \n",
       "192                         0                             1   \n",
       "149                         0                             1   \n",
       "163                         0                             1   \n",
       "95                          1                             1   \n",
       "108                         1                             1   \n",
       "27                          1                             1   \n",
       "145                         0                             1   \n",
       "142                         0                             1   \n",
       "49                          1                             1   \n",
       "97                          1                             1   \n",
       "61                          1                             1   \n",
       "185                         0                             1   \n",
       "146                         0                             1   \n",
       "20                          1                             1   \n",
       "106                         1                             1   \n",
       "191                         0                             1   \n",
       "111                         1                             1   \n",
       "190                         0                             1   \n",
       "178                         0                             1   \n",
       "203                         0                             1   \n",
       "197                         0                             1   \n",
       "161                         0                             1   \n",
       "35                          1                             1   \n",
       "219                         0                             1   \n",
       "215                         0                             1   \n",
       "102                         1                             1   \n",
       "42                          1                             1   \n",
       "33                          1                             1   \n",
       "88                          1                             0   \n",
       "36                          1                             1   \n",
       "144                         0                             1   \n",
       "57                          1                             1   \n",
       "137                         0                             0   \n",
       "46                          1                             1   \n",
       "67                          1                             1   \n",
       "79                          1                             1   \n",
       "104                         1                             1   \n",
       "167                         0                             1   \n",
       "188                         0                             1   \n",
       "165                         0                             1   \n",
       "15                          1                             1   \n",
       "130                         0                             0   \n",
       "94                          1                             1   \n",
       "24                          1                             1   \n",
       "44                          1                             1   \n",
       "8                           1                             1   \n",
       "141                         0                             1   \n",
       "56                          1                             1   \n",
       "22                          1                             1   \n",
       "64                          1                             1   \n",
       "100                         1                             1   \n",
       "68                          1                             1   \n",
       "180                         0                             1   \n",
       "208                         0                             1   \n",
       "187                         0                             1   \n",
       "210                         0                             1   \n",
       "73                          1                             1   \n",
       "117                         0                             1   \n",
       "12                          1                             1   \n",
       "212                         0                             1   \n",
       "189                         0                             1   \n",
       "183                         0                             1   \n",
       "1                           1                             1   \n",
       "179                         0                             1   \n",
       "173                         0                             1   \n",
       "199                         0                             1   \n",
       "119                         0                             1   \n",
       "21                          1                             1   \n",
       "138                         0                             0   \n",
       "114                         1                             1   \n",
       "90                          1                             1   \n",
       "98                          1                             1   \n",
       "121                         0                             1   \n",
       "0                           1                             1   \n",
       "105                         1                             1   \n",
       "169                         0                             1   \n",
       "62                          1                             1   \n",
       "78                          1                             1   \n",
       "55                          1                             1   \n",
       "170                         0                             1   \n",
       "103                         1                             1   \n",
       "82                          1                             1   \n",
       "4                           1                             1   \n",
       "28                          1                             1   \n",
       "10                          1                             1   \n",
       "195                         0                             1   \n",
       "216                         0                             1   \n",
       "207                         0                             1   \n",
       "162                         0                             1   \n",
       "133                         0                             0   \n",
       "135                         0                             0   \n",
       "221                         0                             1   \n",
       "109                         1                             1   \n",
       "196                         0                             1   \n",
       "85                          1                             0   \n",
       "18                          1                             1   \n",
       "223                         0                             1   \n",
       "86                          1                             0   \n",
       "70                          1                             1   \n",
       "72                          1                             1   \n",
       "41                          1                             1   \n",
       "16                          1                             1   \n",
       "45                          1                             1   \n",
       "84                          1                             0   \n",
       "89                          1                             1   \n",
       "159                         0                             0   \n",
       "113                         1                             1   \n",
       "206                         0                             1   \n",
       "96                          1                             1   \n",
       "218                         0                             1   \n",
       "13                          1                             1   \n",
       "87                          1                             0   \n",
       "81                          1                             1   \n",
       "156                         0                             1   \n",
       "174                         0                             1   \n",
       "184                         0                             1   \n",
       "60                          1                             1   \n",
       "14                          1                             1   \n",
       "107                         1                             1   \n",
       "115                         0                             1   \n",
       "140                         0                             1   \n",
       "19                          1                             1   \n",
       "2                           1                             1   \n",
       "3                           1                             1   \n",
       "202                         0                             1   \n",
       "75                          1                             1   \n",
       "128                         0                             1   \n",
       "139                         0                             0   \n",
       "166                         0                             1   \n",
       "34                          1                             1   \n",
       "37                          1                             1   \n",
       "76                          1                             1   \n",
       "50                          1                             1   \n",
       "175                         0                             1   \n",
       "127                         0                             1   \n",
       "201                         0                             1   \n",
       "80                          1                             1   \n",
       "69                          1                             1   \n",
       "112                         1                             1   \n",
       "120                         0                             1   \n",
       "186                         0                             1   \n",
       "118                         0                             1   \n",
       "26                          1                             1   \n",
       "124                         0                             1   \n",
       "158                         0                             0   \n",
       "171                         0                             1   \n",
       "30                          1                             1   \n",
       "154                         0                             1   \n",
       "23                          1                             1   \n",
       "177                         0                             1   \n",
       "143                         0                             1   \n",
       "152                         0                             1   \n",
       "116                         0                             1   \n",
       "204                         0                             1   \n",
       "53                          1                             1   \n",
       "38                          1                             1   \n",
       "211                         0                             1   \n",
       "47                          1                             1   \n",
       "\n",
       "     electrode_configuration_BGTC  film_deposition_type_MGC  \\\n",
       "39                              0                         0   \n",
       "172                             0                         1   \n",
       "25                              0                         0   \n",
       "151                             0                         1   \n",
       "157                             0                         0   \n",
       "125                             0                         1   \n",
       "17                              0                         0   \n",
       "123                             0                         1   \n",
       "122                             0                         0   \n",
       "11                              0                         0   \n",
       "192                             0                         0   \n",
       "149                             0                         1   \n",
       "163                             0                         0   \n",
       "95                              0                         1   \n",
       "108                             0                         1   \n",
       "27                              0                         0   \n",
       "145                             0                         1   \n",
       "142                             0                         1   \n",
       "49                              0                         0   \n",
       "97                              0                         0   \n",
       "61                              0                         0   \n",
       "185                             0                         0   \n",
       "146                             0                         1   \n",
       "20                              0                         0   \n",
       "106                             0                         1   \n",
       "191                             0                         0   \n",
       "111                             0                         0   \n",
       "190                             0                         0   \n",
       "178                             0                         1   \n",
       "203                             0                         0   \n",
       "197                             0                         0   \n",
       "161                             0                         0   \n",
       "35                              0                         0   \n",
       "219                             0                         0   \n",
       "215                             0                         0   \n",
       "102                             0                         0   \n",
       "42                              0                         0   \n",
       "33                              0                         0   \n",
       "88                              1                         1   \n",
       "36                              0                         0   \n",
       "144                             0                         1   \n",
       "57                              0                         0   \n",
       "137                             1                         0   \n",
       "46                              0                         0   \n",
       "67                              0                         0   \n",
       "79                              0                         0   \n",
       "104                             0                         0   \n",
       "167                             0                         0   \n",
       "188                             0                         0   \n",
       "165                             0                         0   \n",
       "15                              0                         0   \n",
       "130                             1                         0   \n",
       "94                              0                         1   \n",
       "24                              0                         0   \n",
       "44                              0                         0   \n",
       "8                               0                         0   \n",
       "141                             0                         1   \n",
       "56                              0                         0   \n",
       "22                              0                         0   \n",
       "64                              0                         0   \n",
       "100                             0                         0   \n",
       "68                              0                         0   \n",
       "180                             0                         1   \n",
       "208                             0                         0   \n",
       "187                             0                         0   \n",
       "210                             0                         0   \n",
       "73                              0                         0   \n",
       "117                             0                         0   \n",
       "12                              0                         0   \n",
       "212                             0                         0   \n",
       "189                             0                         0   \n",
       "183                             0                         0   \n",
       "1                               0                         0   \n",
       "179                             0                         1   \n",
       "173                             0                         1   \n",
       "199                             0                         0   \n",
       "119                             0                         0   \n",
       "21                              0                         0   \n",
       "138                             1                         0   \n",
       "114                             0                         0   \n",
       "90                              0                         0   \n",
       "98                              0                         0   \n",
       "121                             0                         0   \n",
       "0                               0                         0   \n",
       "105                             0                         0   \n",
       "169                             0                         0   \n",
       "62                              0                         0   \n",
       "78                              0                         0   \n",
       "55                              0                         0   \n",
       "170                             0                         1   \n",
       "103                             0                         0   \n",
       "82                              0                         0   \n",
       "4                               0                         0   \n",
       "28                              0                         0   \n",
       "10                              0                         0   \n",
       "195                             0                         0   \n",
       "216                             0                         0   \n",
       "207                             0                         0   \n",
       "162                             0                         0   \n",
       "133                             1                         0   \n",
       "135                             1                         0   \n",
       "221                             0                         0   \n",
       "109                             0                         1   \n",
       "196                             0                         0   \n",
       "85                              1                         0   \n",
       "18                              0                         0   \n",
       "223                             0                         0   \n",
       "86                              1                         0   \n",
       "70                              0                         0   \n",
       "72                              0                         0   \n",
       "41                              0                         0   \n",
       "16                              0                         0   \n",
       "45                              0                         0   \n",
       "84                              1                         0   \n",
       "89                              0                         0   \n",
       "159                             0                         0   \n",
       "113                             0                         0   \n",
       "206                             0                         0   \n",
       "96                              0                         1   \n",
       "218                             0                         0   \n",
       "13                              0                         0   \n",
       "87                              1                         1   \n",
       "81                              0                         0   \n",
       "156                             0                         1   \n",
       "174                             0                         1   \n",
       "184                             0                         0   \n",
       "60                              0                         0   \n",
       "14                              0                         0   \n",
       "107                             0                         1   \n",
       "115                             0                         0   \n",
       "140                             0                         1   \n",
       "19                              0                         0   \n",
       "2                               0                         0   \n",
       "3                               0                         0   \n",
       "202                             0                         0   \n",
       "75                              0                         0   \n",
       "128                             0                         0   \n",
       "139                             1                         0   \n",
       "166                             0                         0   \n",
       "34                              0                         0   \n",
       "37                              0                         0   \n",
       "76                              0                         0   \n",
       "50                              0                         0   \n",
       "175                             0                         1   \n",
       "127                             0                         1   \n",
       "201                             0                         0   \n",
       "80                              0                         0   \n",
       "69                              0                         0   \n",
       "112                             0                         0   \n",
       "120                             0                         0   \n",
       "186                             0                         0   \n",
       "118                             0                         0   \n",
       "26                              0                         0   \n",
       "124                             0                         0   \n",
       "158                             0                         0   \n",
       "171                             0                         1   \n",
       "30                              0                         0   \n",
       "154                             0                         1   \n",
       "23                              0                         0   \n",
       "177                             0                         1   \n",
       "143                             0                         1   \n",
       "152                             0                         1   \n",
       "116                             0                         0   \n",
       "204                             0                         0   \n",
       "53                              0                         0   \n",
       "38                              0                         0   \n",
       "211                             0                         0   \n",
       "47                              0                         0   \n",
       "\n",
       "     solvent_boiling_point  hole_mobility  \n",
       "39                     213       0.003810  \n",
       "172                     62       0.064400  \n",
       "25                      62       0.005440  \n",
       "151                     62       1.651400  \n",
       "157                     62       0.058170  \n",
       "125                     62       0.003450  \n",
       "17                      62       0.018700  \n",
       "123                     62       0.049400  \n",
       "122                     62       0.027300  \n",
       "11                     213       0.063100  \n",
       "192                     62       0.023000  \n",
       "149                     62       0.008115  \n",
       "163                     62       0.111300  \n",
       "95                      62       0.085000  \n",
       "108                     62       0.006870  \n",
       "27                      62       0.002960  \n",
       "145                     62       0.004459  \n",
       "142                     62       0.002300  \n",
       "49                      62       0.008920  \n",
       "97                     213       0.000150  \n",
       "61                      62       0.035220  \n",
       "185                     62       0.022000  \n",
       "146                     62       0.006890  \n",
       "20                      62       0.061200  \n",
       "106                     62       0.000114  \n",
       "191                     62       0.019000  \n",
       "111                     62       0.000034  \n",
       "190                     62       0.042000  \n",
       "178                     62       0.090670  \n",
       "203                     62       0.085000  \n",
       "197                     62       0.021000  \n",
       "161                     62       0.086700  \n",
       "35                     213       0.077000  \n",
       "219                     62       0.082400  \n",
       "215                     62       0.015700  \n",
       "102                     62       0.000049  \n",
       "42                     213       0.135000  \n",
       "33                      62       0.009930  \n",
       "88                      62       0.010600  \n",
       "36                     213       0.068000  \n",
       "144                     62       0.004360  \n",
       "57                     132       0.000426  \n",
       "137                    111       0.008030  \n",
       "46                      62       0.010700  \n",
       "67                      62       0.011230  \n",
       "79                     138       0.023900  \n",
       "104                     62       0.003990  \n",
       "167                     62       0.109100  \n",
       "188                     62       0.035000  \n",
       "165                     62       0.101600  \n",
       "15                     213       0.143500  \n",
       "130                    111       0.003680  \n",
       "94                     138       0.015000  \n",
       "24                      62       0.014500  \n",
       "44                      62       0.004230  \n",
       "8                      213       0.001700  \n",
       "141                     62       0.002720  \n",
       "56                     132       0.000413  \n",
       "22                      62       0.065200  \n",
       "64                      62       0.103430  \n",
       "100                     62       0.002300  \n",
       "68                      62       0.000005  \n",
       "180                     62       0.104200  \n",
       "208                     62       0.094000  \n",
       "187                     62       0.020000  \n",
       "210                     62       0.111000  \n",
       "73                      62       0.000537  \n",
       "117                     62       0.029400  \n",
       "12                     213       0.077500  \n",
       "212                     62       0.050000  \n",
       "189                     62       0.044000  \n",
       "183                     62       0.004000  \n",
       "1                       62       0.004960  \n",
       "179                     62       0.135000  \n",
       "173                     62       0.048700  \n",
       "199                     62       0.023000  \n",
       "119                     62       0.030600  \n",
       "21                      62       0.066200  \n",
       "138                    111       0.009530  \n",
       "114                     62       0.036400  \n",
       "90                     138       0.015000  \n",
       "98                     138       0.001400  \n",
       "121                     62       0.015300  \n",
       "0                       62       0.007600  \n",
       "105                     62       0.022900  \n",
       "169                     62       0.013000  \n",
       "62                      62       0.051820  \n",
       "78                      62       0.007670  \n",
       "55                     132       0.000556  \n",
       "170                     62       0.138900  \n",
       "103                     62       0.000551  \n",
       "82                      80       0.014500  \n",
       "4                       62       0.005290  \n",
       "28                      62       0.022900  \n",
       "10                     213       0.030500  \n",
       "195                     62       0.018000  \n",
       "216                     62       0.046700  \n",
       "207                     62       0.072000  \n",
       "162                     62       0.149000  \n",
       "133                    111       0.000871  \n",
       "135                    111       0.001020  \n",
       "221                     62       0.018000  \n",
       "109                     62       0.017000  \n",
       "196                     62       0.026000  \n",
       "85                      62       0.016000  \n",
       "18                      62       0.027300  \n",
       "223                     62       0.079000  \n",
       "86                      62       0.000001  \n",
       "70                      62       0.000735  \n",
       "72                      62       0.000016  \n",
       "41                     213       0.137800  \n",
       "16                      62       0.009940  \n",
       "45                      62       0.007930  \n",
       "84                      62       0.019000  \n",
       "89                     213       0.023000  \n",
       "159                    132       0.000000  \n",
       "113                     62       0.004140  \n",
       "206                     62       0.120000  \n",
       "96                      62       0.150000  \n",
       "218                     62       0.065200  \n",
       "13                     213       0.098200  \n",
       "87                      62       0.000609  \n",
       "81                     145       0.033500  \n",
       "156                     62       0.120523  \n",
       "174                     62       0.042790  \n",
       "184                     62       0.010000  \n",
       "60                     132       0.000377  \n",
       "14                     213       0.122600  \n",
       "107                     62       0.000912  \n",
       "115                     62       0.000202  \n",
       "140                     62       0.008397  \n",
       "19                      62       0.053900  \n",
       "2                       62       0.009480  \n",
       "3                       62       0.012000  \n",
       "202                     62       0.091000  \n",
       "75                      62       0.009080  \n",
       "128                     62       0.020500  \n",
       "139                    111       0.009150  \n",
       "166                     62       0.090500  \n",
       "34                      62       0.007570  \n",
       "37                     213       0.065000  \n",
       "76                      62       0.000002  \n",
       "50                      62       0.004700  \n",
       "175                     62       0.101700  \n",
       "127                     62       0.043600  \n",
       "201                     62       0.081000  \n",
       "80                     111       0.030500  \n",
       "69                      62       0.000356  \n",
       "112                     62       0.000709  \n",
       "120                     62       0.036100  \n",
       "186                     62       0.020000  \n",
       "118                     62       0.050600  \n",
       "26                      62       0.003630  \n",
       "124                     62       0.001160  \n",
       "158                     62       0.026300  \n",
       "171                     62       0.108900  \n",
       "30                      62       0.014000  \n",
       "154                     62       0.150000  \n",
       "23                      62       0.081300  \n",
       "177                     62       0.064800  \n",
       "143                     62       0.002395  \n",
       "152                     62       0.175180  \n",
       "116                     62       0.007190  \n",
       "204                     62       0.090000  \n",
       "53                      62       0.002280  \n",
       "38                     213       0.007620  \n",
       "211                     62       0.051000  \n",
       "47                      62       0.016500  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.concat((df_X_train,df_Y_train),axis=1)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>solution_concentration</th>\n",
       "      <th>polymer_mw</th>\n",
       "      <th>polymer_mn</th>\n",
       "      <th>polymer_dispersity</th>\n",
       "      <th>solution_treatment</th>\n",
       "      <th>substrate_pretreatment</th>\n",
       "      <th>post_process</th>\n",
       "      <th>channel_width</th>\n",
       "      <th>channel_length</th>\n",
       "      <th>film_deposition_type_drop</th>\n",
       "      <th>film_deposition_type_spin</th>\n",
       "      <th>dielectric_material_SiO2</th>\n",
       "      <th>electrode_configuration_BGBC</th>\n",
       "      <th>electrode_configuration_BGTC</th>\n",
       "      <th>film_deposition_type_MGC</th>\n",
       "      <th>solvent_boiling_point</th>\n",
       "      <th>hole_mobility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.0</td>\n",
       "      <td>52.0000</td>\n",
       "      <td>22.60</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.005540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.002990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>213</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.0000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.054680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>12.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>5.0</td>\n",
       "      <td>129.6000</td>\n",
       "      <td>54.00</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>15.0</td>\n",
       "      <td>5.1000</td>\n",
       "      <td>5.10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.058400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>11.10</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.001490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.002140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.1</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.0</td>\n",
       "      <td>270.0000</td>\n",
       "      <td>117.40</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>5.0</td>\n",
       "      <td>41.0000</td>\n",
       "      <td>20.50</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.083050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>5.0</td>\n",
       "      <td>50.7000</td>\n",
       "      <td>33.80</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.007480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.018300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>3.5</td>\n",
       "      <td>70.0000</td>\n",
       "      <td>35.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.001350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.012700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.0000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.020930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>5.0</td>\n",
       "      <td>71.0000</td>\n",
       "      <td>32.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.017100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>3.5</td>\n",
       "      <td>70.0000</td>\n",
       "      <td>35.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.000629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.1</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>5.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.103900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>3.5</td>\n",
       "      <td>23.0000</td>\n",
       "      <td>12.70</td>\n",
       "      <td>1.811024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.0000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.310000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.002550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.004060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>5.0</td>\n",
       "      <td>56.6200</td>\n",
       "      <td>29.80</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.004190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0.000309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>10.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>3.5</td>\n",
       "      <td>163.2000</td>\n",
       "      <td>68.00</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.003050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>5.0</td>\n",
       "      <td>19.4400</td>\n",
       "      <td>16.20</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.007850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.125300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>5.0</td>\n",
       "      <td>66.0000</td>\n",
       "      <td>33.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>5.0</td>\n",
       "      <td>71.0000</td>\n",
       "      <td>32.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>5.0</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>34.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>3.5</td>\n",
       "      <td>23.0000</td>\n",
       "      <td>12.70</td>\n",
       "      <td>1.811024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.014800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>10.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.011071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>5.0</td>\n",
       "      <td>71.0000</td>\n",
       "      <td>32.27</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.089400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0.000254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1.0</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0.006050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.002790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.7000</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.101000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>3.5</td>\n",
       "      <td>70.0000</td>\n",
       "      <td>35.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.001020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     solution_concentration  polymer_mw  polymer_mn  polymer_dispersity  \\\n",
       "5                      10.0     52.0000       22.60            2.300000   \n",
       "198                     5.0     69.0000       34.60            2.000000   \n",
       "6                      10.0     76.0000       44.70            1.700000   \n",
       "93                      1.0     37.7400       25.50            1.480000   \n",
       "63                      5.0    207.0000       90.00            2.300000   \n",
       "153                    12.0     74.0000       33.63            2.200000   \n",
       "182                     5.0    129.6000       54.00            2.400000   \n",
       "83                     15.0      5.1000        5.10            1.000000   \n",
       "217                     5.0     43.7000       19.70            2.220000   \n",
       "9                      10.0     20.0000       11.10            1.800000   \n",
       "52                      5.0     91.4810       40.30            2.270000   \n",
       "99                      0.1     37.7400       25.50            1.480000   \n",
       "7                      10.0    270.0000      117.40            2.300000   \n",
       "193                     5.0     69.0000       34.60            2.000000   \n",
       "176                     5.0     41.0000       20.50            2.000000   \n",
       "71                      5.0     50.7000       33.80            1.500000   \n",
       "29                     10.0     29.0000       19.30            1.500000   \n",
       "131                     3.5     70.0000       35.00            2.000000   \n",
       "148                     2.0     74.0000       33.63            2.200000   \n",
       "200                     5.0     43.7000       19.70            2.220000   \n",
       "66                      5.0    207.0000       90.00            2.300000   \n",
       "92                      1.0     37.7400       25.50            1.480000   \n",
       "91                      1.0     37.7400       25.50            1.480000   \n",
       "164                     5.0     71.0000       32.27            2.200000   \n",
       "31                     10.0     15.4000       10.30            1.500000   \n",
       "48                      5.0     91.4810       40.30            2.270000   \n",
       "132                     3.5     70.0000       35.00            2.000000   \n",
       "101                     0.1     37.7400       25.50            1.480000   \n",
       "150                     5.0     74.0000       33.63            2.200000   \n",
       "126                     3.5     23.0000       12.70            1.811024   \n",
       "220                     5.0     43.7000       19.70            2.220000   \n",
       "65                      5.0    207.0000       90.00            2.300000   \n",
       "110                     0.5      1.4279        1.09            1.310000   \n",
       "40                     10.0     15.4000       10.30            1.500000   \n",
       "51                      5.0     91.4810       40.30            2.270000   \n",
       "74                      5.0     56.6200       29.80            1.900000   \n",
       "59                      5.0     91.4810       40.30            2.270000   \n",
       "222                    10.0     43.7000       19.70            2.220000   \n",
       "136                     3.5    163.2000       68.00            2.400000   \n",
       "77                      5.0     19.4400       16.20            1.200000   \n",
       "43                     10.0     76.0000       44.70            1.700000   \n",
       "181                     5.0     66.0000       33.00            2.000000   \n",
       "160                     5.0     71.0000       32.27            2.200000   \n",
       "194                     5.0     69.0000       34.60            2.000000   \n",
       "205                     5.0     43.7000       19.70            2.220000   \n",
       "129                     3.5     23.0000       12.70            1.811024   \n",
       "155                    10.0     74.0000       33.63            2.200000   \n",
       "168                     5.0     71.0000       32.27            2.200000   \n",
       "58                      5.0     91.4810       40.30            2.270000   \n",
       "214                     5.0     43.7000       19.70            2.220000   \n",
       "213                     5.0     43.7000       19.70            2.220000   \n",
       "147                     1.0     74.0000       33.63            2.200000   \n",
       "32                     10.0     15.4000       10.30            1.500000   \n",
       "54                      5.0     91.4810       40.30            2.270000   \n",
       "209                     5.0     43.7000       19.70            2.220000   \n",
       "134                     3.5     70.0000       35.00            2.000000   \n",
       "\n",
       "     solution_treatment  substrate_pretreatment  post_process  channel_width  \\\n",
       "5                     1                       1             1          10000   \n",
       "198                   1                       1             1           2000   \n",
       "6                     1                       1             1          10000   \n",
       "93                    0                       0             0           1000   \n",
       "63                    1                       0             0           2000   \n",
       "153                   1                       1             0             50   \n",
       "182                   0                       1             1           1000   \n",
       "83                    0                       1             0         148500   \n",
       "217                   1                       1             1           2000   \n",
       "9                     1                       1             1          10000   \n",
       "52                    1                       0             0           2000   \n",
       "99                    0                       0             0           1000   \n",
       "7                     1                       1             1          10000   \n",
       "193                   1                       1             1           2000   \n",
       "176                   1                       1             1             50   \n",
       "71                    0                       1             0          40000   \n",
       "29                    1                       1             1          10000   \n",
       "131                   1                       1             0            500   \n",
       "148                   1                       1             0             50   \n",
       "200                   1                       1             1           2000   \n",
       "66                    1                       0             0           2000   \n",
       "92                    0                       1             0           1000   \n",
       "91                    0                       0             0           1000   \n",
       "164                   1                       1             0             50   \n",
       "31                    1                       1             1          10000   \n",
       "48                    1                       0             0           2000   \n",
       "132                   1                       1             0            500   \n",
       "101                   0                       1             1           1000   \n",
       "150                   1                       1             0             50   \n",
       "126                   1                       1             1           2000   \n",
       "220                   1                       1             1           2000   \n",
       "65                    1                       0             0           2000   \n",
       "110                   1                       1             0           9000   \n",
       "40                    1                       1             1          10000   \n",
       "51                    1                       0             0           2000   \n",
       "74                    0                       1             0          40000   \n",
       "59                    1                       0             0           2000   \n",
       "222                   1                       1             1           2000   \n",
       "136                   1                       1             0            500   \n",
       "77                    0                       1             0          40000   \n",
       "43                    1                       1             1          10000   \n",
       "181                   1                       1             1             50   \n",
       "160                   1                       1             0             50   \n",
       "194                   1                       1             1           2000   \n",
       "205                   1                       1             1           2000   \n",
       "129                   1                       1             1           2000   \n",
       "155                   1                       1             0             50   \n",
       "168                   1                       1             0             50   \n",
       "58                    1                       0             0           2000   \n",
       "214                   1                       1             1           2000   \n",
       "213                   1                       1             1           2000   \n",
       "147                   1                       1             0             50   \n",
       "32                    1                       1             1          10000   \n",
       "54                    1                       0             0           2000   \n",
       "209                   1                       1             1           2000   \n",
       "134                   1                       1             0            500   \n",
       "\n",
       "     channel_length  film_deposition_type_drop  film_deposition_type_spin  \\\n",
       "5              20.0                          0                          1   \n",
       "198            50.0                          0                          1   \n",
       "6              20.0                          0                          1   \n",
       "93              7.5                          0                          0   \n",
       "63             50.0                          0                          1   \n",
       "153          2000.0                          0                          0   \n",
       "182           100.0                          0                          1   \n",
       "83            100.0                          0                          1   \n",
       "217            50.0                          0                          1   \n",
       "9              20.0                          0                          1   \n",
       "52             50.0                          0                          1   \n",
       "99              7.5                          1                          0   \n",
       "7              20.0                          0                          1   \n",
       "193            50.0                          0                          1   \n",
       "176          2000.0                          0                          0   \n",
       "71              5.0                          0                          1   \n",
       "29              5.0                          0                          1   \n",
       "131            30.0                          0                          1   \n",
       "148          2000.0                          0                          0   \n",
       "200            50.0                          0                          1   \n",
       "66             50.0                          0                          1   \n",
       "92              7.5                          0                          1   \n",
       "91              7.5                          0                          1   \n",
       "164          2000.0                          0                          1   \n",
       "31              2.0                          0                          1   \n",
       "48             50.0                          0                          1   \n",
       "132            30.0                          0                          1   \n",
       "101             7.5                          1                          0   \n",
       "150          2000.0                          0                          0   \n",
       "126            50.0                          0                          1   \n",
       "220            50.0                          0                          1   \n",
       "65             50.0                          0                          1   \n",
       "110            20.0                          1                          0   \n",
       "40             10.0                          0                          1   \n",
       "51             50.0                          0                          1   \n",
       "74              5.0                          0                          1   \n",
       "59             50.0                          0                          1   \n",
       "222            50.0                          0                          1   \n",
       "136            30.0                          0                          1   \n",
       "77              5.0                          0                          1   \n",
       "43             10.0                          0                          1   \n",
       "181          2000.0                          0                          0   \n",
       "160          2000.0                          0                          1   \n",
       "194            50.0                          0                          1   \n",
       "205            50.0                          0                          1   \n",
       "129            50.0                          0                          0   \n",
       "155          2000.0                          0                          0   \n",
       "168          2000.0                          0                          1   \n",
       "58             50.0                          0                          1   \n",
       "214            50.0                          0                          1   \n",
       "213            50.0                          0                          1   \n",
       "147          2000.0                          0                          0   \n",
       "32              5.0                          0                          1   \n",
       "54             50.0                          0                          1   \n",
       "209            50.0                          0                          1   \n",
       "134            30.0                          0                          1   \n",
       "\n",
       "     dielectric_material_SiO2  electrode_configuration_BGBC  \\\n",
       "5                           1                             1   \n",
       "198                         0                             1   \n",
       "6                           1                             1   \n",
       "93                          1                             1   \n",
       "63                          1                             1   \n",
       "153                         0                             1   \n",
       "182                         0                             0   \n",
       "83                          1                             0   \n",
       "217                         0                             1   \n",
       "9                           1                             1   \n",
       "52                          1                             1   \n",
       "99                          1                             1   \n",
       "7                           1                             1   \n",
       "193                         0                             1   \n",
       "176                         0                             1   \n",
       "71                          1                             1   \n",
       "29                          1                             1   \n",
       "131                         0                             0   \n",
       "148                         0                             1   \n",
       "200                         0                             1   \n",
       "66                          1                             1   \n",
       "92                          1                             1   \n",
       "91                          1                             1   \n",
       "164                         0                             1   \n",
       "31                          1                             1   \n",
       "48                          1                             1   \n",
       "132                         0                             0   \n",
       "101                         1                             1   \n",
       "150                         0                             1   \n",
       "126                         0                             1   \n",
       "220                         0                             1   \n",
       "65                          1                             1   \n",
       "110                         1                             1   \n",
       "40                          1                             1   \n",
       "51                          1                             1   \n",
       "74                          1                             1   \n",
       "59                          1                             1   \n",
       "222                         0                             1   \n",
       "136                         0                             0   \n",
       "77                          1                             1   \n",
       "43                          1                             1   \n",
       "181                         0                             1   \n",
       "160                         0                             1   \n",
       "194                         0                             1   \n",
       "205                         0                             1   \n",
       "129                         0                             1   \n",
       "155                         0                             1   \n",
       "168                         0                             1   \n",
       "58                          1                             1   \n",
       "214                         0                             1   \n",
       "213                         0                             1   \n",
       "147                         0                             1   \n",
       "32                          1                             1   \n",
       "54                          1                             1   \n",
       "209                         0                             1   \n",
       "134                         0                             0   \n",
       "\n",
       "     electrode_configuration_BGTC  film_deposition_type_MGC  \\\n",
       "5                               0                         0   \n",
       "198                             0                         0   \n",
       "6                               0                         0   \n",
       "93                              0                         1   \n",
       "63                              0                         0   \n",
       "153                             0                         1   \n",
       "182                             1                         0   \n",
       "83                              1                         0   \n",
       "217                             0                         0   \n",
       "9                               0                         0   \n",
       "52                              0                         0   \n",
       "99                              0                         0   \n",
       "7                               0                         0   \n",
       "193                             0                         0   \n",
       "176                             0                         1   \n",
       "71                              0                         0   \n",
       "29                              0                         0   \n",
       "131                             1                         0   \n",
       "148                             0                         1   \n",
       "200                             0                         0   \n",
       "66                              0                         0   \n",
       "92                              0                         0   \n",
       "91                              0                         0   \n",
       "164                             0                         0   \n",
       "31                              0                         0   \n",
       "48                              0                         0   \n",
       "132                             1                         0   \n",
       "101                             0                         0   \n",
       "150                             0                         1   \n",
       "126                             0                         0   \n",
       "220                             0                         0   \n",
       "65                              0                         0   \n",
       "110                             0                         0   \n",
       "40                              0                         0   \n",
       "51                              0                         0   \n",
       "74                              0                         0   \n",
       "59                              0                         0   \n",
       "222                             0                         0   \n",
       "136                             1                         0   \n",
       "77                              0                         0   \n",
       "43                              0                         0   \n",
       "181                             0                         1   \n",
       "160                             0                         0   \n",
       "194                             0                         0   \n",
       "205                             0                         0   \n",
       "129                             0                         1   \n",
       "155                             0                         1   \n",
       "168                             0                         0   \n",
       "58                              0                         0   \n",
       "214                             0                         0   \n",
       "213                             0                         0   \n",
       "147                             0                         1   \n",
       "32                              0                         0   \n",
       "54                              0                         0   \n",
       "209                             0                         0   \n",
       "134                             1                         0   \n",
       "\n",
       "     solvent_boiling_point  hole_mobility  \n",
       "5                       62       0.005540  \n",
       "198                     62       0.019000  \n",
       "6                       62       0.002990  \n",
       "93                     213       0.002400  \n",
       "63                      62       0.054680  \n",
       "153                     62       0.187000  \n",
       "182                     62       0.030000  \n",
       "83                      62       0.000046  \n",
       "217                     62       0.058400  \n",
       "9                      213       0.001490  \n",
       "52                      62       0.002140  \n",
       "99                      62       0.021000  \n",
       "7                       62       0.008100  \n",
       "193                     62       0.030000  \n",
       "176                     62       0.083050  \n",
       "71                      62       0.007480  \n",
       "29                      62       0.018300  \n",
       "131                    111       0.001350  \n",
       "148                     62       0.012700  \n",
       "200                     62       0.069000  \n",
       "66                      62       0.020930  \n",
       "92                      62       0.001600  \n",
       "91                      62       0.001100  \n",
       "164                     62       0.022500  \n",
       "31                      62       0.017100  \n",
       "48                      62       0.010100  \n",
       "132                    111       0.000629  \n",
       "101                     62       0.250000  \n",
       "150                     62       0.103900  \n",
       "126                     62       0.060800  \n",
       "220                     62       0.080400  \n",
       "65                      62       0.053900  \n",
       "110                     62       0.000015  \n",
       "40                     213       0.002550  \n",
       "51                      62       0.004060  \n",
       "74                      62       0.004190  \n",
       "59                     132       0.000309  \n",
       "222                     62       0.002100  \n",
       "136                    111       0.003050  \n",
       "77                      62       0.007850  \n",
       "43                     213       0.125300  \n",
       "181                     62       0.125000  \n",
       "160                     62       0.013000  \n",
       "194                     62       0.032000  \n",
       "205                     62       0.102000  \n",
       "129                     62       0.014800  \n",
       "155                     62       0.011071  \n",
       "168                     62       0.089400  \n",
       "58                     132       0.000254  \n",
       "214                     62       0.077000  \n",
       "213                     62       0.059000  \n",
       "147                     62       0.006050  \n",
       "32                      62       0.012000  \n",
       "54                      62       0.002790  \n",
       "209                     62       0.101000  \n",
       "134                    111       0.001020  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Storing testing data\n",
    "df_test = pd.concat((df_X_test,df_Y_test),axis=1)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 16)\n",
      "(56, 16)\n",
      "(168,)\n",
      "(56,)\n"
     ]
    }
   ],
   "source": [
    "#splitting training and testing data into X and Y \n",
    "\n",
    "#making X and Y dataframes for train and test\n",
    "df_X_train = df_train.iloc[:,0:-1].copy() \n",
    "df_X_test = df_test.iloc[:,0:-1].copy()\n",
    "df_Y_train = df_train.iloc[:,-1].copy()\n",
    "df_Y_test = df_test.iloc[:,-1].copy()\n",
    "\n",
    "#making X and Y matrices for train and test\n",
    "X_train = df_X_train.values\n",
    "X_test = df_X_test.values\n",
    "y_train = df_Y_train.values\n",
    "y_test = df_Y_test.values\n",
    "\n",
    "#Y_train = Y_train.flatten()\n",
    "#Y_test = Y_test.reshape(368,)\n",
    "\n",
    "print(df_X_train.shape)\n",
    "print(df_X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 0.11.7 of tpot is outdated. Version 0.12.1 was released Tuesday August 15, 2023.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.35138865925558815\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.42114136581132344\tAdaBoostRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=8, DecisionTreeRegressor__min_samples_leaf=8, DecisionTreeRegressor__min_samples_split=19), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4173345919524188\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.42114136581132344\tAdaBoostRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=8, DecisionTreeRegressor__min_samples_leaf=8, DecisionTreeRegressor__min_samples_split=19), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4173345919524188\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.4643928508345499\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=20.0, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.01000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4173345919524188\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.46624162479898257\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=20.0, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4173345919524188\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.46624162479898257\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=20.0, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "training r2 is :  0.9670200949015716\n",
      "testing r2 is :  0.24111732111740114\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=5, population_size=50, verbosity=3, scoring='r2', random_state=1234)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_1.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 0.11.7 of tpot is outdated. Version 0.12.1 was released Tuesday August 15, 2023.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/2550 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.35138865925558815\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.42114136581132344\tAdaBoostRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=8, DecisionTreeRegressor__min_samples_leaf=8, DecisionTreeRegressor__min_samples_split=19), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4173345919524188\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.42114136581132344\tAdaBoostRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=8, DecisionTreeRegressor__min_samples_leaf=8, DecisionTreeRegressor__min_samples_split=19), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4173345919524188\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.4643928508345499\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=20.0, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.01000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4173345919524188\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.46624162479898257\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=20.0, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4173345919524188\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.46624162479898257\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=20.0, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5307915624934582\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5307915624934582\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5307915624934582\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5307915624934582\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5307915624934582\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:38:01] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5307915624934582\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5307915624934582\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5491639997763033\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5307915624934582\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5491639997763033\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5364968611700804\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5491639997763033\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:41:18] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5364968611700804\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5491639997763033\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:41:57] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5364968611700804\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5491639997763033\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5364968611700804\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5491639997763033\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:44:01] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5364968611700804\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5491639997763033\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5532052304478784\tXGBRegressor(SelectPercentile(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SelectPercentile__percentile=89), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5595183545653425\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:45:38] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [15:45:38] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:45:39] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [15:45:40] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:45:41] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5595183545653425\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5595183545653425\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:47:37] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5595578041738218\tXGBRegressor(PolynomialFeatures(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=12, DecisionTreeRegressor__min_samples_split=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 X contains negative values..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:49:34] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:54:13] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5696486812106117\tXGBRegressor(RandomForestRegressor(MinMaxScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5696486812106117\tXGBRegressor(RandomForestRegressor(MinMaxScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [16:03:00] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [16:03:08] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5696486812106117\tXGBRegressor(RandomForestRegressor(MinMaxScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5696486812106117\tXGBRegressor(RandomForestRegressor(MinMaxScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [16:06:09] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5696486812106117\tXGBRegressor(RandomForestRegressor(MinMaxScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5702980491628076\tXGBRegressor(StandardScaler(RandomForestRegressor(RobustScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5696486812106117\tXGBRegressor(RandomForestRegressor(MinMaxScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5702980491628076\tXGBRegressor(StandardScaler(RandomForestRegressor(RobustScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5696486812106117\tXGBRegressor(RandomForestRegressor(MinMaxScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5702980491628076\tXGBRegressor(StandardScaler(RandomForestRegressor(RobustScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5696486812106117\tXGBRegressor(RandomForestRegressor(MinMaxScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5702980491628076\tXGBRegressor(StandardScaler(RandomForestRegressor(RobustScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5698339652302544\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5702980491628076\tXGBRegressor(StandardScaler(RandomForestRegressor(RobustScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5696697703507906\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5698339652302544\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5702980491628076\tXGBRegressor(StandardScaler(RandomForestRegressor(RobustScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5696697703507906\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5698339652302544\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5702980491628076\tXGBRegressor(StandardScaler(RandomForestRegressor(RobustScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [16:26:42] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [16:26:47] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [16:35:55] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [16:35:55] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "training r2 is :  0.9649130363577595\n",
      "testing r2 is :  0.28787678217513346\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=50, population_size=50, verbosity=3, scoring='r2', random_state=1234)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 0.11.7 of tpot is outdated. Version 0.12.1 was released Tuesday August 15, 2023.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/10100 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.35138865925558815\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.3709156559466915\tAdaBoostRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.35138865925558815\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.4003995673256876\tAdaBoostRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.35138865925558815\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.40898243045612714\tAdaBoostRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=18, ExtraTreesRegressor__min_samples_split=12, ExtraTreesRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.35138865925558815\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.4309240066055516\tAdaBoostRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=9), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.44087319601705033\tAdaBoostRegressor(RandomForestRegressor(ZeroCount(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.35138865925558815\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.4309240066055516\tAdaBoostRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=9), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.504478100815148\tAdaBoostRegressor(PolynomialFeatures(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.3962332785534418\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.43571914816206847\tAdaBoostRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.504478100815148\tAdaBoostRegressor(PolynomialFeatures(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.3962332785534418\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.47635916987104165\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.504478100815148\tAdaBoostRegressor(PolynomialFeatures(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _mate_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.3962332785534418\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.47635916987104165\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5121948398222453\tAdaBoostRegressor(PolynomialFeatures(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [16:55:18] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.3962332785534418\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.47635916987104165\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5121948398222453\tAdaBoostRegressor(PolynomialFeatures(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.41222545749361145\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.45, GradientBoostingRegressor__min_samples_leaf=12, GradientBoostingRegressor__min_samples_split=12, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.5028896589812712\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5121948398222453\tAdaBoostRegressor(PolynomialFeatures(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4530879489089236\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5028896589812712\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5121948398222453\tAdaBoostRegressor(PolynomialFeatures(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4530879489089236\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5084834546739859\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.515140528940069\tAdaBoostRegressor(PolynomialFeatures(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [17:10:02] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4530879489089236\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5118777677076345\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.515140528940069\tAdaBoostRegressor(PolynomialFeatures(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4530879489089236\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5118777677076345\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.515140528940069\tAdaBoostRegressor(PolynomialFeatures(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [17:14:40] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4530879489089236\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5118777677076345\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.515140528940069\tAdaBoostRegressor(PolynomialFeatures(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5177547463650172\tAdaBoostRegressor(XGBRegressor(RidgeCV(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.46215016969319545\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)\n",
      "\n",
      "-2\t0.5313753367883163\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.46215016969319545\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)\n",
      "\n",
      "-2\t0.5313753367883163\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [17:21:15] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.46215016969319545\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)\n",
      "\n",
      "-2\t0.5313753367883163\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.46215016969319545\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)\n",
      "\n",
      "-2\t0.5313753367883163\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [17:26:36] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.46215016969319545\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)\n",
      "\n",
      "-2\t0.5313753367883163\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [17:29:37] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.46215016969319545\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)\n",
      "\n",
      "-2\t0.5313753367883163\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.46215016969319545\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)\n",
      "\n",
      "-2\t0.5313753367883163\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.533604166489795\tAdaBoostRegressor(OneHotEncoder(RandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [17:34:39] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.46215016969319545\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)\n",
      "\n",
      "-2\t0.5313753367883163\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5327984990564278\tAdaBoostRegressor(GradientBoostingRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.001, GradientBoostingRegressor__loss=lad, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.8, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.533604166489795\tAdaBoostRegressor(OneHotEncoder(RandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5390427427490984\tAdaBoostRegressor(RandomForestRegressor(SGDRegressor(XGBRegressor(StandardScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.5), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.46215016969319545\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)\n",
      "\n",
      "-2\t0.5313753367883163\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5327984990564278\tAdaBoostRegressor(GradientBoostingRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.001, GradientBoostingRegressor__loss=lad, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.8, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.533604166489795\tAdaBoostRegressor(OneHotEncoder(RandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5390427427490984\tAdaBoostRegressor(RandomForestRegressor(SGDRegressor(XGBRegressor(StandardScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.5), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [17:40:35] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [17:40:45] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [17:40:56] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.46215016969319545\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)\n",
      "\n",
      "-2\t0.5313753367883163\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5327984990564278\tAdaBoostRegressor(GradientBoostingRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.001, GradientBoostingRegressor__loss=lad, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.8, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.533604166489795\tAdaBoostRegressor(OneHotEncoder(RandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5390427427490984\tAdaBoostRegressor(RandomForestRegressor(SGDRegressor(XGBRegressor(StandardScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.5), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.46215016969319545\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)\n",
      "\n",
      "-2\t0.5313753367883163\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5348271389217737\tAdaBoostRegressor(RandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5390427427490984\tAdaBoostRegressor(RandomForestRegressor(SGDRegressor(XGBRegressor(StandardScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.5), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [17:46:43] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [17:46:43] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input contains NaN, infinity or a value too large for dtype('float32')..\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.46215016969319545\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)\n",
      "\n",
      "-2\t0.5313753367883163\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5348271389217737\tAdaBoostRegressor(RandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5351443234796422\tAdaBoostRegressor(OneHotEncoder(RandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5390427427490984\tAdaBoostRegressor(RandomForestRegressor(SGDRegressor(XGBRegressor(StandardScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.5), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [17:49:38] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.46215016969319545\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)\n",
      "\n",
      "-2\t0.5313753367883163\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5363368364743903\tAdaBoostRegressor(XGBRegressor(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5390427427490984\tAdaBoostRegressor(RandomForestRegressor(SGDRegressor(XGBRegressor(StandardScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.5), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [17:52:37] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.46215016969319545\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)\n",
      "\n",
      "-2\t0.5313753367883163\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5496470580750864\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5513662230012641\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [17:55:30] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.46215016969319545\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)\n",
      "\n",
      "-2\t0.5313753367883163\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5496470580750864\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5513662230012641\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.46380971007199695\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.8500000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)\n",
      "\n",
      "-2\t0.5313753367883163\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5496470580750864\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5513662230012641\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:01:36] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:01:36] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:01:42] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4824252215048658\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5313753367883163\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5496470580750864\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5513662230012641\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4824252215048658\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5313753367883163\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5496470580750864\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5513662230012641\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:06:59] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4824252215048658\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5313753367883163\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5496470580750864\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5513662230012641\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:10:04] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4824252215048658\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5313753367883163\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5496470580750864\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5513662230012641\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:12:46] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:12:51] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:13:03] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4824252215048658\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.533832612028289\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5496470580750864\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5513662230012641\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4824252215048658\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.533832612028289\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5496470580750864\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5524040789118485\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:18:30] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4872786681686156\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.533832612028289\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5496470580750864\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5524040789118485\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:21:22] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4872786681686156\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.533832612028289\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5509584910144303\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5524040789118485\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4872786681686156\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5509584910144303\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5524040789118485\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:26:34] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:26:38] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4872786681686156\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5509584910144303\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5524040789118485\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.554367866236203\tAdaBoostRegressor(OneHotEncoder(LassoLarsCV(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), LassoLarsCV__normalize=True), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.49232022698723304\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5509584910144303\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5524040789118485\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.554367866236203\tAdaBoostRegressor(OneHotEncoder(LassoLarsCV(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), LassoLarsCV__normalize=True), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:31:58] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:32:04] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.49232022698723304\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5509584910144303\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5524040789118485\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.555251746764869\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:35:10] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.49232022698723304\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5521974770196341\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5524040789118485\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.555251746764869\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [18:38:26] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5227508626972004\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5521974770196341\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5528697416193298\tAdaBoostRegressor(XGBRegressor(ElasticNetCV(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.30000000000000004, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5570822672712096\tAdaBoostRegressor(OneHotEncoder(LassoLarsCV(XGBRegressor(RandomForestRegressor(CombineDFs(input_matrix, input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), LassoLarsCV__normalize=False), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5227508626972004\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5521974770196341\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5528697416193298\tAdaBoostRegressor(XGBRegressor(ElasticNetCV(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.30000000000000004, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5570822672712096\tAdaBoostRegressor(OneHotEncoder(LassoLarsCV(XGBRegressor(RandomForestRegressor(CombineDFs(input_matrix, input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), LassoLarsCV__normalize=False), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:44:49] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5227508626972004\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5521974770196341\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5532080158192678\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5570822672712096\tAdaBoostRegressor(OneHotEncoder(LassoLarsCV(XGBRegressor(RandomForestRegressor(CombineDFs(input_matrix, input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), LassoLarsCV__normalize=False), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:47:26] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5227508626972004\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5521974770196341\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5532080158192678\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5570822672712096\tAdaBoostRegressor(OneHotEncoder(LassoLarsCV(XGBRegressor(RandomForestRegressor(CombineDFs(input_matrix, input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), LassoLarsCV__normalize=False), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:50:44] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:50:50] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5227508626972004\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5521974770196341\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5535812597559728\tAdaBoostRegressor(SelectFromModel(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), SelectFromModel__ExtraTreesRegressor__max_features=0.6500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.4), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5570822672712096\tAdaBoostRegressor(OneHotEncoder(LassoLarsCV(XGBRegressor(RandomForestRegressor(CombineDFs(input_matrix, input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), LassoLarsCV__normalize=False), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5227508626972004\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5530659426236735\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5543156736785134\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5570822672712096\tAdaBoostRegressor(OneHotEncoder(LassoLarsCV(XGBRegressor(RandomForestRegressor(CombineDFs(input_matrix, input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), LassoLarsCV__normalize=False), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:56:46] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 51 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5227508626972004\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5530659426236735\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5553288376781891\tAdaBoostRegressor(LassoLarsCV(XGBRegressor(RandomForestRegressor(CombineDFs(input_matrix, input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), LassoLarsCV__normalize=False), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5570822672712096\tAdaBoostRegressor(OneHotEncoder(LassoLarsCV(XGBRegressor(RandomForestRegressor(CombineDFs(input_matrix, input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), LassoLarsCV__normalize=False), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 52 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5227508626972004\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5530659426236735\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5553288376781891\tAdaBoostRegressor(LassoLarsCV(XGBRegressor(RandomForestRegressor(CombineDFs(input_matrix, input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), LassoLarsCV__normalize=False), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5570822672712096\tAdaBoostRegressor(OneHotEncoder(LassoLarsCV(XGBRegressor(RandomForestRegressor(CombineDFs(input_matrix, input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), LassoLarsCV__normalize=False), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:03:43] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 53 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5227508626972004\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5530659426236735\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5600676522365102\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=13, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "\n",
      "Generation 54 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5227508626972004\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5530659426236735\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:09:50] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 55 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5227508626972004\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5530659426236735\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 56 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5227508626972004\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5530659426236735\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-9\t0.5671442175552008\tAdaBoostRegressor(ElasticNetCV(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.5, ElasticNetCV__tol=1e-05), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:16:30] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:16:55] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 57 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5227508626972004\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5530659426236735\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-8\t0.5714647970402972\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:20:08] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 58 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5227508626972004\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-8\t0.5714647970402972\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 59 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5227508626972004\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-8\t0.5714647970402972\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:26:49] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:26:54] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "\n",
      "Generation 60 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5266472124324549\tXGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-8\t0.5714647970402972\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:29:58] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 61 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5266472124324549\tXGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.5679570743414419\tAdaBoostRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-8\t0.5714647970402972\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 62 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5266472124324549\tXGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.5679570743414419\tAdaBoostRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-8\t0.5714647970402972\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 63 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5266472124324549\tXGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5675710052942604\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.5687506944517908\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=11, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-8\t0.5714647970402972\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:41:08] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 64 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5266472124324549\tXGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5701408181293237\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=11, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-8\t0.5714647970402972\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 65 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5266472124324549\tXGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5701408181293237\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=11, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-8\t0.5714647970402972\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-9\t0.5744751536426976\tAdaBoostRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:48:24] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:48:28] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "\n",
      "Generation 66 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5266472124324549\tXGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5701408181293237\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=11, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-8\t0.5714647970402972\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-9\t0.5744751536426976\tAdaBoostRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:53:45] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "\n",
      "Generation 67 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5266472124324549\tXGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5670580604515424\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5710269740052405\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-8\t0.5714647970402972\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-9\t0.5744751536426976\tAdaBoostRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 68 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5266472124324549\tXGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5670580604515424\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5718349989099243\tAdaBoostRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-9\t0.5744751536426976\tAdaBoostRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-10\t0.5750473547788794\tAdaBoostRegressor(GradientBoostingRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.35000000000000003, GradientBoostingRegressor__min_samples_leaf=2, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.5), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [20:01:44] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "\n",
      "Generation 69 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5266472124324549\tXGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5670580604515424\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5718349989099243\tAdaBoostRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-9\t0.5744751536426976\tAdaBoostRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-10\t0.5750473547788794\tAdaBoostRegressor(GradientBoostingRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.35000000000000003, GradientBoostingRegressor__min_samples_leaf=2, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.5), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
      "\n",
      "Generation 70 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5266472124324549\tXGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5679296830166206\tAdaBoostRegressor(XGBRegressor(LassoLarsCV(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), LassoLarsCV__normalize=True), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.572529928518527\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-9\t0.5744751536426976\tAdaBoostRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-10\t0.5750473547788794\tAdaBoostRegressor(GradientBoostingRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.35000000000000003, GradientBoostingRegressor__min_samples_leaf=2, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.5), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 71 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5266472124324549\tXGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5679296830166206\tAdaBoostRegressor(XGBRegressor(LassoLarsCV(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), LassoLarsCV__normalize=True), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.572529928518527\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-9\t0.5744751536426976\tAdaBoostRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-10\t0.5750473547788794\tAdaBoostRegressor(GradientBoostingRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.35000000000000003, GradientBoostingRegressor__min_samples_leaf=2, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.5), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 72 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5266472124324549\tXGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5705286180180629\tAdaBoostRegressor(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.572529928518527\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-9\t0.5744751536426976\tAdaBoostRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-10\t0.5750473547788794\tAdaBoostRegressor(GradientBoostingRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.35000000000000003, GradientBoostingRegressor__min_samples_leaf=2, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.5), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 73 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5266472124324549\tXGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5705286180180629\tAdaBoostRegressor(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.572529928518527\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-9\t0.5744751536426976\tAdaBoostRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-10\t0.5750473547788794\tAdaBoostRegressor(GradientBoostingRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.35000000000000003, GradientBoostingRegressor__min_samples_leaf=2, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.5), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [20:20:35] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 74 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5266472124324549\tXGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5670575659688517\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5705286180180629\tAdaBoostRegressor(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.572529928518527\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-9\t0.5744751536426976\tAdaBoostRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-10\t0.5750473547788794\tAdaBoostRegressor(GradientBoostingRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.35000000000000003, GradientBoostingRegressor__min_samples_leaf=2, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.5), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [20:24:04] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 75 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5266472124324549\tXGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5369713799120055\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5705286180180629\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.572529928518527\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-9\t0.5744751536426976\tAdaBoostRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-10\t0.5750473547788794\tAdaBoostRegressor(GradientBoostingRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.35000000000000003, GradientBoostingRegressor__min_samples_leaf=2, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.5), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-11\t0.5783008272128647\tAdaBoostRegressor(MinMaxScaler(RobustScaler(SelectFwe(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), SelectFwe__alpha=0.004))), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [20:27:23] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.\n",
      "\n",
      "Generation 76 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5705286180180629\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5708073704420299\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.572529928518527\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-9\t0.5744751536426976\tAdaBoostRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-10\t0.5750473547788794\tAdaBoostRegressor(GradientBoostingRegressor(RobustScaler(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.35000000000000003, GradientBoostingRegressor__min_samples_leaf=2, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.5), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-11\t0.5783008272128647\tAdaBoostRegressor(MinMaxScaler(RobustScaler(SelectFwe(RandomForestRegressor(RidgeCV(OneHotEncoder(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), SelectFwe__alpha=0.004))), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 77 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5705286180180629\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5708073704420299\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.572529928518527\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.578369112620973\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 78 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5705286180180629\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5708073704420299\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.572529928518527\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.578369112620973\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 79 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5705286180180629\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5708073704420299\tAdaBoostRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.572529928518527\tAdaBoostRegressor(RandomForestRegressor(RidgeCV(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.578369112620973\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 80 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5705286180180629\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.572482489399339\tAdaBoostRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5776993392938602\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.578369112620973\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.\n",
      "\n",
      "Generation 81 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5705286180180629\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5727987956393117\tAdaBoostRegressor(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5776993392938602\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.578369112620973\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 82 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5705286180180629\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5727987956393117\tAdaBoostRegressor(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5776993392938602\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.578369112620973\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-8\t0.578468188593656\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(StandardScaler(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=9, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [20:55:10] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.\n",
      "\n",
      "Generation 83 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5705286180180629\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5727987956393117\tAdaBoostRegressor(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5776993392938602\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.578369112620973\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-8\t0.5794512505855047\tAdaBoostRegressor(RandomForestRegressor(ZeroCount(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 84 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5705286180180629\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5727987956393117\tAdaBoostRegressor(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5776993392938602\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.579266511489976\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-8\t0.5794512505855047\tAdaBoostRegressor(RandomForestRegressor(ZeroCount(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [21:03:00] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "\n",
      "Generation 85 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5705286180180629\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5727987956393117\tAdaBoostRegressor(XGBRegressor(XGBRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5776993392938602\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.581360523564924\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 86 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5727987956393117\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5776993392938602\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.581360523564924\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 87 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5727987956393117\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5776993392938602\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.581360523564924\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [21:15:03] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 88 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5727987956393117\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5734074644813425\tAdaBoostRegressor(DecisionTreeRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=7, DecisionTreeRegressor__min_samples_split=4), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5776993392938602\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.581360523564924\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [21:18:57] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [21:19:00] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.\n",
      "\n",
      "Generation 89 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5727987956393117\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5734074644813425\tAdaBoostRegressor(DecisionTreeRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=7, DecisionTreeRegressor__min_samples_split=4), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5776993392938602\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.581360523564924\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 90 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5727987956393117\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5734074644813425\tAdaBoostRegressor(DecisionTreeRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=7, DecisionTreeRegressor__min_samples_split=4), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5776993392938602\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.581360523564924\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 91 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5553014173692157\tAdaBoostRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5727987956393117\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5734074644813425\tAdaBoostRegressor(DecisionTreeRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=7, DecisionTreeRegressor__min_samples_split=4), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5776993392938602\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.581360523564924\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [21:30:38] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 92 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.543100990138384\tExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5575987268695972\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5727987956393117\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5734074644813425\tAdaBoostRegressor(DecisionTreeRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=7, DecisionTreeRegressor__min_samples_split=4), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.578452129441601\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(CombineDFs(input_matrix, input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.581360523564924\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 93 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.543100990138384\tExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5575987268695972\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5727987956393117\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5734074644813425\tAdaBoostRegressor(DecisionTreeRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=7, DecisionTreeRegressor__min_samples_split=4), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.578452129441601\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(CombineDFs(input_matrix, input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.581360523564924\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 94 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5499723823411456\tExtraTreesRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5575987268695972\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5727987956393117\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5734074644813425\tAdaBoostRegressor(DecisionTreeRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=7, DecisionTreeRegressor__min_samples_split=4), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.578452129441601\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(CombineDFs(input_matrix, input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.581360523564924\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [21:42:51] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 95 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5499723823411456\tExtraTreesRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5575987268695972\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5727987956393117\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5734074644813425\tAdaBoostRegressor(DecisionTreeRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=7, DecisionTreeRegressor__min_samples_split=4), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5785595457138577\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(CombineDFs(input_matrix, input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.581360523564924\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [21:46:59] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "\n",
      "Generation 96 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5499723823411456\tExtraTreesRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5575987268695972\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5727987956393117\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5734074644813425\tAdaBoostRegressor(DecisionTreeRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=7, DecisionTreeRegressor__min_samples_split=4), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5785595457138577\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(CombineDFs(input_matrix, input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.581360523564924\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 97 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5499723823411456\tExtraTreesRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5575987268695972\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5727987956393117\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.5734074644813425\tAdaBoostRegressor(DecisionTreeRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=7, DecisionTreeRegressor__min_samples_split=4), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5785595457138577\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(CombineDFs(input_matrix, input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.581360523564924\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [21:55:07] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "\n",
      "Generation 98 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5380409565656864\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5499723823411456\tExtraTreesRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5575987268695972\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5744963052974226\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5785595457138577\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(CombineDFs(input_matrix, input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.581360523564924\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [21:58:54] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [21:59:08] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 99 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5463122940087259\tXGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5499723823411456\tExtraTreesRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5575987268695972\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5744963052974226\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5785595457138577\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(CombineDFs(input_matrix, input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.581360523564924\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 100 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5463122940087259\tXGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5499723823411456\tExtraTreesRegressor(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5575987268695972\tAdaBoostRegressor(XGBRegressor(CombineDFs(input_matrix, RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.5744963052974226\tAdaBoostRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.5785595457138577\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(XGBRegressor(XGBRegressor(RandomForestRegressor(CombineDFs(input_matrix, input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.581360523564924\tAdaBoostRegressor(RandomForestRegressor(RandomForestRegressor(OneHotEncoder(XGBRegressor(XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=1.0, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=15, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.4, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "training r2 is :  0.9773353879371016\n",
      "testing r2 is :  0.366096974979763\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=100, population_size=100, verbosity=3, scoring='r2', random_state=1234)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_3.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 0.11.7 of tpot is outdated. Version 0.12.1 was released Tuesday August 15, 2023.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.45204629985237627\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _mate_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.45204629985237627\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.45204629985237627\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.45204629985237627\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.46729202642165324\tGradientBoostingRegressor(Normalizer(input_matrix, Normalizer__norm=l2), GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=8, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.45204629985237627\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.46729202642165324\tGradientBoostingRegressor(Normalizer(input_matrix, Normalizer__norm=l2), GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=8, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001)\n",
      "training r2 is :  0.22161493396786558\n",
      "testing r2 is :  0.08267639147386752\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=5, population_size=50, verbosity=3, scoring='r2', random_state=100)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_4.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 0.11.7 of tpot is outdated. Version 0.12.1 was released Tuesday August 15, 2023.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.40831064434701664\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=13, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.40831064434701664\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=13, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.41301018897035835\tAdaBoostRegressor(MaxAbsScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.43721606741223606\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.43721606741223606\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.43721606741223606\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.4480659506453405\tAdaBoostRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=7, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "training r2 is :  0.9746433744315848\n",
      "testing r2 is :  0.2600816761909871\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=5, population_size=50, verbosity=3, scoring='r2', random_state=500)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_5.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 0.11.7 of tpot is outdated. Version 0.12.1 was released Tuesday August 15, 2023.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.3220908891128508\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.34578351280447067\tRidgeCV(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.3220908891128508\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.3502515371013769\tAdaBoostRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.4205711209758475\tRidgeCV(AdaBoostRegressor(FastICA(input_matrix, FastICA__tol=0.65), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.3220908891128508\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.3958858883435498\tRidgeCV(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100))\n",
      "\n",
      "-3\t0.4205711209758475\tRidgeCV(AdaBoostRegressor(FastICA(input_matrix, FastICA__tol=0.65), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100))\n",
      "\n",
      "-4\t0.4601743739430651\tRidgeCV(AdaBoostRegressor(FastICA(RobustScaler(input_matrix), FastICA__tol=0.65), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.3220908891128508\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.3958858883435498\tRidgeCV(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100))\n",
      "\n",
      "-3\t0.4205711209758475\tRidgeCV(AdaBoostRegressor(FastICA(input_matrix, FastICA__tol=0.65), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100))\n",
      "\n",
      "-4\t0.4601743739430651\tRidgeCV(AdaBoostRegressor(FastICA(RobustScaler(input_matrix), FastICA__tol=0.65), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.3567294468598893\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.3958858883435498\tRidgeCV(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100))\n",
      "\n",
      "-3\t0.43067786505701644\tKNeighborsRegressor(AdaBoostRegressor(MinMaxScaler(input_matrix), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=30, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.4601743739430651\tRidgeCV(AdaBoostRegressor(FastICA(RobustScaler(input_matrix), FastICA__tol=0.65), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100))\n",
      "training r2 is :  0.9673775758695248\n",
      "testing r2 is :  0.22457339240205698\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=5, population_size=50, verbosity=3, scoring='r2', random_state=2000)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_6.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 0.11.7 of tpot is outdated. Version 0.12.1 was released Tuesday August 15, 2023.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.44566698400721066\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=13, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.44566698400721066\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=13, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.44566698400721066\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=13, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.44566698400721066\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=13, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.44566698400721066\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=13, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.4682720699744289\tGradientBoostingRegressor(PolynomialFeatures(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "training r2 is :  0.17970797300394736\n",
      "testing r2 is :  0.1472567497095143\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=5, population_size=50, verbosity=3, scoring='r2')\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_7.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 0.11.7 of tpot is outdated. Version 0.12.1 was released Tuesday August 15, 2023.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.3831059375069331\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.42109390834875293\tGradientBoostingRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=lad, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=4, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001), GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.9500000000000001, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=10, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.3831059375069331\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.42109390834875293\tGradientBoostingRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=lad, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=4, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001), GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.9500000000000001, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=10, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.3831059375069331\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.42109390834875293\tGradientBoostingRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=lad, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=4, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001), GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.9500000000000001, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=10, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [22:21:14] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.3831059375069331\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.42109390834875293\tGradientBoostingRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=lad, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=4, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001), GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.9500000000000001, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=10, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.3831059375069331\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.42109390834875293\tGradientBoostingRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=lad, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=4, GradientBoostingRegressor__min_samples_split=19, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001), GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.9500000000000001, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=10, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)\n",
      "\n",
      "-3\t0.4282810955810241\tAdaBoostRegressor(GradientBoostingRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.001, GradientBoostingRegressor__loss=lad, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=15, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training r2 is :  0.18880690865532168\n",
      "testing r2 is :  0.24573829946305892\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=5, population_size=50, verbosity=3, scoring='r2')\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_8.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 0.11.7 of tpot is outdated. Version 0.12.1 was released Tuesday August 15, 2023.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/10100 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.3273306402892433\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.38761382792126586\tAdaBoostRegressor(FastICA(input_matrix, FastICA__tol=0.8), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4087400228192033\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.4165662816083925\tAdaBoostRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4087400228192033\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.43272091012269415\tAdaBoostRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4087400228192033\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.43272091012269415\tAdaBoostRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4087400228192033\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.43272091012269415\tAdaBoostRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.47106671978250514\tAdaBoostRegressor(GradientBoostingRegressor(LassoLarsCV(input_matrix, LassoLarsCV__normalize=False), GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=10, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4087400228192033\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.4454340460516226\tAdaBoostRegressor(AdaBoostRegressor(CombineDFs(input_matrix, input_matrix), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.47106671978250514\tAdaBoostRegressor(GradientBoostingRegressor(LassoLarsCV(input_matrix, LassoLarsCV__normalize=False), GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=10, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5001411127775832\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5001411127775832\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5001411127775832\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5025518159282114\tRandomForestRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=9, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5001411127775832\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5025518159282114\tRandomForestRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=9, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [22:38:39] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5001411127775832\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5162228613646098\tXGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=21, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5001411127775832\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5162228613646098\tXGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=21, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5048345861113344\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5162228613646098\tXGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=21, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5087240559651168\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5162228613646098\tXGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=21, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.509456716158868\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5162228613646098\tXGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=21, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.517948803706475\tXGBRegressor(XGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.35000000000000003, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.525397759933881\tXGBRegressor(VarianceThreshold(LassoLarsCV(GradientBoostingRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002, XGBRegressor__verbosity=0), GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.001, GradientBoostingRegressor__loss=ls, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.15000000000000002, GradientBoostingRegressor__min_samples_leaf=14, GradientBoostingRegressor__min_samples_split=15, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), LassoLarsCV__normalize=True), VarianceThreshold__threshold=0.001), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [22:49:37] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.509456716158868\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5162228613646098\tXGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=21, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5249253133282252\tXGBRegressor(SGDRegressor(StandardScaler(input_matrix), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.525397759933881\tXGBRegressor(VarianceThreshold(LassoLarsCV(GradientBoostingRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002, XGBRegressor__verbosity=0), GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.001, GradientBoostingRegressor__loss=ls, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.15000000000000002, GradientBoostingRegressor__min_samples_leaf=14, GradientBoostingRegressor__min_samples_split=15, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), LassoLarsCV__normalize=True), VarianceThreshold__threshold=0.001), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [22:51:41] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.509456716158868\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5333199867994501\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.35000000000000003, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [22:53:38] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.509456716158868\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5333199867994501\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.35000000000000003, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5394319005971905\tXGBRegressor(StandardScaler(MinMaxScaler(input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5406368583443332\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [22:56:55] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5406368583443332\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [22:58:38] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5406368583443332\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5406368583443332\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5485581483200069\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=8, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.35000000000000003, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:02:37] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:02:47] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5406368583443332\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5451282418624016\tXGBRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5496310484211671\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.35000000000000003, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5406368583443332\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5451282418624016\tXGBRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.554142117713271\tXGBRegressor(RandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.35000000000000003, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5406368583443332\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5451282418624016\tXGBRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.554142117713271\tXGBRegressor(RandomForestRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.35000000000000003, XGBRegressor__verbosity=0)\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5582017172276107\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:10:38] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5582017172276107\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5582017172276107\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:14:31] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5582017172276107\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5714283323962579\tXGBRegressor(ExtraTreesRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=11, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5582017172276107\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5714283323962579\tXGBRegressor(ExtraTreesRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=11, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-6\t0.5757389694857808\tXGBRegressor(XGBRegressor(MinMaxScaler(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=8, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:18:07] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5582017172276107\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5714283323962579\tXGBRegressor(ExtraTreesRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=11, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-6\t0.5757389694857808\tXGBRegressor(XGBRegressor(MinMaxScaler(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=8, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:19:59] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:20:08] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5582017172276107\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5714283323962579\tXGBRegressor(ExtraTreesRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=11, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-6\t0.5757389694857808\tXGBRegressor(XGBRegressor(MinMaxScaler(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=8, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5583288456525969\tXGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.55, ElasticNetCV__tol=0.1), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5714283323962579\tXGBRegressor(ExtraTreesRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=11, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-6\t0.5757389694857808\tXGBRegressor(XGBRegressor(MinMaxScaler(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=8, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:23:30] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:23:39] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5583288456525969\tXGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.55, ElasticNetCV__tol=0.1), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5714283323962579\tXGBRegressor(ExtraTreesRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=11, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-6\t0.5757389694857808\tXGBRegressor(XGBRegressor(MinMaxScaler(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=8, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:25:05] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:25:18] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5698991019801009\tXGBRegressor(KNeighborsRegressor(CombineDFs(input_matrix, input_matrix), KNeighborsRegressor__n_neighbors=41, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5714283323962579\tXGBRegressor(ExtraTreesRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=11, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-6\t0.5757389694857808\tXGBRegressor(XGBRegressor(MinMaxScaler(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=8, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:26:35] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5698991019801009\tXGBRegressor(KNeighborsRegressor(CombineDFs(input_matrix, input_matrix), KNeighborsRegressor__n_neighbors=41, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5714283323962579\tXGBRegressor(ExtraTreesRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=11, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-6\t0.5757389694857808\tXGBRegressor(XGBRegressor(MinMaxScaler(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=8, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5698991019801009\tXGBRegressor(KNeighborsRegressor(CombineDFs(input_matrix, input_matrix), KNeighborsRegressor__n_neighbors=41, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5714283323962579\tXGBRegressor(ExtraTreesRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=11, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5785550384220033\tXGBRegressor(MinMaxScaler(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=8, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:30:04] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:30:17] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:33:45] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:33:48] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:39:46] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:41:23] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:45:03] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:47:02] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:48:53] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:52:26] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:52:27] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:52:34] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:52:40] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:54:14] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 51 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:55:39] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:55:50] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:55:51] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 52 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:57:18] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:57:21] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:57:24] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 53 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 54 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:00:10] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:00:11] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 55 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:01:37] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:01:40] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:01:44] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 56 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 57 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:05:35] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 58 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:06:47] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:06:53] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 59 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:07:59] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 60 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:09:16] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:09:21] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:09:25] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/data/data.cc:1111: Check failed: valid: Input data contains `inf` or `nan`.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:09:29] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 61 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:10:40] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:10:41] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 62 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:11:52] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 63 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:13:22] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:13:25] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 64 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:14:39] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:14:43] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 65 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:15:51] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 66 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:17:16] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 67 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:18:32] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:18:34] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 68 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:19:42] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:19:43] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:19:46] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:19:46] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 69 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:20:44] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:20:52] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [00:20:55] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 70 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:21:48] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 71 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:22:56] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:22:58] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:23:04] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 72 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:23:53] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:24:00] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 73 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 74 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:26:23] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:26:24] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 75 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:27:19] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:27:24] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:27:29] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 76 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:28:19] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:28:26] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 77 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:29:05] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [00:29:07] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 78 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:30:20] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:30:24] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 79 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:31:24] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 80 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 81 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:33:35] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:33:36] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:33:36] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:33:37] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 82 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:34:23] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:34:23] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:34:31] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 83 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:35:31] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:35:36] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:35:37] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 84 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:36:40] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 85 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:37:31] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:37:33] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [00:37:36] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:37:37] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:37:38] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:37:41] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 86 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:38:26] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:38:35] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 87 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:39:29] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:39:31] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [00:39:31] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 88 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 89 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:41:36] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:41:38] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 90 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:42:38] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:42:40] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 91 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:43:57] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 92 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:45:02] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 93 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:46:09] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:46:10] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:46:10] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:46:13] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 94 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:47:06] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:47:07] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [00:47:11] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:47:13] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 95 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:48:00] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:48:05] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:48:05] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:48:06] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 96 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:49:14] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:49:21] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 97 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:50:14] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [00:50:14] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:50:19] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:50:24] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l2 was provided as affinity. Ward can only work with euclidean distances..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 98 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:51:29] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 99 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:52:40] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:52:47] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:52:48] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 100 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5438572510480961\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5853653235210853\tXGBRegressor(CombineDFs(input_matrix, ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5879485889607708\tXGBRegressor(GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5911811508837705\tXGBRegressor(XGBRegressor(PolynomialFeatures(ExtraTreesRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "training r2 is :  0.9721632306502339\n",
      "testing r2 is :  0.2872662486334865\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=100, population_size=100, verbosity=3, scoring='r2')\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_9.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 0.11.7 of tpot is outdated. Version 0.12.1 was released Tuesday August 15, 2023.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/5050 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.35138865925558815\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.42114136581132344\tAdaBoostRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=8, DecisionTreeRegressor__min_samples_leaf=8, DecisionTreeRegressor__min_samples_split=19), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4173345919524188\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.42114136581132344\tAdaBoostRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=8, DecisionTreeRegressor__min_samples_leaf=8, DecisionTreeRegressor__min_samples_split=19), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4173345919524188\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.4643928508345499\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=20.0, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.01000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4173345919524188\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.46624162479898257\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=20.0, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4173345919524188\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.46624162479898257\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=20.0, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5307915624934582\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5307915624934582\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5307915624934582\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5307915624934582\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5307915624934582\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:02:16] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5307915624934582\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5307915624934582\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5491639997763033\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5307915624934582\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5491639997763033\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5364968611700804\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5491639997763033\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:05:04] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5364968611700804\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5491639997763033\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:05:38] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5364968611700804\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5491639997763033\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5364968611700804\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5491639997763033\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:07:27] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5364968611700804\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5491639997763033\tXGBRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5532052304478784\tXGBRegressor(SelectPercentile(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SelectPercentile__percentile=89), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5595183545653425\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:08:52] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [01:08:52] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:08:53] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [01:08:54] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:08:55] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5595183545653425\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5595183545653425\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:10:36] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5595578041738218\tXGBRegressor(PolynomialFeatures(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=12, DecisionTreeRegressor__min_samples_split=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 X contains negative values..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:12:19] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:16:23] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5473947837179268\tXGBRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5696486812106117\tXGBRegressor(RandomForestRegressor(MinMaxScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5696486812106117\tXGBRegressor(RandomForestRegressor(MinMaxScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:24:04] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:24:11] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5696486812106117\tXGBRegressor(RandomForestRegressor(MinMaxScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5696486812106117\tXGBRegressor(RandomForestRegressor(MinMaxScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:26:48] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5696486812106117\tXGBRegressor(RandomForestRegressor(MinMaxScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5702980491628076\tXGBRegressor(StandardScaler(RandomForestRegressor(RobustScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5496645195075092\tXGBRegressor(RobustScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5696486812106117\tXGBRegressor(RandomForestRegressor(MinMaxScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5702980491628076\tXGBRegressor(StandardScaler(RandomForestRegressor(RobustScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5696486812106117\tXGBRegressor(RandomForestRegressor(MinMaxScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5702980491628076\tXGBRegressor(StandardScaler(RandomForestRegressor(RobustScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5696486812106117\tXGBRegressor(RandomForestRegressor(MinMaxScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5702980491628076\tXGBRegressor(StandardScaler(RandomForestRegressor(RobustScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5691445151704497\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5698339652302544\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5702980491628076\tXGBRegressor(StandardScaler(RandomForestRegressor(RobustScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5696697703507906\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5698339652302544\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5702980491628076\tXGBRegressor(StandardScaler(RandomForestRegressor(RobustScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5696697703507906\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5698339652302544\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5702980491628076\tXGBRegressor(StandardScaler(RandomForestRegressor(RobustScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:44:16] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:44:20] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:52:18] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [01:52:18] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 51 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 52 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:57:42] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 53 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5753811771960949\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 54 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:59:20] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:59:23] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 55 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 56 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 57 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:01:30] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 58 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 59 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [02:03:07] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 60 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 61 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 62 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:05:07] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 63 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 64 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 65 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:07:12] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:07:12] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:07:17] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 66 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:07:57] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 67 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 68 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 69 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 70 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:11:02] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 71 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:11:39] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 72 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 73 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 74 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 75 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 76 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:15:22] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 77 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 78 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 79 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:18:40] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 80 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:19:16] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 81 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5798982806436148\tXGBRegressor(RandomForestRegressor(Normalizer(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 82 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5502856732886534\tXGBRegressor(CombineDFs(RobustScaler(input_matrix), input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5798982806436148\tXGBRegressor(RandomForestRegressor(Normalizer(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 83 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5504012318040765\tXGBRegressor(CombineDFs(SelectPercentile(CombineDFs(input_matrix, input_matrix), SelectPercentile__percentile=85), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5798982806436148\tXGBRegressor(RandomForestRegressor(Normalizer(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 84 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5504012318040765\tXGBRegressor(CombineDFs(SelectPercentile(CombineDFs(input_matrix, input_matrix), SelectPercentile__percentile=85), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5798982806436148\tXGBRegressor(RandomForestRegressor(Normalizer(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
      "\n",
      "Generation 85 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5504012318040765\tXGBRegressor(CombineDFs(SelectPercentile(CombineDFs(input_matrix, input_matrix), SelectPercentile__percentile=85), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5798982806436148\tXGBRegressor(RandomForestRegressor(Normalizer(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:23:08] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 86 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5504012318040765\tXGBRegressor(CombineDFs(SelectPercentile(CombineDFs(input_matrix, input_matrix), SelectPercentile__percentile=85), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5798982806436148\tXGBRegressor(RandomForestRegressor(Normalizer(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 87 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5504012318040765\tXGBRegressor(CombineDFs(SelectPercentile(CombineDFs(input_matrix, input_matrix), SelectPercentile__percentile=85), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5798982806436148\tXGBRegressor(RandomForestRegressor(Normalizer(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 88 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5504012318040765\tXGBRegressor(CombineDFs(SelectPercentile(CombineDFs(input_matrix, input_matrix), SelectPercentile__percentile=85), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5798982806436148\tXGBRegressor(RandomForestRegressor(Normalizer(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:25:50] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 89 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5504012318040765\tXGBRegressor(CombineDFs(SelectPercentile(CombineDFs(input_matrix, input_matrix), SelectPercentile__percentile=85), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5798982806436148\tXGBRegressor(RandomForestRegressor(Normalizer(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:27:00] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:27:01] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 90 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5504012318040765\tXGBRegressor(CombineDFs(SelectPercentile(CombineDFs(input_matrix, input_matrix), SelectPercentile__percentile=85), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5798982806436148\tXGBRegressor(RandomForestRegressor(Normalizer(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 91 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5504012318040765\tXGBRegressor(CombineDFs(SelectPercentile(CombineDFs(input_matrix, input_matrix), SelectPercentile__percentile=85), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5798982806436148\tXGBRegressor(RandomForestRegressor(Normalizer(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:28:52] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 92 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5504012318040765\tXGBRegressor(CombineDFs(SelectPercentile(CombineDFs(input_matrix, input_matrix), SelectPercentile__percentile=85), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5798982806436148\tXGBRegressor(RandomForestRegressor(Normalizer(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 93 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5504012318040765\tXGBRegressor(CombineDFs(SelectPercentile(CombineDFs(input_matrix, input_matrix), SelectPercentile__percentile=85), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5798982806436148\tXGBRegressor(RandomForestRegressor(Normalizer(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:30:41] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [02:30:41] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 94 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5504012318040765\tXGBRegressor(CombineDFs(SelectPercentile(CombineDFs(input_matrix, input_matrix), SelectPercentile__percentile=85), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5798982806436148\tXGBRegressor(RandomForestRegressor(Normalizer(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.\n",
      "\n",
      "Generation 95 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5504012318040765\tXGBRegressor(CombineDFs(SelectPercentile(CombineDFs(input_matrix, input_matrix), SelectPercentile__percentile=85), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5798982806436148\tXGBRegressor(RandomForestRegressor(Normalizer(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 96 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5504012318040765\tXGBRegressor(CombineDFs(SelectPercentile(CombineDFs(input_matrix, input_matrix), SelectPercentile__percentile=85), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5798982806436148\tXGBRegressor(RandomForestRegressor(Normalizer(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:33:22] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:33:25] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:33:28] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 97 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5504012318040765\tXGBRegressor(CombineDFs(SelectPercentile(CombineDFs(input_matrix, input_matrix), SelectPercentile__percentile=85), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5798982806436148\tXGBRegressor(RandomForestRegressor(Normalizer(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:34:23] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 98 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5504012318040765\tXGBRegressor(CombineDFs(SelectPercentile(CombineDFs(input_matrix, input_matrix), SelectPercentile__percentile=85), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5798982806436148\tXGBRegressor(RandomForestRegressor(Normalizer(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:35:18] C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:652: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 99 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5504012318040765\tXGBRegressor(CombineDFs(SelectPercentile(CombineDFs(input_matrix, input_matrix), SelectPercentile__percentile=85), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5798982806436148\tXGBRegressor(RandomForestRegressor(Normalizer(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 100 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5481014219967543\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.5504012318040765\tXGBRegressor(CombineDFs(SelectPercentile(CombineDFs(input_matrix, input_matrix), SelectPercentile__percentile=85), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.5, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.5759315310509069\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.5776179121491589\tXGBRegressor(RandomForestRegressor(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "\n",
      "-5\t0.5798982806436148\tXGBRegressor(RandomForestRegressor(Normalizer(PolynomialFeatures(MaxAbsScaler(input_matrix), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.05, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)\n",
      "training r2 is :  0.9659597456315528\n",
      "testing r2 is :  0.2384250043424332\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=100, population_size=50, verbosity=3, scoring='r2',random_state=1234)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_10.py')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
