{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf976ef",
   "metadata": {},
   "source": [
    "# Regression modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa786006",
   "metadata": {},
   "source": [
    "## 1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "070ef466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\tpot\\builtins\\__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "source": [
    "import pylab as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from tpot import TPOTRegressor\n",
    "\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b5ea18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>solution_concentration</th>\n",
       "      <th>polymer_mw</th>\n",
       "      <th>polymer_mn</th>\n",
       "      <th>polymer_dispersity</th>\n",
       "      <th>hole_mobility</th>\n",
       "      <th>solution_treatment</th>\n",
       "      <th>substrate_pretreatment</th>\n",
       "      <th>post_process</th>\n",
       "      <th>channel_width</th>\n",
       "      <th>channel_length</th>\n",
       "      <th>dielectric_thickness</th>\n",
       "      <th>film_deposition_type_drop</th>\n",
       "      <th>film_deposition_type_spin</th>\n",
       "      <th>dielectric_material_SiO2</th>\n",
       "      <th>electrode_configuration_BGBC</th>\n",
       "      <th>electrode_configuration_BGTC</th>\n",
       "      <th>electrode_configuration_TGBC</th>\n",
       "      <th>gate_material_Other</th>\n",
       "      <th>film_deposition_type_MGC</th>\n",
       "      <th>dielectric_material_other</th>\n",
       "      <th>solvent_boiling_point</th>\n",
       "      <th>blend_conjugated_polymer</th>\n",
       "      <th>insulating_polymer</th>\n",
       "      <th>treatment_type_sam</th>\n",
       "      <th>treatment_type_plasma</th>\n",
       "      <th>treatment_type_uv_ozone</th>\n",
       "      <th>solution_treatment_poor_solvent</th>\n",
       "      <th>solution_treatment_aging</th>\n",
       "      <th>solution_treatment_sonication</th>\n",
       "      <th>solution_treatment_mixing</th>\n",
       "      <th>solution_treatment_uv_irradiation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>11.10</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0000</td>\n",
       "      <td>13.80</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.009480</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0000</td>\n",
       "      <td>23.30</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.005290</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.0</td>\n",
       "      <td>52.0000</td>\n",
       "      <td>22.60</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.005540</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.002990</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.0</td>\n",
       "      <td>270.0000</td>\n",
       "      <td>117.40</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>11.10</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0000</td>\n",
       "      <td>13.80</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0000</td>\n",
       "      <td>23.30</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.0</td>\n",
       "      <td>52.0000</td>\n",
       "      <td>22.60</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.098200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.0</td>\n",
       "      <td>270.0000</td>\n",
       "      <td>117.40</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.143500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>11.10</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0000</td>\n",
       "      <td>13.80</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0000</td>\n",
       "      <td>23.30</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10.0</td>\n",
       "      <td>52.0000</td>\n",
       "      <td>22.60</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10.0</td>\n",
       "      <td>270.0000</td>\n",
       "      <td>117.40</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.005440</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.003630</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.002960</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.009930</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.007570</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.007620</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.003810</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>10.30</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.137800</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>44.70</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.125300</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.004230</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.007930</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.008920</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.002280</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>5.0</td>\n",
       "      <td>91.4810</td>\n",
       "      <td>40.30</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.0000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.035220</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.0000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.051820</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.0000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.054680</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.0000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.103430</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.0000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.0000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.020930</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5.0</td>\n",
       "      <td>207.0000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.4000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10.8000</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>5.0</td>\n",
       "      <td>13.9200</td>\n",
       "      <td>11.60</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>5.0</td>\n",
       "      <td>50.7000</td>\n",
       "      <td>33.80</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.9800</td>\n",
       "      <td>4.60</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>5.0</td>\n",
       "      <td>24.1200</td>\n",
       "      <td>13.40</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>5.0</td>\n",
       "      <td>56.6200</td>\n",
       "      <td>29.80</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.004190</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>5.0</td>\n",
       "      <td>62.0500</td>\n",
       "      <td>36.50</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.009080</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.4800</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>5.0</td>\n",
       "      <td>19.4400</td>\n",
       "      <td>16.20</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.007850</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>5.0</td>\n",
       "      <td>43.5400</td>\n",
       "      <td>31.10</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2.0</td>\n",
       "      <td>48.9600</td>\n",
       "      <td>24.00</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2.0</td>\n",
       "      <td>48.9600</td>\n",
       "      <td>24.00</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2.0</td>\n",
       "      <td>48.9600</td>\n",
       "      <td>24.00</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2.0</td>\n",
       "      <td>48.9600</td>\n",
       "      <td>24.00</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>15.0</td>\n",
       "      <td>5.1000</td>\n",
       "      <td>5.10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>15.0</td>\n",
       "      <td>11.3000</td>\n",
       "      <td>11.30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>15.0</td>\n",
       "      <td>18.6000</td>\n",
       "      <td>18.60</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>15.0</td>\n",
       "      <td>6.6080</td>\n",
       "      <td>5.60</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>15.0</td>\n",
       "      <td>20.4240</td>\n",
       "      <td>13.80</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>15.0</td>\n",
       "      <td>25.6500</td>\n",
       "      <td>19.00</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.1</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.1</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.1</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.1</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.1</td>\n",
       "      <td>37.7400</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.7550</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.5920</td>\n",
       "      <td>5.60</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2.0</td>\n",
       "      <td>21.3840</td>\n",
       "      <td>10.80</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.003990</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2.0</td>\n",
       "      <td>60.7500</td>\n",
       "      <td>27.00</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.7550</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.0</td>\n",
       "      <td>11.5920</td>\n",
       "      <td>5.60</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1.0</td>\n",
       "      <td>21.3840</td>\n",
       "      <td>10.80</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.006870</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1.0</td>\n",
       "      <td>60.7500</td>\n",
       "      <td>27.00</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.4279</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2.7550</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.5</td>\n",
       "      <td>11.5920</td>\n",
       "      <td>5.60</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.5</td>\n",
       "      <td>21.3840</td>\n",
       "      <td>10.80</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.5</td>\n",
       "      <td>60.7500</td>\n",
       "      <td>27.00</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     solution_concentration  polymer_mw  polymer_mn  polymer_dispersity  \\\n",
       "0                      10.0     15.4000       10.30                1.50   \n",
       "1                      10.0     20.0000       11.10                1.80   \n",
       "2                      10.0     22.0000       13.80                1.60   \n",
       "3                      10.0     29.0000       19.30                1.50   \n",
       "4                      10.0     42.0000       23.30                1.80   \n",
       "5                      10.0     52.0000       22.60                2.30   \n",
       "6                      10.0     76.0000       44.70                1.70   \n",
       "7                      10.0    270.0000      117.40                2.30   \n",
       "8                      10.0     15.4000       10.30                1.50   \n",
       "9                      10.0     20.0000       11.10                1.80   \n",
       "10                     10.0     22.0000       13.80                1.60   \n",
       "11                     10.0     29.0000       19.30                1.50   \n",
       "12                     10.0     42.0000       23.30                1.80   \n",
       "13                     10.0     52.0000       22.60                2.30   \n",
       "14                     10.0     76.0000       44.70                1.70   \n",
       "15                     10.0    270.0000      117.40                2.30   \n",
       "16                     10.0     15.4000       10.30                1.50   \n",
       "17                     10.0     20.0000       11.10                1.80   \n",
       "18                     10.0     22.0000       13.80                1.60   \n",
       "19                     10.0     29.0000       19.30                1.50   \n",
       "20                     10.0     42.0000       23.30                1.80   \n",
       "21                     10.0     52.0000       22.60                2.30   \n",
       "22                     10.0     76.0000       44.70                1.70   \n",
       "23                     10.0    270.0000      117.40                2.30   \n",
       "24                     10.0     76.0000       44.70                1.70   \n",
       "25                     10.0     76.0000       44.70                1.70   \n",
       "26                     10.0     76.0000       44.70                1.70   \n",
       "27                     10.0     76.0000       44.70                1.70   \n",
       "28                     10.0     29.0000       19.30                1.50   \n",
       "29                     10.0     29.0000       19.30                1.50   \n",
       "30                     10.0     29.0000       19.30                1.50   \n",
       "31                     10.0     15.4000       10.30                1.50   \n",
       "32                     10.0     15.4000       10.30                1.50   \n",
       "33                     10.0     15.4000       10.30                1.50   \n",
       "34                     10.0     15.4000       10.30                1.50   \n",
       "35                     10.0     29.0000       19.30                1.50   \n",
       "36                     10.0     29.0000       19.30                1.50   \n",
       "37                     10.0     29.0000       19.30                1.50   \n",
       "38                     10.0     15.4000       10.30                1.50   \n",
       "39                     10.0     15.4000       10.30                1.50   \n",
       "40                     10.0     15.4000       10.30                1.50   \n",
       "41                     10.0     76.0000       44.70                1.70   \n",
       "42                     10.0     76.0000       44.70                1.70   \n",
       "43                     10.0     76.0000       44.70                1.70   \n",
       "44                      5.0     91.4810       40.30                2.27   \n",
       "45                      5.0     91.4810       40.30                2.27   \n",
       "46                      5.0     91.4810       40.30                2.27   \n",
       "47                      5.0     91.4810       40.30                2.27   \n",
       "48                      5.0     91.4810       40.30                2.27   \n",
       "49                      5.0     91.4810       40.30                2.27   \n",
       "50                      5.0     91.4810       40.30                2.27   \n",
       "51                      5.0     91.4810       40.30                2.27   \n",
       "52                      5.0     91.4810       40.30                2.27   \n",
       "53                      5.0     91.4810       40.30                2.27   \n",
       "54                      5.0     91.4810       40.30                2.27   \n",
       "55                      5.0     91.4810       40.30                2.27   \n",
       "56                      5.0     91.4810       40.30                2.27   \n",
       "57                      5.0     91.4810       40.30                2.27   \n",
       "58                      5.0     91.4810       40.30                2.27   \n",
       "59                      5.0     91.4810       40.30                2.27   \n",
       "60                      5.0     91.4810       40.30                2.27   \n",
       "61                      5.0    207.0000       90.00                2.30   \n",
       "62                      5.0    207.0000       90.00                2.30   \n",
       "63                      5.0    207.0000       90.00                2.30   \n",
       "64                      5.0    207.0000       90.00                2.30   \n",
       "65                      5.0    207.0000       90.00                2.30   \n",
       "66                      5.0    207.0000       90.00                2.30   \n",
       "67                      5.0    207.0000       90.00                2.30   \n",
       "68                      5.0      4.4000        4.00                1.10   \n",
       "69                      5.0     10.8000        9.00                1.20   \n",
       "70                      5.0     13.9200       11.60                1.20   \n",
       "71                      5.0     50.7000       33.80                1.50   \n",
       "72                      5.0      5.9800        4.60                1.30   \n",
       "73                      5.0     24.1200       13.40                1.80   \n",
       "74                      5.0     56.6200       29.80                1.90   \n",
       "75                      5.0     62.0500       36.50                1.70   \n",
       "76                      5.0      4.4800        3.20                1.40   \n",
       "77                      5.0     19.4400       16.20                1.20   \n",
       "78                      5.0     43.5400       31.10                1.40   \n",
       "79                      2.0     48.9600       24.00                2.04   \n",
       "80                      2.0     48.9600       24.00                2.04   \n",
       "81                      2.0     48.9600       24.00                2.04   \n",
       "82                      2.0     48.9600       24.00                2.04   \n",
       "83                     15.0      5.1000        5.10                1.00   \n",
       "84                     15.0     11.3000       11.30                1.00   \n",
       "85                     15.0     18.6000       18.60                1.00   \n",
       "86                     15.0      6.6080        5.60                1.18   \n",
       "87                     15.0     20.4240       13.80                1.48   \n",
       "88                     15.0     25.6500       19.00                1.35   \n",
       "89                      1.0     37.7400       25.50                1.48   \n",
       "90                      1.0     37.7400       25.50                1.48   \n",
       "91                      1.0     37.7400       25.50                1.48   \n",
       "92                      1.0     37.7400       25.50                1.48   \n",
       "93                      1.0     37.7400       25.50                1.48   \n",
       "94                      1.0     37.7400       25.50                1.48   \n",
       "95                      1.0     37.7400       25.50                1.48   \n",
       "96                      1.0     37.7400       25.50                1.48   \n",
       "97                      0.1     37.7400       25.50                1.48   \n",
       "98                      0.1     37.7400       25.50                1.48   \n",
       "99                      0.1     37.7400       25.50                1.48   \n",
       "100                     0.1     37.7400       25.50                1.48   \n",
       "101                     0.1     37.7400       25.50                1.48   \n",
       "102                     2.0      2.7550        1.90                1.45   \n",
       "103                     2.0     11.5920        5.60                2.07   \n",
       "104                     2.0     21.3840       10.80                1.98   \n",
       "105                     2.0     60.7500       27.00                2.25   \n",
       "106                     1.0      2.7550        1.90                1.45   \n",
       "107                     1.0     11.5920        5.60                2.07   \n",
       "108                     1.0     21.3840       10.80                1.98   \n",
       "109                     1.0     60.7500       27.00                2.25   \n",
       "110                     0.5      1.4279        1.09                1.31   \n",
       "111                     0.5      2.7550        1.90                1.45   \n",
       "112                     0.5     11.5920        5.60                2.07   \n",
       "113                     0.5     21.3840       10.80                1.98   \n",
       "114                     0.5     60.7500       27.00                2.25   \n",
       "\n",
       "     hole_mobility  solution_treatment  substrate_pretreatment  post_process  \\\n",
       "0         0.007600                   1                       1             1   \n",
       "1         0.004960                   1                       1             1   \n",
       "2         0.009480                   1                       1             1   \n",
       "3         0.012000                   1                       1             1   \n",
       "4         0.005290                   1                       1             1   \n",
       "5         0.005540                   1                       1             1   \n",
       "6         0.002990                   1                       1             1   \n",
       "7         0.008100                   1                       1             1   \n",
       "8         0.001700                   1                       1             1   \n",
       "9         0.001490                   1                       1             1   \n",
       "10        0.030500                   1                       1             1   \n",
       "11        0.063100                   1                       1             1   \n",
       "12        0.077500                   1                       1             1   \n",
       "13        0.098200                   1                       1             1   \n",
       "14        0.122600                   1                       1             1   \n",
       "15        0.143500                   1                       1             1   \n",
       "16        0.009940                   1                       1             1   \n",
       "17        0.018700                   1                       1             1   \n",
       "18        0.027300                   1                       1             1   \n",
       "19        0.053900                   1                       1             1   \n",
       "20        0.061200                   1                       1             1   \n",
       "21        0.066200                   1                       1             1   \n",
       "22        0.065200                   1                       1             1   \n",
       "23        0.081300                   1                       1             1   \n",
       "24        0.014500                   1                       1             1   \n",
       "25        0.005440                   1                       1             1   \n",
       "26        0.003630                   1                       1             1   \n",
       "27        0.002960                   1                       1             1   \n",
       "28        0.022900                   1                       1             1   \n",
       "29        0.018300                   1                       1             1   \n",
       "30        0.014000                   1                       1             1   \n",
       "31        0.017100                   1                       1             1   \n",
       "32        0.012000                   1                       1             1   \n",
       "33        0.009930                   1                       1             1   \n",
       "34        0.007570                   1                       1             1   \n",
       "35        0.077000                   1                       1             1   \n",
       "36        0.068000                   1                       1             1   \n",
       "37        0.065000                   1                       1             1   \n",
       "38        0.007620                   1                       1             1   \n",
       "39        0.003810                   1                       1             1   \n",
       "40        0.002550                   1                       1             1   \n",
       "41        0.137800                   1                       1             1   \n",
       "42        0.135000                   1                       1             1   \n",
       "43        0.125300                   1                       1             1   \n",
       "44        0.004230                   0                       0             0   \n",
       "45        0.007930                   1                       0             0   \n",
       "46        0.010700                   1                       0             0   \n",
       "47        0.016500                   1                       0             0   \n",
       "48        0.010100                   1                       0             0   \n",
       "49        0.008920                   1                       0             0   \n",
       "50        0.004700                   1                       0             0   \n",
       "51        0.004060                   1                       0             0   \n",
       "52        0.002140                   1                       0             0   \n",
       "53        0.002280                   1                       0             0   \n",
       "54        0.002790                   1                       0             0   \n",
       "55        0.000556                   0                       0             0   \n",
       "56        0.000413                   1                       0             0   \n",
       "57        0.000426                   1                       0             0   \n",
       "58        0.000254                   1                       0             0   \n",
       "59        0.000309                   1                       0             0   \n",
       "60        0.000377                   1                       0             0   \n",
       "61        0.035220                   0                       0             0   \n",
       "62        0.051820                   1                       0             0   \n",
       "63        0.054680                   1                       0             0   \n",
       "64        0.103430                   1                       0             0   \n",
       "65        0.053900                   1                       0             0   \n",
       "66        0.020930                   1                       0             0   \n",
       "67        0.011230                   1                       0             0   \n",
       "68        0.000005                   0                       1             0   \n",
       "69        0.000356                   0                       1             0   \n",
       "70        0.000735                   0                       1             0   \n",
       "71        0.007480                   0                       1             0   \n",
       "72        0.000016                   0                       1             0   \n",
       "73        0.000537                   0                       1             0   \n",
       "74        0.004190                   0                       1             0   \n",
       "75        0.009080                   0                       1             0   \n",
       "76        0.000002                   0                       1             0   \n",
       "77        0.007850                   0                       1             0   \n",
       "78        0.007670                   0                       1             0   \n",
       "79        0.023900                   1                       1             0   \n",
       "80        0.030500                   1                       1             0   \n",
       "81        0.033500                   1                       1             0   \n",
       "82        0.014500                   1                       1             0   \n",
       "83        0.000046                   0                       1             0   \n",
       "84        0.019000                   0                       1             0   \n",
       "85        0.016000                   0                       1             0   \n",
       "86        0.000001                   0                       1             0   \n",
       "87        0.000609                   0                       1             0   \n",
       "88        0.010600                   0                       1             0   \n",
       "89        0.023000                   0                       0             0   \n",
       "90        0.015000                   0                       0             0   \n",
       "91        0.001100                   0                       0             0   \n",
       "92        0.001600                   0                       1             0   \n",
       "93        0.002400                   0                       0             0   \n",
       "94        0.015000                   0                       0             0   \n",
       "95        0.085000                   0                       0             0   \n",
       "96        0.150000                   0                       1             0   \n",
       "97        0.000150                   0                       0             0   \n",
       "98        0.001400                   0                       0             0   \n",
       "99        0.021000                   0                       0             0   \n",
       "100       0.002300                   0                       1             0   \n",
       "101       0.250000                   0                       1             1   \n",
       "102       0.000049                   0                       1             0   \n",
       "103       0.000551                   0                       1             0   \n",
       "104       0.003990                   0                       1             0   \n",
       "105       0.022900                   0                       1             0   \n",
       "106       0.000114                   0                       0             0   \n",
       "107       0.000912                   0                       0             0   \n",
       "108       0.006870                   0                       0             0   \n",
       "109       0.017000                   0                       0             0   \n",
       "110       0.000015                   1                       1             0   \n",
       "111       0.000034                   1                       1             0   \n",
       "112       0.000709                   1                       1             0   \n",
       "113       0.004140                   1                       1             0   \n",
       "114       0.036400                   1                       1             0   \n",
       "\n",
       "     channel_width  channel_length  dielectric_thickness  \\\n",
       "0            10000            20.0                   230   \n",
       "1            10000            20.0                   230   \n",
       "2            10000            20.0                   200   \n",
       "3            10000            20.0                   200   \n",
       "4            10000            20.0                   200   \n",
       "5            10000            20.0                   200   \n",
       "6            10000            20.0                   200   \n",
       "7            10000            20.0                   200   \n",
       "8            10000            20.0                   200   \n",
       "9            10000            20.0                   200   \n",
       "10           10000            20.0                   200   \n",
       "11           10000            20.0                   200   \n",
       "12           10000            20.0                   200   \n",
       "13           10000            20.0                   200   \n",
       "14           10000            20.0                   200   \n",
       "15           10000            20.0                   200   \n",
       "16           10000            20.0                   200   \n",
       "17           10000            20.0                   200   \n",
       "18           10000            20.0                   200   \n",
       "19           10000            20.0                   200   \n",
       "20           10000            20.0                   200   \n",
       "21           10000            20.0                   200   \n",
       "22           10000            20.0                   200   \n",
       "23           10000            20.0                   200   \n",
       "24           10000             2.0                   200   \n",
       "25           10000             5.0                   200   \n",
       "26           10000            10.0                   200   \n",
       "27           10000            20.0                   200   \n",
       "28           10000             2.0                   200   \n",
       "29           10000             5.0                   200   \n",
       "30           10000            10.0                   200   \n",
       "31           10000             2.0                   200   \n",
       "32           10000             5.0                   200   \n",
       "33           10000            10.0                   200   \n",
       "34           10000            20.0                   200   \n",
       "35           10000             2.0                   200   \n",
       "36           10000             5.0                   200   \n",
       "37           10000            10.0                   200   \n",
       "38           10000             2.0                   200   \n",
       "39           10000             5.0                   200   \n",
       "40           10000            10.0                   200   \n",
       "41           10000             2.0                   200   \n",
       "42           10000             5.0                   200   \n",
       "43           10000            10.0                   200   \n",
       "44            2000            50.0                   300   \n",
       "45            2000            50.0                   300   \n",
       "46            2000            50.0                   300   \n",
       "47            2000            50.0                   300   \n",
       "48            2000            50.0                   300   \n",
       "49            2000            50.0                   300   \n",
       "50            2000            50.0                   300   \n",
       "51            2000            50.0                   300   \n",
       "52            2000            50.0                   300   \n",
       "53            2000            50.0                   300   \n",
       "54            2000            50.0                   300   \n",
       "55            2000            50.0                   300   \n",
       "56            2000            50.0                   300   \n",
       "57            2000            50.0                   300   \n",
       "58            2000            50.0                   300   \n",
       "59            2000            50.0                   300   \n",
       "60            2000            50.0                   300   \n",
       "61            2000            50.0                   300   \n",
       "62            2000            50.0                   300   \n",
       "63            2000            50.0                   300   \n",
       "64            2000            50.0                   300   \n",
       "65            2000            50.0                   300   \n",
       "66            2000            50.0                   300   \n",
       "67            2000            50.0                   300   \n",
       "68           40000             5.0                   230   \n",
       "69           40000             5.0                   230   \n",
       "70           40000             5.0                   230   \n",
       "71           40000             5.0                   230   \n",
       "72           40000             5.0                   230   \n",
       "73           40000             5.0                   230   \n",
       "74           40000             5.0                   230   \n",
       "75           40000             5.0                   230   \n",
       "76           40000             5.0                   230   \n",
       "77           40000             5.0                   230   \n",
       "78           40000             5.0                   230   \n",
       "79             800            20.0                   200   \n",
       "80             800            20.0                   200   \n",
       "81             800            20.0                   200   \n",
       "82             800            20.0                   200   \n",
       "83          148500           100.0                   200   \n",
       "84          148500           100.0                   200   \n",
       "85          148500           100.0                   200   \n",
       "86          148500           100.0                   200   \n",
       "87          148500           100.0                   200   \n",
       "88          148500           100.0                   200   \n",
       "89            1000             7.5                   200   \n",
       "90            1000             7.5                   200   \n",
       "91            1000             7.5                   200   \n",
       "92            1000             7.5                   200   \n",
       "93            1000             7.5                   200   \n",
       "94            1000             7.5                   200   \n",
       "95            1000             7.5                   200   \n",
       "96            1000             7.5                   200   \n",
       "97            1000             7.5                   200   \n",
       "98            1000             7.5                   200   \n",
       "99            1000             7.5                   200   \n",
       "100           1000             7.5                   200   \n",
       "101           1000             7.5                   200   \n",
       "102           9000            20.0                   300   \n",
       "103           9000            20.0                   300   \n",
       "104           9000            20.0                   300   \n",
       "105           9000            20.0                   300   \n",
       "106           9000            20.0                   300   \n",
       "107           9000            20.0                   300   \n",
       "108           9000            20.0                   300   \n",
       "109           9000            20.0                   300   \n",
       "110           9000            20.0                   300   \n",
       "111           9000            20.0                   300   \n",
       "112           9000            20.0                   300   \n",
       "113           9000            20.0                   300   \n",
       "114           9000            20.0                   300   \n",
       "\n",
       "     film_deposition_type_drop  film_deposition_type_spin  \\\n",
       "0                            0                          1   \n",
       "1                            0                          1   \n",
       "2                            0                          1   \n",
       "3                            0                          1   \n",
       "4                            0                          1   \n",
       "5                            0                          1   \n",
       "6                            0                          1   \n",
       "7                            0                          1   \n",
       "8                            0                          1   \n",
       "9                            0                          1   \n",
       "10                           0                          1   \n",
       "11                           0                          1   \n",
       "12                           0                          1   \n",
       "13                           0                          1   \n",
       "14                           0                          1   \n",
       "15                           0                          1   \n",
       "16                           1                          0   \n",
       "17                           1                          0   \n",
       "18                           1                          0   \n",
       "19                           1                          0   \n",
       "20                           1                          0   \n",
       "21                           1                          0   \n",
       "22                           1                          0   \n",
       "23                           1                          0   \n",
       "24                           0                          1   \n",
       "25                           0                          1   \n",
       "26                           0                          1   \n",
       "27                           0                          1   \n",
       "28                           0                          1   \n",
       "29                           0                          1   \n",
       "30                           0                          1   \n",
       "31                           0                          1   \n",
       "32                           0                          1   \n",
       "33                           0                          1   \n",
       "34                           0                          1   \n",
       "35                           0                          1   \n",
       "36                           0                          1   \n",
       "37                           0                          1   \n",
       "38                           0                          1   \n",
       "39                           0                          1   \n",
       "40                           0                          1   \n",
       "41                           0                          1   \n",
       "42                           0                          1   \n",
       "43                           0                          1   \n",
       "44                           0                          1   \n",
       "45                           0                          1   \n",
       "46                           0                          1   \n",
       "47                           0                          1   \n",
       "48                           0                          1   \n",
       "49                           0                          1   \n",
       "50                           0                          1   \n",
       "51                           0                          1   \n",
       "52                           0                          1   \n",
       "53                           0                          1   \n",
       "54                           0                          1   \n",
       "55                           0                          1   \n",
       "56                           0                          1   \n",
       "57                           0                          1   \n",
       "58                           0                          1   \n",
       "59                           0                          1   \n",
       "60                           0                          1   \n",
       "61                           0                          1   \n",
       "62                           0                          1   \n",
       "63                           0                          1   \n",
       "64                           0                          1   \n",
       "65                           0                          1   \n",
       "66                           0                          1   \n",
       "67                           0                          1   \n",
       "68                           0                          1   \n",
       "69                           0                          1   \n",
       "70                           0                          1   \n",
       "71                           0                          1   \n",
       "72                           0                          1   \n",
       "73                           0                          1   \n",
       "74                           0                          1   \n",
       "75                           0                          1   \n",
       "76                           0                          1   \n",
       "77                           0                          1   \n",
       "78                           0                          1   \n",
       "79                           1                          0   \n",
       "80                           1                          0   \n",
       "81                           1                          0   \n",
       "82                           1                          0   \n",
       "83                           0                          1   \n",
       "84                           0                          1   \n",
       "85                           0                          1   \n",
       "86                           0                          1   \n",
       "87                           0                          0   \n",
       "88                           0                          0   \n",
       "89                           0                          1   \n",
       "90                           0                          1   \n",
       "91                           0                          1   \n",
       "92                           0                          1   \n",
       "93                           0                          0   \n",
       "94                           0                          0   \n",
       "95                           0                          0   \n",
       "96                           0                          0   \n",
       "97                           1                          0   \n",
       "98                           1                          0   \n",
       "99                           1                          0   \n",
       "100                          1                          0   \n",
       "101                          1                          0   \n",
       "102                          0                          1   \n",
       "103                          0                          1   \n",
       "104                          0                          1   \n",
       "105                          0                          1   \n",
       "106                          0                          0   \n",
       "107                          0                          0   \n",
       "108                          0                          0   \n",
       "109                          0                          0   \n",
       "110                          1                          0   \n",
       "111                          1                          0   \n",
       "112                          1                          0   \n",
       "113                          1                          0   \n",
       "114                          1                          0   \n",
       "\n",
       "     dielectric_material_SiO2  electrode_configuration_BGBC  \\\n",
       "0                           1                             1   \n",
       "1                           1                             1   \n",
       "2                           1                             1   \n",
       "3                           1                             1   \n",
       "4                           1                             1   \n",
       "5                           1                             1   \n",
       "6                           1                             1   \n",
       "7                           1                             1   \n",
       "8                           1                             1   \n",
       "9                           1                             1   \n",
       "10                          1                             1   \n",
       "11                          1                             1   \n",
       "12                          1                             1   \n",
       "13                          1                             1   \n",
       "14                          1                             1   \n",
       "15                          1                             1   \n",
       "16                          1                             1   \n",
       "17                          1                             1   \n",
       "18                          1                             1   \n",
       "19                          1                             1   \n",
       "20                          1                             1   \n",
       "21                          1                             1   \n",
       "22                          1                             1   \n",
       "23                          1                             1   \n",
       "24                          1                             1   \n",
       "25                          1                             1   \n",
       "26                          1                             1   \n",
       "27                          1                             1   \n",
       "28                          1                             1   \n",
       "29                          1                             1   \n",
       "30                          1                             1   \n",
       "31                          1                             1   \n",
       "32                          1                             1   \n",
       "33                          1                             1   \n",
       "34                          1                             1   \n",
       "35                          1                             1   \n",
       "36                          1                             1   \n",
       "37                          1                             1   \n",
       "38                          1                             1   \n",
       "39                          1                             1   \n",
       "40                          1                             1   \n",
       "41                          1                             1   \n",
       "42                          1                             1   \n",
       "43                          1                             1   \n",
       "44                          1                             1   \n",
       "45                          1                             1   \n",
       "46                          1                             1   \n",
       "47                          1                             1   \n",
       "48                          1                             1   \n",
       "49                          1                             1   \n",
       "50                          1                             1   \n",
       "51                          1                             1   \n",
       "52                          1                             1   \n",
       "53                          1                             1   \n",
       "54                          1                             1   \n",
       "55                          1                             1   \n",
       "56                          1                             1   \n",
       "57                          1                             1   \n",
       "58                          1                             1   \n",
       "59                          1                             1   \n",
       "60                          1                             1   \n",
       "61                          1                             1   \n",
       "62                          1                             1   \n",
       "63                          1                             1   \n",
       "64                          1                             1   \n",
       "65                          1                             1   \n",
       "66                          1                             1   \n",
       "67                          1                             1   \n",
       "68                          1                             1   \n",
       "69                          1                             1   \n",
       "70                          1                             1   \n",
       "71                          1                             1   \n",
       "72                          1                             1   \n",
       "73                          1                             1   \n",
       "74                          1                             1   \n",
       "75                          1                             1   \n",
       "76                          1                             1   \n",
       "77                          1                             1   \n",
       "78                          1                             1   \n",
       "79                          1                             1   \n",
       "80                          1                             1   \n",
       "81                          1                             1   \n",
       "82                          1                             1   \n",
       "83                          1                             0   \n",
       "84                          1                             0   \n",
       "85                          1                             0   \n",
       "86                          1                             0   \n",
       "87                          1                             0   \n",
       "88                          1                             0   \n",
       "89                          1                             1   \n",
       "90                          1                             1   \n",
       "91                          1                             1   \n",
       "92                          1                             1   \n",
       "93                          1                             1   \n",
       "94                          1                             1   \n",
       "95                          1                             1   \n",
       "96                          1                             1   \n",
       "97                          1                             1   \n",
       "98                          1                             1   \n",
       "99                          1                             1   \n",
       "100                         1                             1   \n",
       "101                         1                             1   \n",
       "102                         1                             1   \n",
       "103                         1                             1   \n",
       "104                         1                             1   \n",
       "105                         1                             1   \n",
       "106                         1                             1   \n",
       "107                         1                             1   \n",
       "108                         1                             1   \n",
       "109                         1                             1   \n",
       "110                         1                             1   \n",
       "111                         1                             1   \n",
       "112                         1                             1   \n",
       "113                         1                             1   \n",
       "114                         1                             1   \n",
       "\n",
       "     electrode_configuration_BGTC  electrode_configuration_TGBC  \\\n",
       "0                               0                             0   \n",
       "1                               0                             0   \n",
       "2                               0                             0   \n",
       "3                               0                             0   \n",
       "4                               0                             0   \n",
       "5                               0                             0   \n",
       "6                               0                             0   \n",
       "7                               0                             0   \n",
       "8                               0                             0   \n",
       "9                               0                             0   \n",
       "10                              0                             0   \n",
       "11                              0                             0   \n",
       "12                              0                             0   \n",
       "13                              0                             0   \n",
       "14                              0                             0   \n",
       "15                              0                             0   \n",
       "16                              0                             0   \n",
       "17                              0                             0   \n",
       "18                              0                             0   \n",
       "19                              0                             0   \n",
       "20                              0                             0   \n",
       "21                              0                             0   \n",
       "22                              0                             0   \n",
       "23                              0                             0   \n",
       "24                              0                             0   \n",
       "25                              0                             0   \n",
       "26                              0                             0   \n",
       "27                              0                             0   \n",
       "28                              0                             0   \n",
       "29                              0                             0   \n",
       "30                              0                             0   \n",
       "31                              0                             0   \n",
       "32                              0                             0   \n",
       "33                              0                             0   \n",
       "34                              0                             0   \n",
       "35                              0                             0   \n",
       "36                              0                             0   \n",
       "37                              0                             0   \n",
       "38                              0                             0   \n",
       "39                              0                             0   \n",
       "40                              0                             0   \n",
       "41                              0                             0   \n",
       "42                              0                             0   \n",
       "43                              0                             0   \n",
       "44                              0                             0   \n",
       "45                              0                             0   \n",
       "46                              0                             0   \n",
       "47                              0                             0   \n",
       "48                              0                             0   \n",
       "49                              0                             0   \n",
       "50                              0                             0   \n",
       "51                              0                             0   \n",
       "52                              0                             0   \n",
       "53                              0                             0   \n",
       "54                              0                             0   \n",
       "55                              0                             0   \n",
       "56                              0                             0   \n",
       "57                              0                             0   \n",
       "58                              0                             0   \n",
       "59                              0                             0   \n",
       "60                              0                             0   \n",
       "61                              0                             0   \n",
       "62                              0                             0   \n",
       "63                              0                             0   \n",
       "64                              0                             0   \n",
       "65                              0                             0   \n",
       "66                              0                             0   \n",
       "67                              0                             0   \n",
       "68                              0                             0   \n",
       "69                              0                             0   \n",
       "70                              0                             0   \n",
       "71                              0                             0   \n",
       "72                              0                             0   \n",
       "73                              0                             0   \n",
       "74                              0                             0   \n",
       "75                              0                             0   \n",
       "76                              0                             0   \n",
       "77                              0                             0   \n",
       "78                              0                             0   \n",
       "79                              0                             0   \n",
       "80                              0                             0   \n",
       "81                              0                             0   \n",
       "82                              0                             0   \n",
       "83                              1                             0   \n",
       "84                              1                             0   \n",
       "85                              1                             0   \n",
       "86                              1                             0   \n",
       "87                              1                             0   \n",
       "88                              1                             0   \n",
       "89                              0                             0   \n",
       "90                              0                             0   \n",
       "91                              0                             0   \n",
       "92                              0                             0   \n",
       "93                              0                             0   \n",
       "94                              0                             0   \n",
       "95                              0                             0   \n",
       "96                              0                             0   \n",
       "97                              0                             0   \n",
       "98                              0                             0   \n",
       "99                              0                             0   \n",
       "100                             0                             0   \n",
       "101                             0                             0   \n",
       "102                             0                             0   \n",
       "103                             0                             0   \n",
       "104                             0                             0   \n",
       "105                             0                             0   \n",
       "106                             0                             0   \n",
       "107                             0                             0   \n",
       "108                             0                             0   \n",
       "109                             0                             0   \n",
       "110                             0                             0   \n",
       "111                             0                             0   \n",
       "112                             0                             0   \n",
       "113                             0                             0   \n",
       "114                             0                             0   \n",
       "\n",
       "     gate_material_Other  film_deposition_type_MGC  dielectric_material_other  \\\n",
       "0                      0                         0                          0   \n",
       "1                      0                         0                          0   \n",
       "2                      0                         0                          0   \n",
       "3                      0                         0                          0   \n",
       "4                      0                         0                          0   \n",
       "5                      0                         0                          0   \n",
       "6                      0                         0                          0   \n",
       "7                      0                         0                          0   \n",
       "8                      0                         0                          0   \n",
       "9                      0                         0                          0   \n",
       "10                     0                         0                          0   \n",
       "11                     0                         0                          0   \n",
       "12                     0                         0                          0   \n",
       "13                     0                         0                          0   \n",
       "14                     0                         0                          0   \n",
       "15                     0                         0                          0   \n",
       "16                     0                         0                          0   \n",
       "17                     0                         0                          0   \n",
       "18                     0                         0                          0   \n",
       "19                     0                         0                          0   \n",
       "20                     0                         0                          0   \n",
       "21                     0                         0                          0   \n",
       "22                     0                         0                          0   \n",
       "23                     0                         0                          0   \n",
       "24                     0                         0                          0   \n",
       "25                     0                         0                          0   \n",
       "26                     0                         0                          0   \n",
       "27                     0                         0                          0   \n",
       "28                     0                         0                          0   \n",
       "29                     0                         0                          0   \n",
       "30                     0                         0                          0   \n",
       "31                     0                         0                          0   \n",
       "32                     0                         0                          0   \n",
       "33                     0                         0                          0   \n",
       "34                     0                         0                          0   \n",
       "35                     0                         0                          0   \n",
       "36                     0                         0                          0   \n",
       "37                     0                         0                          0   \n",
       "38                     0                         0                          0   \n",
       "39                     0                         0                          0   \n",
       "40                     0                         0                          0   \n",
       "41                     0                         0                          0   \n",
       "42                     0                         0                          0   \n",
       "43                     0                         0                          0   \n",
       "44                     0                         0                          0   \n",
       "45                     0                         0                          0   \n",
       "46                     0                         0                          0   \n",
       "47                     0                         0                          0   \n",
       "48                     0                         0                          0   \n",
       "49                     0                         0                          0   \n",
       "50                     0                         0                          0   \n",
       "51                     0                         0                          0   \n",
       "52                     0                         0                          0   \n",
       "53                     0                         0                          0   \n",
       "54                     0                         0                          0   \n",
       "55                     0                         0                          0   \n",
       "56                     0                         0                          0   \n",
       "57                     0                         0                          0   \n",
       "58                     0                         0                          0   \n",
       "59                     0                         0                          0   \n",
       "60                     0                         0                          0   \n",
       "61                     0                         0                          0   \n",
       "62                     0                         0                          0   \n",
       "63                     0                         0                          0   \n",
       "64                     0                         0                          0   \n",
       "65                     0                         0                          0   \n",
       "66                     0                         0                          0   \n",
       "67                     0                         0                          0   \n",
       "68                     0                         0                          0   \n",
       "69                     0                         0                          0   \n",
       "70                     0                         0                          0   \n",
       "71                     0                         0                          0   \n",
       "72                     0                         0                          0   \n",
       "73                     0                         0                          0   \n",
       "74                     0                         0                          0   \n",
       "75                     0                         0                          0   \n",
       "76                     0                         0                          0   \n",
       "77                     0                         0                          0   \n",
       "78                     0                         0                          0   \n",
       "79                     0                         0                          0   \n",
       "80                     0                         0                          0   \n",
       "81                     0                         0                          0   \n",
       "82                     0                         0                          0   \n",
       "83                     0                         0                          0   \n",
       "84                     0                         0                          0   \n",
       "85                     0                         0                          0   \n",
       "86                     0                         0                          0   \n",
       "87                     0                         1                          0   \n",
       "88                     0                         1                          0   \n",
       "89                     0                         0                          0   \n",
       "90                     0                         0                          0   \n",
       "91                     0                         0                          0   \n",
       "92                     0                         0                          0   \n",
       "93                     0                         1                          0   \n",
       "94                     0                         1                          0   \n",
       "95                     0                         1                          0   \n",
       "96                     0                         1                          0   \n",
       "97                     0                         0                          0   \n",
       "98                     0                         0                          0   \n",
       "99                     0                         0                          0   \n",
       "100                    0                         0                          0   \n",
       "101                    0                         0                          0   \n",
       "102                    0                         0                          0   \n",
       "103                    0                         0                          0   \n",
       "104                    0                         0                          0   \n",
       "105                    0                         0                          0   \n",
       "106                    0                         1                          0   \n",
       "107                    0                         1                          0   \n",
       "108                    0                         1                          0   \n",
       "109                    0                         1                          0   \n",
       "110                    0                         0                          0   \n",
       "111                    0                         0                          0   \n",
       "112                    0                         0                          0   \n",
       "113                    0                         0                          0   \n",
       "114                    0                         0                          0   \n",
       "\n",
       "     solvent_boiling_point  blend_conjugated_polymer  insulating_polymer  \\\n",
       "0                       62                         0                   0   \n",
       "1                       62                         0                   0   \n",
       "2                       62                         0                   0   \n",
       "3                       62                         0                   0   \n",
       "4                       62                         0                   0   \n",
       "5                       62                         0                   0   \n",
       "6                       62                         0                   0   \n",
       "7                       62                         0                   0   \n",
       "8                      213                         0                   0   \n",
       "9                      213                         0                   0   \n",
       "10                     213                         0                   0   \n",
       "11                     213                         0                   0   \n",
       "12                     213                         0                   0   \n",
       "13                     213                         0                   0   \n",
       "14                     213                         0                   0   \n",
       "15                     213                         0                   0   \n",
       "16                      62                         0                   0   \n",
       "17                      62                         0                   0   \n",
       "18                      62                         0                   0   \n",
       "19                      62                         0                   0   \n",
       "20                      62                         0                   0   \n",
       "21                      62                         0                   0   \n",
       "22                      62                         0                   0   \n",
       "23                      62                         0                   0   \n",
       "24                      62                         0                   0   \n",
       "25                      62                         0                   0   \n",
       "26                      62                         0                   0   \n",
       "27                      62                         0                   0   \n",
       "28                      62                         0                   0   \n",
       "29                      62                         0                   0   \n",
       "30                      62                         0                   0   \n",
       "31                      62                         0                   0   \n",
       "32                      62                         0                   0   \n",
       "33                      62                         0                   0   \n",
       "34                      62                         0                   0   \n",
       "35                     213                         0                   0   \n",
       "36                     213                         0                   0   \n",
       "37                     213                         0                   0   \n",
       "38                     213                         0                   0   \n",
       "39                     213                         0                   0   \n",
       "40                     213                         0                   0   \n",
       "41                     213                         0                   0   \n",
       "42                     213                         0                   0   \n",
       "43                     213                         0                   0   \n",
       "44                      62                         0                   0   \n",
       "45                      62                         0                   0   \n",
       "46                      62                         0                   0   \n",
       "47                      62                         0                   0   \n",
       "48                      62                         0                   0   \n",
       "49                      62                         0                   0   \n",
       "50                      62                         0                   0   \n",
       "51                      62                         0                   0   \n",
       "52                      62                         0                   0   \n",
       "53                      62                         0                   0   \n",
       "54                      62                         0                   0   \n",
       "55                     132                         0                   0   \n",
       "56                     132                         0                   0   \n",
       "57                     132                         0                   0   \n",
       "58                     132                         0                   0   \n",
       "59                     132                         0                   0   \n",
       "60                     132                         0                   0   \n",
       "61                      62                         0                   0   \n",
       "62                      62                         0                   0   \n",
       "63                      62                         0                   0   \n",
       "64                      62                         0                   0   \n",
       "65                      62                         0                   0   \n",
       "66                      62                         0                   0   \n",
       "67                      62                         0                   0   \n",
       "68                      62                         0                   0   \n",
       "69                      62                         0                   0   \n",
       "70                      62                         0                   0   \n",
       "71                      62                         0                   0   \n",
       "72                      62                         0                   0   \n",
       "73                      62                         0                   0   \n",
       "74                      62                         0                   0   \n",
       "75                      62                         0                   0   \n",
       "76                      62                         0                   0   \n",
       "77                      62                         0                   0   \n",
       "78                      62                         0                   0   \n",
       "79                     138                         0                   0   \n",
       "80                     111                         0                   0   \n",
       "81                     145                         0                   0   \n",
       "82                      80                         0                   0   \n",
       "83                      62                         0                   0   \n",
       "84                      62                         0                   0   \n",
       "85                      62                         0                   0   \n",
       "86                      62                         0                   0   \n",
       "87                      62                         0                   0   \n",
       "88                      62                         0                   0   \n",
       "89                     213                         0                   0   \n",
       "90                     138                         0                   0   \n",
       "91                      62                         0                   0   \n",
       "92                      62                         0                   0   \n",
       "93                     213                         0                   0   \n",
       "94                     138                         0                   0   \n",
       "95                      62                         0                   0   \n",
       "96                      62                         0                   0   \n",
       "97                     213                         0                   0   \n",
       "98                     138                         0                   0   \n",
       "99                      62                         0                   0   \n",
       "100                     62                         0                   0   \n",
       "101                     62                         0                   0   \n",
       "102                     62                         0                   0   \n",
       "103                     62                         0                   0   \n",
       "104                     62                         0                   0   \n",
       "105                     62                         0                   0   \n",
       "106                     62                         0                   0   \n",
       "107                     62                         0                   0   \n",
       "108                     62                         0                   0   \n",
       "109                     62                         0                   0   \n",
       "110                     62                         0                   0   \n",
       "111                     62                         0                   0   \n",
       "112                     62                         0                   0   \n",
       "113                     62                         0                   0   \n",
       "114                     62                         0                   0   \n",
       "\n",
       "     treatment_type_sam  treatment_type_plasma  treatment_type_uv_ozone  \\\n",
       "0                     1                      0                        0   \n",
       "1                     1                      0                        0   \n",
       "2                     1                      0                        0   \n",
       "3                     1                      0                        0   \n",
       "4                     1                      0                        0   \n",
       "5                     1                      0                        0   \n",
       "6                     1                      0                        0   \n",
       "7                     1                      0                        0   \n",
       "8                     1                      0                        0   \n",
       "9                     1                      0                        0   \n",
       "10                    1                      0                        0   \n",
       "11                    1                      0                        0   \n",
       "12                    1                      0                        0   \n",
       "13                    1                      0                        0   \n",
       "14                    1                      0                        0   \n",
       "15                    1                      0                        0   \n",
       "16                    1                      0                        0   \n",
       "17                    1                      0                        0   \n",
       "18                    1                      0                        0   \n",
       "19                    1                      0                        0   \n",
       "20                    1                      0                        0   \n",
       "21                    1                      0                        0   \n",
       "22                    1                      0                        0   \n",
       "23                    1                      0                        0   \n",
       "24                    1                      0                        0   \n",
       "25                    1                      0                        0   \n",
       "26                    1                      0                        0   \n",
       "27                    1                      0                        0   \n",
       "28                    1                      0                        0   \n",
       "29                    1                      0                        0   \n",
       "30                    1                      0                        0   \n",
       "31                    1                      0                        0   \n",
       "32                    1                      0                        0   \n",
       "33                    1                      0                        0   \n",
       "34                    1                      0                        0   \n",
       "35                    1                      0                        0   \n",
       "36                    1                      0                        0   \n",
       "37                    1                      0                        0   \n",
       "38                    1                      0                        0   \n",
       "39                    1                      0                        0   \n",
       "40                    1                      0                        0   \n",
       "41                    1                      0                        0   \n",
       "42                    1                      0                        0   \n",
       "43                    1                      0                        0   \n",
       "44                    0                      0                        0   \n",
       "45                    0                      0                        0   \n",
       "46                    0                      0                        0   \n",
       "47                    0                      0                        0   \n",
       "48                    0                      0                        0   \n",
       "49                    0                      0                        0   \n",
       "50                    0                      0                        0   \n",
       "51                    0                      0                        0   \n",
       "52                    0                      0                        0   \n",
       "53                    0                      0                        0   \n",
       "54                    0                      0                        0   \n",
       "55                    0                      0                        0   \n",
       "56                    0                      0                        0   \n",
       "57                    0                      0                        0   \n",
       "58                    0                      0                        0   \n",
       "59                    0                      0                        0   \n",
       "60                    0                      0                        0   \n",
       "61                    0                      0                        0   \n",
       "62                    0                      0                        0   \n",
       "63                    0                      0                        0   \n",
       "64                    0                      0                        0   \n",
       "65                    0                      0                        0   \n",
       "66                    0                      0                        0   \n",
       "67                    0                      0                        0   \n",
       "68                    1                      0                        0   \n",
       "69                    1                      0                        0   \n",
       "70                    1                      0                        0   \n",
       "71                    1                      0                        0   \n",
       "72                    1                      0                        0   \n",
       "73                    1                      0                        0   \n",
       "74                    1                      0                        0   \n",
       "75                    1                      0                        0   \n",
       "76                    1                      0                        0   \n",
       "77                    1                      0                        0   \n",
       "78                    1                      0                        0   \n",
       "79                    1                      0                        0   \n",
       "80                    1                      0                        0   \n",
       "81                    1                      0                        0   \n",
       "82                    1                      0                        0   \n",
       "83                    1                      0                        0   \n",
       "84                    1                      0                        0   \n",
       "85                    1                      0                        0   \n",
       "86                    1                      0                        0   \n",
       "87                    1                      0                        0   \n",
       "88                    1                      0                        0   \n",
       "89                    0                      0                        0   \n",
       "90                    0                      0                        0   \n",
       "91                    0                      0                        0   \n",
       "92                    1                      0                        0   \n",
       "93                    0                      0                        0   \n",
       "94                    0                      0                        0   \n",
       "95                    0                      0                        0   \n",
       "96                    1                      0                        0   \n",
       "97                    0                      0                        0   \n",
       "98                    0                      0                        0   \n",
       "99                    0                      0                        0   \n",
       "100                   1                      0                        0   \n",
       "101                   1                      0                        0   \n",
       "102                   1                      0                        0   \n",
       "103                   1                      0                        0   \n",
       "104                   1                      0                        0   \n",
       "105                   1                      0                        0   \n",
       "106                   0                      0                        0   \n",
       "107                   0                      0                        0   \n",
       "108                   0                      0                        0   \n",
       "109                   0                      0                        0   \n",
       "110                   1                      0                        0   \n",
       "111                   1                      0                        0   \n",
       "112                   1                      0                        0   \n",
       "113                   1                      0                        0   \n",
       "114                   1                      0                        0   \n",
       "\n",
       "     solution_treatment_poor_solvent  solution_treatment_aging  \\\n",
       "0                                  0                         1   \n",
       "1                                  0                         1   \n",
       "2                                  0                         1   \n",
       "3                                  0                         1   \n",
       "4                                  0                         1   \n",
       "5                                  0                         1   \n",
       "6                                  0                         1   \n",
       "7                                  0                         1   \n",
       "8                                  0                         1   \n",
       "9                                  0                         1   \n",
       "10                                 0                         1   \n",
       "11                                 0                         1   \n",
       "12                                 0                         1   \n",
       "13                                 0                         1   \n",
       "14                                 0                         1   \n",
       "15                                 0                         1   \n",
       "16                                 0                         1   \n",
       "17                                 0                         1   \n",
       "18                                 0                         1   \n",
       "19                                 0                         1   \n",
       "20                                 0                         1   \n",
       "21                                 0                         1   \n",
       "22                                 0                         1   \n",
       "23                                 0                         1   \n",
       "24                                 0                         1   \n",
       "25                                 0                         1   \n",
       "26                                 0                         1   \n",
       "27                                 0                         1   \n",
       "28                                 0                         1   \n",
       "29                                 0                         1   \n",
       "30                                 0                         1   \n",
       "31                                 0                         1   \n",
       "32                                 0                         1   \n",
       "33                                 0                         1   \n",
       "34                                 0                         1   \n",
       "35                                 0                         1   \n",
       "36                                 0                         1   \n",
       "37                                 0                         1   \n",
       "38                                 0                         1   \n",
       "39                                 0                         1   \n",
       "40                                 0                         1   \n",
       "41                                 0                         1   \n",
       "42                                 0                         1   \n",
       "43                                 0                         1   \n",
       "44                                 0                         0   \n",
       "45                                 1                         0   \n",
       "46                                 1                         0   \n",
       "47                                 1                         0   \n",
       "48                                 1                         0   \n",
       "49                                 1                         0   \n",
       "50                                 1                         0   \n",
       "51                                 1                         0   \n",
       "52                                 1                         0   \n",
       "53                                 1                         0   \n",
       "54                                 1                         0   \n",
       "55                                 0                         0   \n",
       "56                                 1                         0   \n",
       "57                                 1                         0   \n",
       "58                                 1                         0   \n",
       "59                                 1                         0   \n",
       "60                                 1                         0   \n",
       "61                                 0                         0   \n",
       "62                                 1                         0   \n",
       "63                                 1                         0   \n",
       "64                                 1                         0   \n",
       "65                                 1                         0   \n",
       "66                                 1                         0   \n",
       "67                                 1                         0   \n",
       "68                                 0                         0   \n",
       "69                                 0                         0   \n",
       "70                                 0                         0   \n",
       "71                                 0                         0   \n",
       "72                                 0                         0   \n",
       "73                                 0                         0   \n",
       "74                                 0                         0   \n",
       "75                                 0                         0   \n",
       "76                                 0                         0   \n",
       "77                                 0                         0   \n",
       "78                                 0                         0   \n",
       "79                                 0                         1   \n",
       "80                                 0                         1   \n",
       "81                                 0                         1   \n",
       "82                                 0                         1   \n",
       "83                                 0                         0   \n",
       "84                                 0                         0   \n",
       "85                                 0                         0   \n",
       "86                                 0                         0   \n",
       "87                                 0                         0   \n",
       "88                                 0                         0   \n",
       "89                                 0                         0   \n",
       "90                                 0                         0   \n",
       "91                                 0                         0   \n",
       "92                                 0                         0   \n",
       "93                                 0                         0   \n",
       "94                                 0                         0   \n",
       "95                                 0                         0   \n",
       "96                                 0                         0   \n",
       "97                                 0                         0   \n",
       "98                                 0                         0   \n",
       "99                                 0                         0   \n",
       "100                                0                         0   \n",
       "101                                0                         0   \n",
       "102                                0                         0   \n",
       "103                                0                         0   \n",
       "104                                0                         0   \n",
       "105                                0                         0   \n",
       "106                                0                         0   \n",
       "107                                0                         0   \n",
       "108                                0                         0   \n",
       "109                                0                         0   \n",
       "110                                1                         0   \n",
       "111                                1                         0   \n",
       "112                                1                         0   \n",
       "113                                1                         0   \n",
       "114                                1                         0   \n",
       "\n",
       "     solution_treatment_sonication  solution_treatment_mixing  \\\n",
       "0                                0                          0   \n",
       "1                                0                          0   \n",
       "2                                0                          0   \n",
       "3                                0                          0   \n",
       "4                                0                          0   \n",
       "5                                0                          0   \n",
       "6                                0                          0   \n",
       "7                                0                          0   \n",
       "8                                0                          0   \n",
       "9                                0                          0   \n",
       "10                               0                          0   \n",
       "11                               0                          0   \n",
       "12                               0                          0   \n",
       "13                               0                          0   \n",
       "14                               0                          0   \n",
       "15                               0                          0   \n",
       "16                               0                          0   \n",
       "17                               0                          0   \n",
       "18                               0                          0   \n",
       "19                               0                          0   \n",
       "20                               0                          0   \n",
       "21                               0                          0   \n",
       "22                               0                          0   \n",
       "23                               0                          0   \n",
       "24                               0                          0   \n",
       "25                               0                          0   \n",
       "26                               0                          0   \n",
       "27                               0                          0   \n",
       "28                               0                          0   \n",
       "29                               0                          0   \n",
       "30                               0                          0   \n",
       "31                               0                          0   \n",
       "32                               0                          0   \n",
       "33                               0                          0   \n",
       "34                               0                          0   \n",
       "35                               0                          0   \n",
       "36                               0                          0   \n",
       "37                               0                          0   \n",
       "38                               0                          0   \n",
       "39                               0                          0   \n",
       "40                               0                          0   \n",
       "41                               0                          0   \n",
       "42                               0                          0   \n",
       "43                               0                          0   \n",
       "44                               0                          0   \n",
       "45                               0                          0   \n",
       "46                               0                          0   \n",
       "47                               0                          0   \n",
       "48                               0                          0   \n",
       "49                               0                          0   \n",
       "50                               0                          0   \n",
       "51                               0                          0   \n",
       "52                               0                          0   \n",
       "53                               0                          0   \n",
       "54                               0                          0   \n",
       "55                               0                          0   \n",
       "56                               0                          0   \n",
       "57                               0                          0   \n",
       "58                               0                          0   \n",
       "59                               0                          0   \n",
       "60                               0                          0   \n",
       "61                               0                          0   \n",
       "62                               0                          0   \n",
       "63                               0                          0   \n",
       "64                               0                          0   \n",
       "65                               0                          0   \n",
       "66                               0                          0   \n",
       "67                               0                          0   \n",
       "68                               0                          0   \n",
       "69                               0                          0   \n",
       "70                               0                          0   \n",
       "71                               0                          0   \n",
       "72                               0                          0   \n",
       "73                               0                          0   \n",
       "74                               0                          0   \n",
       "75                               0                          0   \n",
       "76                               0                          0   \n",
       "77                               0                          0   \n",
       "78                               0                          0   \n",
       "79                               0                          0   \n",
       "80                               0                          0   \n",
       "81                               0                          0   \n",
       "82                               0                          0   \n",
       "83                               0                          0   \n",
       "84                               0                          0   \n",
       "85                               0                          0   \n",
       "86                               0                          0   \n",
       "87                               0                          0   \n",
       "88                               0                          0   \n",
       "89                               0                          0   \n",
       "90                               0                          0   \n",
       "91                               0                          0   \n",
       "92                               0                          0   \n",
       "93                               0                          0   \n",
       "94                               0                          0   \n",
       "95                               0                          0   \n",
       "96                               0                          0   \n",
       "97                               0                          0   \n",
       "98                               0                          0   \n",
       "99                               0                          0   \n",
       "100                              0                          0   \n",
       "101                              0                          0   \n",
       "102                              0                          0   \n",
       "103                              0                          0   \n",
       "104                              0                          0   \n",
       "105                              0                          0   \n",
       "106                              0                          0   \n",
       "107                              0                          0   \n",
       "108                              0                          0   \n",
       "109                              0                          0   \n",
       "110                              0                          0   \n",
       "111                              0                          0   \n",
       "112                              0                          0   \n",
       "113                              0                          0   \n",
       "114                              0                          0   \n",
       "\n",
       "     solution_treatment_uv_irradiation  \n",
       "0                                    0  \n",
       "1                                    0  \n",
       "2                                    0  \n",
       "3                                    0  \n",
       "4                                    0  \n",
       "5                                    0  \n",
       "6                                    0  \n",
       "7                                    0  \n",
       "8                                    0  \n",
       "9                                    0  \n",
       "10                                   0  \n",
       "11                                   0  \n",
       "12                                   0  \n",
       "13                                   0  \n",
       "14                                   0  \n",
       "15                                   0  \n",
       "16                                   0  \n",
       "17                                   0  \n",
       "18                                   0  \n",
       "19                                   0  \n",
       "20                                   0  \n",
       "21                                   0  \n",
       "22                                   0  \n",
       "23                                   0  \n",
       "24                                   0  \n",
       "25                                   0  \n",
       "26                                   0  \n",
       "27                                   0  \n",
       "28                                   0  \n",
       "29                                   0  \n",
       "30                                   0  \n",
       "31                                   0  \n",
       "32                                   0  \n",
       "33                                   0  \n",
       "34                                   0  \n",
       "35                                   0  \n",
       "36                                   0  \n",
       "37                                   0  \n",
       "38                                   0  \n",
       "39                                   0  \n",
       "40                                   0  \n",
       "41                                   0  \n",
       "42                                   0  \n",
       "43                                   0  \n",
       "44                                   0  \n",
       "45                                   0  \n",
       "46                                   0  \n",
       "47                                   0  \n",
       "48                                   0  \n",
       "49                                   0  \n",
       "50                                   0  \n",
       "51                                   0  \n",
       "52                                   0  \n",
       "53                                   0  \n",
       "54                                   0  \n",
       "55                                   0  \n",
       "56                                   0  \n",
       "57                                   0  \n",
       "58                                   0  \n",
       "59                                   0  \n",
       "60                                   0  \n",
       "61                                   0  \n",
       "62                                   0  \n",
       "63                                   0  \n",
       "64                                   0  \n",
       "65                                   0  \n",
       "66                                   0  \n",
       "67                                   0  \n",
       "68                                   0  \n",
       "69                                   0  \n",
       "70                                   0  \n",
       "71                                   0  \n",
       "72                                   0  \n",
       "73                                   0  \n",
       "74                                   0  \n",
       "75                                   0  \n",
       "76                                   0  \n",
       "77                                   0  \n",
       "78                                   0  \n",
       "79                                   0  \n",
       "80                                   0  \n",
       "81                                   0  \n",
       "82                                   0  \n",
       "83                                   0  \n",
       "84                                   0  \n",
       "85                                   0  \n",
       "86                                   0  \n",
       "87                                   0  \n",
       "88                                   0  \n",
       "89                                   0  \n",
       "90                                   0  \n",
       "91                                   0  \n",
       "92                                   0  \n",
       "93                                   0  \n",
       "94                                   0  \n",
       "95                                   0  \n",
       "96                                   0  \n",
       "97                                   0  \n",
       "98                                   0  \n",
       "99                                   0  \n",
       "100                                  0  \n",
       "101                                  0  \n",
       "102                                  0  \n",
       "103                                  0  \n",
       "104                                  0  \n",
       "105                                  0  \n",
       "106                                  0  \n",
       "107                                  0  \n",
       "108                                  0  \n",
       "109                                  0  \n",
       "110                                  0  \n",
       "111                                  0  \n",
       "112                                  0  \n",
       "113                                  0  \n",
       "114                                  0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P3HT_df = pd.read_csv(\"combined_df_P3HT_step.csv\")\n",
    "P3HT_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b3a5dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['solution_concentration',\n",
       " 'polymer_mw',\n",
       " 'polymer_mn',\n",
       " 'polymer_dispersity',\n",
       " 'hole_mobility',\n",
       " 'solution_treatment',\n",
       " 'substrate_pretreatment',\n",
       " 'post_process',\n",
       " 'channel_width',\n",
       " 'channel_length',\n",
       " 'dielectric_thickness',\n",
       " 'film_deposition_type_drop',\n",
       " 'film_deposition_type_spin',\n",
       " 'dielectric_material_SiO2',\n",
       " 'electrode_configuration_BGBC',\n",
       " 'electrode_configuration_BGTC',\n",
       " 'electrode_configuration_TGBC',\n",
       " 'gate_material_Other',\n",
       " 'film_deposition_type_MGC',\n",
       " 'dielectric_material_other',\n",
       " 'solvent_boiling_point',\n",
       " 'blend_conjugated_polymer',\n",
       " 'insulating_polymer',\n",
       " 'treatment_type_sam',\n",
       " 'treatment_type_plasma',\n",
       " 'treatment_type_uv_ozone',\n",
       " 'solution_treatment_poor_solvent',\n",
       " 'solution_treatment_aging',\n",
       " 'solution_treatment_sonication',\n",
       " 'solution_treatment_mixing',\n",
       " 'solution_treatment_uv_irradiation']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P3HT_df_columns = P3HT_df.columns.tolist()\n",
    "P3HT_df_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3906f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115, 30)\n",
      "(115, 1)\n"
     ]
    }
   ],
   "source": [
    "P3HT_Y = P3HT_df[['hole_mobility']]\n",
    "P3HT_X = P3HT_df.drop(labels = 'hole_mobility', axis = 1)\n",
    "print(P3HT_X.shape)\n",
    "print(P3HT_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2701dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error Train: 0.0008373243221231764\n",
      "R^2 Score Train: 0.5735591864884009\n",
      "Mean Squared Error Test: 0.0007614945873207683\n",
      "R^2 Score Test: -0.5469076303137805\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(P3HT_X, P3HT_Y, test_size = 0.2, random_state=42)\n",
    "LR_model = LinearRegression()\n",
    "LR_model.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_train = LR_model.predict(X_train)\n",
    "Y_pred_test = LR_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse_train = mean_squared_error(Y_train, Y_pred_train)\n",
    "r2_train = r2_score(Y_train,Y_pred_train)\n",
    "mse_test = mean_squared_error(Y_test, Y_pred_test)\n",
    "r2_test = r2_score(Y_test,Y_pred_test)\n",
    "\n",
    "print(f\"Mean Squared Error Train: {mse_train}\")\n",
    "print(f\"R^2 Score Train: {r2_train}\")\n",
    "print(f\"Mean Squared Error Test: {mse_test}\")\n",
    "print(f\"R^2 Score Test: {r2_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34a51754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ba16cf",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ffa140e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0005817350988815453, 0.004369526514936092, 0.07220513506804832, 6.165463207020517, 153.9443172957012, 8026.633260620959]\n",
      "[-0.18174242898740212, -7.8762993452057035, -145.67822495966257, -12523.582890227977, -312723.00754157157, -16305381.130516302]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "degrees = range(2, 8)  # Polynomial degrees from 2 to 10\n",
    "mse_values = []  # To store mean squared error values\n",
    "r2_values = []  # To store R^2 values\n",
    "\n",
    "for degree in degrees:\n",
    "    polynomial_features = PolynomialFeatures(degree=degree)\n",
    "    X_poly = polynomial_features.fit_transform(P3HT_X)\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_poly, P3HT_Y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    Y_pred = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(Y_test, Y_pred)\n",
    "    r2 = r2_score(Y_test, Y_pred)\n",
    "    \n",
    "    mse_values.append(mse)\n",
    "    r2_values.append(r2)\n",
    "\n",
    "# Plotting Polynomial Degree vs MSE\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot\n",
    "plt.plot(degrees, mse_values, marker='o', linestyle='-', color='b')\n",
    "plt.title('Polynomial Degree vs MSE')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "\n",
    "# Plotting Polynomial Degree vs R^2\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot\n",
    "plt.plot(degrees, r2_values, marker='o', linestyle='-', color='r')\n",
    "plt.title('Polynomial Degree vs R^2 Score')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('R^2 Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(mse_values)\n",
    "print(r2_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b98140",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55de5eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5169279202783257, -0.48234370967012374, -0.4494655772434395, -0.419421006101415, -0.3922418613150549, -0.3676837254561729, -0.3454469969144529, -0.3252437743088765, -0.3068170695475212, -0.2899439238605468]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(P3HT_X, P3HT_Y, test_size=0.2, random_state=42)\n",
    "\n",
    "alphas = np.arange(0.1, 1.1, 0.1)\n",
    "\n",
    "mse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    mse_scores.append(mean_squared_error(Y_test, Y_pred))\n",
    "    r2_scores.append(r2_score(Y_test, Y_pred))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(alphas, mse_scores, marker='o', linestyle='-', color='b')\n",
    "plt.title('Alphas vs MSE')\n",
    "plt.xlabel('Alphas')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(alphas, r2_scores, marker='o', linestyle='-', color='r')\n",
    "plt.title('Alphas vs R^2 Score')\n",
    "plt.xlabel('Alphas')\n",
    "plt.ylabel('R^2 Score')\n",
    "print(r2_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62cb34",
   "metadata": {},
   "source": [
    "## Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70587409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2610923423046708, 0.25162244357995256, 0.20609228662820966, 0.12449881714856026, 0.006842035141002678, -0.14101245690529018, -0.19448737775552694, -0.19443833463037796, -0.1943893224667399, -0.19434034126461208]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(P3HT_X, P3HT_Y, test_size=0.2, random_state=42)\n",
    "\n",
    "alphas = np.arange(0.1, 1.1, 0.1)\n",
    "\n",
    "mse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    model = Lasso(alpha=alpha)\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    mse_scores.append(mean_squared_error(Y_test, Y_pred))\n",
    "    r2_scores.append(r2_score(Y_test, Y_pred))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(alphas, mse_scores, marker='o', linestyle='-', color='b')\n",
    "plt.title('Alphas vs MSE')\n",
    "plt.xlabel('Alphas')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(alphas, r2_scores, marker='o', linestyle='-', color='r')\n",
    "plt.title('Alphas vs R^2 Score')\n",
    "plt.xlabel('Alphas')\n",
    "plt.ylabel('R^2 Score')\n",
    "print(r2_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476faf74",
   "metadata": {},
   "source": [
    "## Elsaticnet regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "339c12ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.1: [0.21698506514083982, 0.2287861662181052, 0.2335846602789282, 0.24505989252308358, 0.2523649965129645, 0.2548512575922559, 0.2569747200797138, 0.25873615722681076, 0.26013523645588355, 0.26117232681040126], 0.2: [0.22875906607325946, 0.24496814707181735, 0.25481255260564106, 0.2586928298133485, 0.2611279256428285, 0.26211778613044, 0.2616630167575056, 0.25976057694512555, 0.2564090102699046, 0.2516297745294236], 0.30000000000000004: [0.23353573599834365, 0.2547996425267479, 0.26007617197881994, 0.2621036664274735, 0.2608824839215309, 0.2564021916524418, 0.24869366565830497, 0.23772986691312714, 0.22351961373485352, 0.2060634619501076], 0.4: [0.24492146295085915, 0.25867114313123973, 0.262096601890912, 0.2597455808738216, 0.2516252070269709, 0.23773276890627604, 0.21806933582404808, 0.19263575472645056, 0.16143287241410598, 0.12446153554212647], 0.5: [0.2523090497780692, 0.26110124334343965, 0.26087340769612866, 0.25162428779127133, 0.23336036729451015, 0.20607993785773526, 0.1697841022284151, 0.12447396299608982, 0.07015062259240123, 0.006815183291121074], 0.6: [0.25478672823758997, 0.262089534229782, 0.25639535333899677, 0.23773566526892576, 0.2060840548922246, 0.16145005683761937, 0.1038349413552071, 0.03323997854954508, -0.050332185069511315, -0.1410194800199147], 0.7000000000000001: [0.25690357743248415, 0.26163627029406744, 0.24869253522283563, 0.21807937128391242, 0.16979516776631276, 0.10384122146963204, 0.02021882908202044, -0.08106937076222742, -0.1824822631017431, -0.19448737772462188], 0.8: [0.2586602940426296, 0.2597380679500625, 0.2377371113376594, 0.1926548415281314, 0.12449260370513093, 0.0332515270501762, -0.08106591681824349, -0.1921840879679413, -0.19447756663503735, -0.19443833460979665], 0.9: [0.2600564546457309, 0.2563930695212304, 0.22353710783883785, 0.16146151162882438, 0.07017526033916599, -0.05031955354227624, -0.18248143360751135, -0.19447756664533689, -0.19443343201148555, -0.1943893224564599]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(P3HT_X, P3HT_Y, test_size=0.2, random_state=42)\n",
    "\n",
    "alphas = np.arange(0.1, 1.1, 0.1)\n",
    "l1_ratios = np.arange(0.1, 1, 0.1)\n",
    "\n",
    "mse_scores = {l1_ratio: [] for l1_ratio in l1_ratios}\n",
    "r2_scores = {l1_ratio: [] for l1_ratio in l1_ratios}\n",
    "\n",
    "for l1_ratio in l1_ratios:\n",
    "    for alpha in alphas:\n",
    "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "        model.fit(X_train, Y_train)\n",
    "        Y_pred = model.predict(X_test)\n",
    "        mse = mean_squared_error(Y_test, Y_pred)\n",
    "        r2 = r2_score(Y_test, Y_pred)\n",
    "        \n",
    "        mse_scores[l1_ratio].append(mse)\n",
    "        r2_scores[l1_ratio].append(r2)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# MSE plot\n",
    "plt.subplot(1, 2, 1)\n",
    "for l1_ratio in l1_ratios:\n",
    "    plt.plot(alphas, mse_scores[l1_ratio], label=f'L1_ratio={l1_ratio:.1f}', marker = 'o')\n",
    "plt.title('Alpha vs MSE for different L1_ratios')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "\n",
    "# R^2 score plot\n",
    "plt.subplot(1, 2, 2)\n",
    "for l1_ratio in l1_ratios:\n",
    "    plt.plot(alphas, r2_scores[l1_ratio], label=f'L1_ratio={l1_ratio:.1f}', marker = 'o')\n",
    "plt.title('Alpha vs R^2 Score for different L1_ratios')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('R^2 Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(r2_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a1372",
   "metadata": {},
   "source": [
    "## Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57465365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-13.43257595 -22.92329693 -22.92329693 -22.92329693 -22.92329693\n",
      "  -22.92329693 -22.92329693 -22.92329693 -22.92329693 -22.92329693]\n",
      " [-13.43572514 -22.92329693 -22.92329693 -22.92329693 -22.92329693\n",
      "  -22.92329693 -22.92329693 -22.92329693 -22.92329693 -22.92329693]\n",
      " [-13.43846401 -22.92329693 -22.92329693 -22.92329693 -22.92329693\n",
      "  -22.92329693 -22.92329693 -22.92329693 -22.92329693 -22.92329693]\n",
      " [-13.44120368 -22.92329693 -22.92329693 -22.92329693 -22.92329693\n",
      "  -22.92329693 -22.92329693 -22.92329693 -22.92329693 -22.92329693]\n",
      " [-13.44394415 -22.92329693 -22.92329693 -22.92329693 -22.92329693\n",
      "  -22.92329693 -22.92329693 -22.92329693 -22.92329693 -22.92329693]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(P3HT_X, P3HT_Y, test_size=0.2, random_state=42)\n",
    "\n",
    "C_values = np.arange(1.0, 6.0, 1)\n",
    "epsilon_values = np.arange(0.1, 1.1, 0.1)\n",
    "\n",
    "mse_scores = np.zeros((len(C_values), len(epsilon_values)))\n",
    "r2_scores = np.zeros((len(C_values), len(epsilon_values)))\n",
    "\n",
    "for i, C in enumerate(C_values):\n",
    "    for j, epsilon in enumerate(epsilon_values):\n",
    "        svr_model = SVR(kernel='rbf', C=C, epsilon=epsilon)\n",
    "        svr_model.fit(X_train, Y_train)\n",
    "        Y_pred = svr_model.predict(X_test)\n",
    "        mse_scores[i, j] = mean_squared_error(Y_test, Y_pred)\n",
    "        r2_scores[i, j] = r2_score(Y_test, Y_pred)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for j, epsilon in enumerate(epsilon_values):\n",
    "    plt.plot(C_values, mse_scores[:, j], label=f'Epsilon={epsilon:.1f}', marker = 'o')\n",
    "plt.title('C vs MSE for varying Epsilon')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for j, epsilon in enumerate(epsilon_values):\n",
    "    plt.plot(C_values, r2_scores[:, j], label=f'Epsilon={epsilon:.1f}', marker = 'o')\n",
    "plt.title('C vs R^2 Score for varying Epsilon')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('R^2 Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(r2_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692872b4",
   "metadata": {},
   "source": [
    "## Decision tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "443d52a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0028744093295697175\n",
      "R^2 Score: -4.839103519042499\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(P3HT_X, P3HT_Y, test_size=0.2, random_state=42)\n",
    "\n",
    "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "dt_regressor.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = dt_regressor.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d3d37",
   "metadata": {},
   "source": [
    "## Random forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e38b5b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05659489157063491, -0.14822928385955403, -0.24238507338459314, -0.2648317560238884, -0.3063841960226996, -0.3362315404985712, -0.28381930260386423, -0.2624687561588859, -0.24238340250851076, -0.25597889164265797, -0.2473253707943004, -0.19072323175238126, -0.2095604211346611, -0.22085301364810928, -0.2581135900816762, -0.2683315327679108, -0.2732186164916768, -0.27048900470019754, -0.2615086044481685, -0.2681504982389198, -0.22816264119468044, -0.2068874574450068, -0.19661216336367482, -0.16827897061334096, -0.1645575943102724, -0.16494533723991012, -0.13017767020525084, -0.10458844611972484, -0.09826136349227199, -0.09517841122985926, -0.11865090636745546]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(P3HT_X, P3HT_Y, test_size=0.2, random_state=42)\n",
    "\n",
    "n_estimators_range = range(100, 410, 10)\n",
    "\n",
    "mse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for n_estimators in n_estimators_range:\n",
    "    rf_regressor = RandomForestRegressor(n_estimators=n_estimators, random_state=42)\n",
    "    rf_regressor.fit(X_train, Y_train)\n",
    "    Y_pred = rf_regressor.predict(X_test)\n",
    "    mse_scores.append(mean_squared_error(Y_test, Y_pred))\n",
    "    r2_scores.append(r2_score(Y_test, Y_pred))\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(n_estimators_range, mse_scores, marker='o', linestyle='-', color='red')\n",
    "plt.title('n_estimators vs MSE')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "\n",
    "# Plotting n_estimators vs R^2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(n_estimators_range, r2_scores, marker='o', linestyle='-', color='blue')\n",
    "plt.title('n_estimators vs R^2 Score')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('R^2 Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(r2_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6ed1b0",
   "metadata": {},
   "source": [
    "## Gradient bossting regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78e1d8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Parameter ranges\n",
    "n_estimators_range = range(100, 310, 10)\n",
    "learning_rate_range = np.arange(0.1, 1.1, 0.1)\n",
    "fixed_max_depth = 3  # Fixed parameter\n",
    "\n",
    "# Initialize storage for performance metrics\n",
    "mse_scores = np.zeros((len(learning_rate_range), len(n_estimators_range)))\n",
    "r2_scores = np.zeros((len(learning_rate_range), len(n_estimators_range)))\n",
    "\n",
    "# Evaluate model performance over varying n_estimators and learning_rate\n",
    "for i, learning_rate in enumerate(learning_rate_range):\n",
    "    for j, n_estimators in enumerate(n_estimators_range):\n",
    "        model = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=fixed_max_depth, random_state=42)\n",
    "        model.fit(X_train, Y_train)\n",
    "        Y_pred = model.predict(X_test)\n",
    "        mse_scores[i, j] = mean_squared_error(Y_test, Y_pred)\n",
    "        r2_scores[i, j] = r2_score(Y_test, Y_pred)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# MSE Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "for i, learning_rate in enumerate(learning_rate_range):\n",
    "    plt.plot(n_estimators_range, mse_scores[i, :], label=f'LR={learning_rate:.1f}', marker = 'o')\n",
    "plt.title('MSE vs n_estimators for varying learning_rate')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "\n",
    "# R^2 Score Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, learning_rate in enumerate(learning_rate_range):\n",
    "    plt.plot(n_estimators_range, r2_scores[i, :], label=f'LR={learning_rate:.1f}', marker = 'o')\n",
    "plt.title('R^2 Score vs n_estimators for varying learning_rate')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('R^2 Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2857b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.043773372325377924, -0.7320767622579705, -2.4990579358846237, -0.37647656360529935, -1.225489564401867, 0.3007640887039059, -3.147386870936061, -4.417113248460265, -4.818760710806168, -4.818761256062544, -4.818761031777436, -4.818761049476549]\n"
     ]
    }
   ],
   "source": [
    "fixed_learning_rate = 0.1\n",
    "fixed_n_estimators = 250\n",
    "max_depth_range = range(3, 15)\n",
    "\n",
    "mse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for max_depth in max_depth_range:\n",
    "    model = GradientBoostingRegressor(n_estimators=fixed_n_estimators, learning_rate=fixed_learning_rate, max_depth=max_depth, random_state=42)\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    mse_scores.append(mean_squared_error(Y_test, Y_pred))\n",
    "    r2_scores.append(r2_score(Y_test, Y_pred))\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(max_depth_range, mse_scores, marker='o', linestyle='-', color='red')\n",
    "plt.title('max_depth vs MSE')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(max_depth_range, r2_scores, marker='o', linestyle='-', color='blue')\n",
    "plt.title('max_depth vs R^2 Score')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('R^2 Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(r2_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461c8d20",
   "metadata": {},
   "source": [
    "## K-nearest Neighbors Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b235dd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.1122703419930509, -0.3858263591069866, -0.22362878044690193, -0.22751810998390565, -0.1388268402545314, 0.0321769715778768, 0.12035085209380003, 0.08324651030090668, -0.060154951256911504, 0.07521848201622583, 0.17002774792139963, 0.25182409732055633, 0.33224173001425605]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "n_neighbors_range = range(3, 16)\n",
    "\n",
    "mse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for n_neighbors in n_neighbors_range:\n",
    "    knn_regressor = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "    knn_regressor.fit(X_train, Y_train)\n",
    "    Y_pred = knn_regressor.predict(X_test)\n",
    "    mse_scores.append(mean_squared_error(Y_test, Y_pred))\n",
    "    r2_scores.append(r2_score(Y_test, Y_pred))\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(n_neighbors_range, mse_scores, marker='o', linestyle='-', color='red')\n",
    "plt.title('n_neighbors vs MSE')\n",
    "plt.xlabel('n_neighbors')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(n_neighbors_range, r2_scores, marker='o', linestyle='-', color='blue')\n",
    "plt.title('n_neighbors vs R^2 Score')\n",
    "plt.xlabel('n_neighbors')\n",
    "plt.ylabel('R^2 Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(r2_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fc023e",
   "metadata": {},
   "source": [
    "## Neural network regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2f22427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-18.691734101442936, -23.928649994698908, -5.583328573449646, -14.989346094232744, -7.362361794193912, -8.077714639130559, -5.749387895665483, -1.808308458234389, -35.388877638688236, -0.1352079548720715, -4.459602691433701, -2.875894397760844, -2.7655734361223825, -12.144092286322104, -1.385001532155536, -4.204332606186687, -2.6512542872300684, -2.2202088071561237, -1.3903962854940146, -2.0367035809906655, -4.795522650951552, -2.5629397572753283]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "hidden_layer_sizes_range = range(50, 270, 10)\n",
    "\n",
    "mse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for size in hidden_layer_sizes_range:\n",
    "    mlp_regressor = MLPRegressor(hidden_layer_sizes=(size,), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
    "    mlp_regressor.fit(X_train_scaled, Y_train)\n",
    "    Y_pred = mlp_regressor.predict(X_test_scaled)\n",
    "    mse_scores.append(mean_squared_error(Y_test, Y_pred))\n",
    "    r2_scores.append(r2_score(Y_test, Y_pred))\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(hidden_layer_sizes_range, mse_scores, marker='o', linestyle='-', color='red')\n",
    "plt.title('Hidden Layer Size vs MSE')\n",
    "plt.xlabel('Hidden Layer Size')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(hidden_layer_sizes_range, r2_scores, marker='o', linestyle='-', color='blue')\n",
    "plt.title('Hidden Layer Size vs R^2 Score')\n",
    "plt.xlabel('Hidden Layer Size')\n",
    "plt.ylabel('R^2 Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01459a53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.41207953950221593\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.4, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.55)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.42207905894238484\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.49878156043180927\tAdaBoostRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.45083321212011995\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.49878156043180927\tAdaBoostRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5533006178897217\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "training r2 is :  0.9095309106069475\n",
      "testing r2 is :  -0.043547059055739634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=5, population_size=50, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_combined_MYL_1.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "858dfc03-09c0-4706-a261-9b821076e723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/550 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.41207953950221593\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.4, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.55)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.42207905894238484\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.49878156043180927\tAdaBoostRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.45083321212011995\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.49878156043180927\tAdaBoostRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5533006178897217\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "training r2 is :  0.9095309106069475\n",
      "testing r2 is :  -0.043547059055739634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=10, population_size=50, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_combined_MYL_2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6301b631-01bd-4bc3-8a37-225c6d746853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/2550 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.41207953950221593\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.4, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.55)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.42207905894238484\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.49878156043180927\tAdaBoostRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.45083321212011995\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.49878156043180927\tAdaBoostRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5533006178897217\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 65, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 100, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6497824163185403\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6497824163185403\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6333589685001457\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6497824163185403\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6485184199402505\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6497824163185403\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6485184199402505\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6497824163185403\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [12:16:41] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6485184199402505\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6497824163185403\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 92, n_samples_fit = 50, n_samples = 50.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6485184199402505\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6505209296692709\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=20, DecisionTreeRegressor__min_samples_split=15), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [12:17:45] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6594811752861462\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6594811752861462\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6594811752861462\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6594811752861462\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6594811752861462\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6529674564874137\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6594811752861462\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6529674564874137\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6594811752861462\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6529674564874137\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6594811752861462\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.6621334139192527\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6787913270794108\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(MaxAbsScaler(input_matrix), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 83, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 89, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6787913270794108\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(MaxAbsScaler(input_matrix), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [12:27:00] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6844214187979857\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(MaxAbsScaler(input_matrix), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6789278856467476\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6844214187979857\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(MaxAbsScaler(input_matrix), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 77, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6789278856467476\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6844214187979857\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(MaxAbsScaler(input_matrix), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 94, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6845210137030244\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6845210137030244\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6845210137030244\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "training r2 is :  0.8061273003427205\n",
      "testing r2 is :  0.3636873676000678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=50, population_size=50, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_combined_MYL_3.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07ee3bce-3fcc-4fec-a4db-a8850c6c69d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/5050 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.41207953950221593\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.4, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.55)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.42207905894238484\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.49878156043180927\tAdaBoostRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.45083321212011995\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.55, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.49878156043180927\tAdaBoostRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5533006178897217\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 65, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 100, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6497824163185403\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6315501442843992\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6497824163185403\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6333589685001457\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6497824163185403\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6485184199402505\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6497824163185403\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6485184199402505\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6497824163185403\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [12:39:44] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6485184199402505\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6497824163185403\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 92, n_samples_fit = 50, n_samples = 50.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6485184199402505\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6505209296692709\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=20, DecisionTreeRegressor__min_samples_split=15), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [12:40:49] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6594811752861462\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6594811752861462\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6594811752861462\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6594811752861462\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6594811752861462\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6529674564874137\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6594811752861462\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6529674564874137\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6594811752861462\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6529674564874137\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6594811752861462\tExtraTreesRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.6621334139192527\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6787913270794108\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(MaxAbsScaler(input_matrix), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 83, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 89, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6787913270794108\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(MaxAbsScaler(input_matrix), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [12:50:05] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6787082435563926\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6844214187979857\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(MaxAbsScaler(input_matrix), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6789278856467476\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6844214187979857\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(MaxAbsScaler(input_matrix), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 77, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6789278856467476\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6844214187979857\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(MaxAbsScaler(input_matrix), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 94, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6845210137030244\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6845210137030244\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6845210137030244\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 51 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6845210137030244\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [12:53:27] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 52 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6845210137030244\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "Generation 53 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6845210137030244\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 54 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6845210137030244\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 55 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6845210137030244\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 56 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6703074275003825\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=2, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=6), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6845210137030244\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 57 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6806999382753853\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=6, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=15), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6845210137030244\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 58 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6806999382753853\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=6, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=15), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6845210137030244\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 59 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6806999382753853\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=6, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=15), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6845210137030244\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 60 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6806999382753853\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=6, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=15), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.68651382813071\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 61 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6806999382753853\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=6, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=15), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.68651382813071\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 62 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6806999382753853\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=6, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=15), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.68651382813071\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 52, n_samples_fit = 50, n_samples = 50.\n",
      "\n",
      "Generation 63 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6806999382753853\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=6, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=15), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.68651382813071\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 64 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6806999382753853\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=6, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=15), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.68651382813071\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 65 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6806999382753853\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=6, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=15), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.68651382813071\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.686525635191298\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 66 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.68651382813071\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.686525635191298\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 92, n_samples_fit = 50, n_samples = 50.\n",
      "\n",
      "Generation 67 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.68651382813071\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.686525635191298\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "Generation 68 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.68651382813071\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.686525635191298\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 69 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.68651382813071\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.686525635191298\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 70 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.68651382813071\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.686525635191298\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.68653555731389\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(StandardScaler(RobustScaler(input_matrix)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 63, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 71 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.68651382813071\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6920496239867183\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 72 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.68651382813071\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6920496239867183\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 73 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.68651382813071\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6920496239867183\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 74 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.68651382813071\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6920496239867183\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 75 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.68651382813071\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6920496239867183\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 76 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.68651382813071\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=6, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6920496239867183\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 77 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6920496239867183\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [13:13:04] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "\n",
      "Generation 78 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6920496239867183\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 79 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6920496239867183\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 80 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6920496239867183\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 77, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 81 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6920496239867183\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 82 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6920496239867183\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 83 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6920496239867183\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 84 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6935132982514522\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "Generation 85 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6935132982514522\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 86 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6935132982514522\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [13:15:41] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "\n",
      "Generation 87 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6935132982514522\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [13:15:58] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 88 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6935132982514522\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 89 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6935132982514522\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 90 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6935132982514522\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 71, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 91 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6935132982514522\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 92 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6935132982514522\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 93 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.6935132982514522\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.6935161351612648\tExtraTreesRegressor(RandomForestRegressor(StandardScaler(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=9, RandomForestRegressor__n_estimators=100)), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 54, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 94 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.694666023305135\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [13:18:55] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 95 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.694666023305135\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "Generation 96 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.694666023305135\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 97 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.694666023305135\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [13:19:48] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 98 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.694666023305135\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [13:20:02] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 99 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.694666023305135\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 100 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6862085863902422\tXGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=19, DecisionTreeRegressor__min_samples_split=18), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6906627925107991\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.694666023305135\tExtraTreesRegressor(RandomForestRegressor(RandomForestRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=20, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=12, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "training r2 is :  0.8243202679127242\n",
      "testing r2 is :  -0.003529791272657512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=100, population_size=50, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_combined_MYL_4.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e23fdbf8-1571-48e4-89ba-0ea01d88adde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/600 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 90, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ElasticNetCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "training r2 is :  0.9759116128613957\n",
      "testing r2 is :  -0.33611376357866285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=5, population_size=100, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_combined_MYL_5.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca7b7ca9-faaa-4435-8666-95374a9e3b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/1100 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 90, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ElasticNetCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 59, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5761274765793847\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.4, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.578445354541721\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.4, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.578445354541721\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.4, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.587106735581138\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "training r2 is :  0.9759116128613957\n",
      "testing r2 is :  -0.33611376357866285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=10, population_size=100, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_combined_MYL_6.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a24e74f2-f3ea-411e-9a85-338026369489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/5100 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 90, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ElasticNetCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 59, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5761274765793847\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.4, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.578445354541721\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.4, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.578445354541721\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.4, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.587106735581138\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5910732310372906\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 68, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5910732310372906\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-3\t0.6237384777234363\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 75, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5935186270042389\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-3\t0.6237384777234363\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6240371403906845\tKNeighborsRegressor(MinMaxScaler(ZeroCount(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100))), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5935186270042389\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-3\t0.6237384777234363\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6240371403906845\tKNeighborsRegressor(MinMaxScaler(ZeroCount(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100))), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 95, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 86, n_samples_fit = 50, n_samples = 50.\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.6568093874928962\tKNeighborsRegressor(MinMaxScaler(CombineDFs(MinMaxScaler(input_matrix), RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100))), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6529674564874137\tXGBRegressor(MaxAbsScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6613043185216237\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6529674564874137\tXGBRegressor(MaxAbsScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6613043185216237\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6529674564874137\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6613043185216237\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6645286566922379\tKNeighborsRegressor(MinMaxScaler(CombineDFs(MinMaxScaler(input_matrix), RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100))), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 62, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6529674564874137\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6613043185216237\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6703696710100429\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6529674564874137\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6668041100198824\tXGBRegressor(MaxAbsScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.6703696710100429\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.6703696710100429\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-5\t0.6735500048552386\tKNeighborsRegressor(KNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=49, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-5\t0.6735500048552386\tKNeighborsRegressor(KNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=49, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6743256291445755\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6743256291445755\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6743256291445755\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6743256291445755\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6743256291445755\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 82, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6743256291445755\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 54, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6743256291445755\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-5\t0.6796841452320592\tKNeighborsRegressor(KNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=16, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=44, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 70, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6743256291445755\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-5\t0.6796841452320592\tKNeighborsRegressor(KNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=16, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=44, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [14:12:34] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 87, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6766648755371372\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-5\t0.6796841452320592\tKNeighborsRegressor(KNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=16, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=44, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6730753591023616\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6766648755371372\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-5\t0.6796841452320592\tKNeighborsRegressor(KNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=16, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=44, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6730753591023616\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6766648755371372\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-5\t0.6796841452320592\tKNeighborsRegressor(KNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=16, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=44, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 63, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [14:15:58] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 60, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 91, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 72, n_samples_fit = 50, n_samples = 50.\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6730753591023616\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6766648755371372\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-5\t0.6796841452320592\tKNeighborsRegressor(KNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=16, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=44, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 74, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [14:17:03] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6730753591023616\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6766648755371372\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-5\t0.6796841452320592\tKNeighborsRegressor(KNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=16, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=44, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6730753591023616\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6865859726126248\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6758289388037124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.6865859726126248\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [14:20:14] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6758289388037124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.6865859726126248\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 64, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6758289388037124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.6865859726126248\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6758289388037124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.6865859726126248\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 77, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6758289388037124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.6865859726126248\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7075121650533807\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7075121650533807\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [14:27:21] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 53, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 92, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7075121650533807\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [14:28:41] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [14:28:42] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7075121650533807\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 74, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7075121650533807\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "training r2 is :  0.9755556834688797\n",
      "testing r2 is :  -1.0349815933810627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=50, population_size=100, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_combined_MYL_7.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f0d371a-b2ca-4ab2-afa1-56d06eb54164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/10100 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 90, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ElasticNetCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 59, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5714963963427607\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5761274765793847\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.4, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.578445354541721\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.4, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.578445354541721\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.4, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.587106735581138\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5910732310372906\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 68, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5910732310372906\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-3\t0.6237384777234363\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 75, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5935186270042389\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-3\t0.6237384777234363\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6240371403906845\tKNeighborsRegressor(MinMaxScaler(ZeroCount(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100))), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5935186270042389\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6124101219934989\tKNeighborsRegressor(MinMaxScaler(input_matrix), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-3\t0.6237384777234363\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6240371403906845\tKNeighborsRegressor(MinMaxScaler(ZeroCount(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100))), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 95, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 86, n_samples_fit = 50, n_samples = 50.\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.6568093874928962\tKNeighborsRegressor(MinMaxScaler(CombineDFs(MinMaxScaler(input_matrix), RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100))), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6529674564874137\tXGBRegressor(MaxAbsScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6613043185216237\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6524471881549649\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6529674564874137\tXGBRegressor(MaxAbsScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6613043185216237\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6529674564874137\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6613043185216237\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6645286566922379\tKNeighborsRegressor(MinMaxScaler(CombineDFs(MinMaxScaler(input_matrix), RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100))), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 62, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6529674564874137\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6613043185216237\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6703696710100429\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6529674564874137\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6668041100198824\tXGBRegressor(MaxAbsScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.6703696710100429\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.6703696710100429\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-5\t0.6735500048552386\tKNeighborsRegressor(KNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=49, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-5\t0.6735500048552386\tKNeighborsRegressor(KNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=49, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6743256291445755\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6743256291445755\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6743256291445755\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6743256291445755\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6743256291445755\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 82, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6743256291445755\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 54, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6743256291445755\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-5\t0.6796841452320592\tKNeighborsRegressor(KNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=16, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=44, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 70, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6743256291445755\tKNeighborsRegressor(MinMaxScaler(SGDRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6500000000000001, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-5\t0.6796841452320592\tKNeighborsRegressor(KNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=16, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=44, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:06:36] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 87, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6729735843235535\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6766648755371372\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-5\t0.6796841452320592\tKNeighborsRegressor(KNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=16, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=44, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6730753591023616\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6766648755371372\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-5\t0.6796841452320592\tKNeighborsRegressor(KNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=16, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=44, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6730753591023616\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6766648755371372\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-5\t0.6796841452320592\tKNeighborsRegressor(KNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=16, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=44, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 63, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:09:51] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 60, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 91, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 72, n_samples_fit = 50, n_samples = 50.\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6730753591023616\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6766648755371372\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-5\t0.6796841452320592\tKNeighborsRegressor(KNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=16, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=44, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 74, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:10:54] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6730753591023616\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6766648755371372\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-5\t0.6796841452320592\tKNeighborsRegressor(KNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=16, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=44, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6730753591023616\tKNeighborsRegressor(MinMaxScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "\n",
      "-4\t0.6865859726126248\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6758289388037124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.6865859726126248\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:14:00] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6758289388037124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.6865859726126248\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 64, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6758289388037124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.6865859726126248\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6758289388037124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.6865859726126248\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 77, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.6758289388037124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=7, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.6865859726126248\tKNeighborsRegressor(MaxAbsScaler(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7075121650533807\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7075121650533807\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:20:48] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 53, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 92, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7075121650533807\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:22:04] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:22:05] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7075121650533807\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 74, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7075121650533807\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:24:34] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:24:37] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:24:40] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 51 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7075121650533807\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 52 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7075121650533807\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:26:42] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:26:48] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 53 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7075121650533807\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 54 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7116179600662124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:28:41] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 81, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:28:46] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 55 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7116179600662124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 56 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7116179600662124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 78, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 57 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6745188695115236\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7116179600662124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 58 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6745188695115236\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7116179600662124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:31:54] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 59 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6745188695115236\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7116179600662124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 60 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6757991290859457\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7116179600662124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [15:33:36] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:33:37] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 61 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6757991290859457\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7116179600662124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:34:27] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:34:36] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 62 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6757991290859457\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7116179600662124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:35:23] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 63 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6757991290859457\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7116179600662124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 56, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 85, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 64 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6757991290859457\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7116179600662124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 65 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6757991290859457\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7116179600662124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:38:14] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:38:14] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [15:38:14] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 66 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6757991290859457\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7116179600662124\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=3, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:39:20] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:39:23] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:39:26] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 72, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:39:29] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:39:29] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [15:39:29] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 67 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6757991290859457\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7122138895846563\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:40:22] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:40:28] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 68 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6757991290859457\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7122138895846563\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:41:19] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:41:22] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 69 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6757991290859457\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7122138895846563\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 70 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6757991290859457\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7122138895846563\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 69, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:42:48] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 72, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 71 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6757991290859457\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7122138895846563\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:43:42] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 81, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 56, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 72 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6757991290859457\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7122138895846563\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 73 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6757991290859457\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:45:24] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 74 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6757991290859457\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:46:05] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 75 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6757991290859457\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 81, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 76 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6757991290859457\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 77 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6757991290859457\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 78 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6757991290859457\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 79 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7266995359565959\tXGBRegressor(KNeighborsRegressor(ZeroCount(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 61, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 82, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 80 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 81 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 82 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 65, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 83 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [15:52:42] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 84 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 91, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 85 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 86 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 87 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 88 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 89 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 90 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 91 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 92 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 93 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 94 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.712950300057439\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 91, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [16:01:58] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 95 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.719003118762019\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 96 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.719003118762019\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 97 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.719003118762019\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 56, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 98 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.719003118762019\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 99 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.719003118762019\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 100 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6668041100198824\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6870520802236978\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.719003118762019\tXGBRegressor(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.7518517716503027\tXGBRegressor(ZeroCount(KNeighborsRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=10, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=6, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=uniform)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "training r2 is :  0.975666476730004\n",
      "testing r2 is :  -1.204435349617885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=100, population_size=100, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_combined_MYL_8.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbbb2a84-7623-4de3-bff3-63e87c8f6679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/1200 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4671046681340917\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.4, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.55)\n",
      "\n",
      "-2\t0.6028879621428158\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5743541510255828\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=12, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)\n",
      "\n",
      "-2\t0.6028879621428158\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5743541510255828\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=12, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)\n",
      "\n",
      "-2\t0.6028879621428158\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.625993432722259\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6270613432115797\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6424555919437996\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6718677863984384\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "training r2 is :  0.9603175843760428\n",
      "testing r2 is :  -0.24334159483573292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=5, population_size=200, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_combined_MYL_9.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b37ec58-f464-42b3-9de8-abed9dc1afc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/2200 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4671046681340917\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.4, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.55)\n",
      "\n",
      "-2\t0.6028879621428158\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5743541510255828\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=12, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)\n",
      "\n",
      "-2\t0.6028879621428158\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5743541510255828\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=12, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)\n",
      "\n",
      "-2\t0.6028879621428158\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.625993432722259\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6270613432115797\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6424555919437996\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6718677863984384\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6424555919437996\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6718677863984384\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6424555919437996\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6718677863984384\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.691350209187934\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.691350209187934\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.691350209187934\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "training r2 is :  0.9759165489159869\n",
      "testing r2 is :  0.5869015741128331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=10, population_size=200, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_combined_MYL_10.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bdcb500-c287-45e2-b71e-7e620c7fd4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/10200 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4671046681340917\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.4, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.55)\n",
      "\n",
      "-2\t0.6028879621428158\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5743541510255828\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=12, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)\n",
      "\n",
      "-2\t0.6028879621428158\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5743541510255828\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=12, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)\n",
      "\n",
      "-2\t0.6028879621428158\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.625993432722259\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6270613432115797\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6424555919437996\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6718677863984384\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6424555919437996\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6718677863984384\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6424555919437996\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6718677863984384\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.691350209187934\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.691350209187934\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.691350209187934\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.691350209187934\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6915358869458739\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 72, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7025453011381159\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7131667434956798\tExtraTreesRegressor(ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7025453011381159\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7131667434956798\tExtraTreesRegressor(ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 74, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7025453011381159\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7131667434956798\tExtraTreesRegressor(ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7135224443300601\tExtraTreesRegressor(ExtraTreesRegressor(MinMaxScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7025453011381159\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7131667434956798\tExtraTreesRegressor(ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7135224443300601\tExtraTreesRegressor(ExtraTreesRegressor(MinMaxScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7131667434956798\tExtraTreesRegressor(ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7135224443300601\tExtraTreesRegressor(ExtraTreesRegressor(MinMaxScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7131667434956798\tExtraTreesRegressor(ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7159110943980795\tExtraTreesRegressor(ExtraTreesRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.0005), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7425313504162576\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 70, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7559350014467412\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7559350014467412\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7559350014467412\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 89, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7559350014467412\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7559350014467412\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7559350014467412\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7559350014467412\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7565275145927821\tExtraTreesRegressor(RobustScaler(Normalizer(input_matrix, Normalizer__norm=max)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 60, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 89, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _mate_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 56, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.7642169415744056\tExtraTreesRegressor(MinMaxScaler(RobustScaler(Normalizer(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), Normalizer__norm=max))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.7642169415744056\tExtraTreesRegressor(MinMaxScaler(RobustScaler(Normalizer(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), Normalizer__norm=max))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7591398771204204\tExtraTreesRegressor(MinMaxScaler(Normalizer(ZeroCount(input_matrix), Normalizer__norm=max)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.7642169415744056\tExtraTreesRegressor(MinMaxScaler(RobustScaler(Normalizer(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), Normalizer__norm=max))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.01000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7591398771204204\tExtraTreesRegressor(Normalizer(ZeroCount(input_matrix), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.7642169415744056\tExtraTreesRegressor(MinMaxScaler(RobustScaler(Normalizer(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), Normalizer__norm=max))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7591398771204204\tExtraTreesRegressor(Normalizer(ZeroCount(input_matrix), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7642894768038775\tExtraTreesRegressor(MinMaxScaler(PolynomialFeatures(Normalizer(input_matrix, Normalizer__norm=max), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by Normalizer..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7618500105551667\tExtraTreesRegressor(RobustScaler(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7642894768038775\tExtraTreesRegressor(MinMaxScaler(PolynomialFeatures(Normalizer(input_matrix, Normalizer__norm=max), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 61, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7642894768038775\tExtraTreesRegressor(MinMaxScaler(PolynomialFeatures(Normalizer(input_matrix, Normalizer__norm=max), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 81, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7642894768038775\tExtraTreesRegressor(MinMaxScaler(PolynomialFeatures(Normalizer(input_matrix, Normalizer__norm=max), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 59, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7642894768038775\tExtraTreesRegressor(MinMaxScaler(PolynomialFeatures(Normalizer(input_matrix, Normalizer__norm=max), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7642894768038775\tExtraTreesRegressor(MinMaxScaler(PolynomialFeatures(Normalizer(input_matrix, Normalizer__norm=max), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by Normalizer..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 76, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by Normalizer..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7647980606465548\tExtraTreesRegressor(Normalizer(CombineDFs(input_matrix, Normalizer(input_matrix, Normalizer__norm=max)), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7647980606465548\tExtraTreesRegressor(Normalizer(CombineDFs(input_matrix, Normalizer(input_matrix, Normalizer__norm=max)), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7651902988991874\tExtraTreesRegressor(RobustScaler(Normalizer(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), Normalizer__norm=max)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 95, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7647980606465548\tExtraTreesRegressor(Normalizer(CombineDFs(input_matrix, Normalizer(input_matrix, Normalizer__norm=max)), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.785636237330727\tExtraTreesRegressor(DecisionTreeRegressor(MinMaxScaler(Normalizer(input_matrix, Normalizer__norm=max)), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=20), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7647980606465548\tExtraTreesRegressor(Normalizer(CombineDFs(input_matrix, Normalizer(input_matrix, Normalizer__norm=max)), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.785636237330727\tExtraTreesRegressor(DecisionTreeRegressor(MinMaxScaler(Normalizer(input_matrix, Normalizer__norm=max)), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=20), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 86, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 60, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7647980606465548\tExtraTreesRegressor(Normalizer(CombineDFs(input_matrix, Normalizer(input_matrix, Normalizer__norm=max)), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.785636237330727\tExtraTreesRegressor(DecisionTreeRegressor(MinMaxScaler(Normalizer(input_matrix, Normalizer__norm=max)), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=20), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 78, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7647980606465548\tExtraTreesRegressor(Normalizer(CombineDFs(input_matrix, Normalizer(input_matrix, Normalizer__norm=max)), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.785636237330727\tExtraTreesRegressor(DecisionTreeRegressor(MinMaxScaler(Normalizer(input_matrix, Normalizer__norm=max)), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=20), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7755351618740816\tExtraTreesRegressor(Normalizer(CombineDFs(input_matrix, Normalizer(input_matrix, Normalizer__norm=max)), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.785636237330727\tExtraTreesRegressor(DecisionTreeRegressor(MinMaxScaler(Normalizer(input_matrix, Normalizer__norm=max)), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=20), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 60, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7755351618740816\tExtraTreesRegressor(Normalizer(CombineDFs(input_matrix, Normalizer(input_matrix, Normalizer__norm=max)), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.785636237330727\tExtraTreesRegressor(DecisionTreeRegressor(MinMaxScaler(Normalizer(input_matrix, Normalizer__norm=max)), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=20), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 59, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.785636237330727\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=20), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "training r2 is :  0.9759165489159869\n",
      "testing r2 is :  0.6444498878474605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=50, population_size=200, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_combined_MYL_11.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6c3f23f-daa1-4b94-a855-dae2ea84c328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/20200 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.4671046681340917\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.4, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.55)\n",
      "\n",
      "-2\t0.6028879621428158\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5743541510255828\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=12, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)\n",
      "\n",
      "-2\t0.6028879621428158\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5743541510255828\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=12, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)\n",
      "\n",
      "-2\t0.6028879621428158\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.625993432722259\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6270613432115797\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6424555919437996\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6718677863984384\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6424555919437996\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6718677863984384\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6424555919437996\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6718677863984384\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.691350209187934\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.691350209187934\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.691350209187934\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.691350209187934\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6915358869458739\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 72, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7025453011381159\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7131667434956798\tExtraTreesRegressor(ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7025453011381159\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7131667434956798\tExtraTreesRegressor(ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 74, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7025453011381159\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7131667434956798\tExtraTreesRegressor(ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7135224443300601\tExtraTreesRegressor(ExtraTreesRegressor(MinMaxScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7025453011381159\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7131667434956798\tExtraTreesRegressor(ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7135224443300601\tExtraTreesRegressor(ExtraTreesRegressor(MinMaxScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7131667434956798\tExtraTreesRegressor(ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7135224443300601\tExtraTreesRegressor(ExtraTreesRegressor(MinMaxScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7131667434956798\tExtraTreesRegressor(ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7159110943980795\tExtraTreesRegressor(ExtraTreesRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.0005), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7425313504162576\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 70, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7559350014467412\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7559350014467412\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7559350014467412\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 89, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7559350014467412\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7559350014467412\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7559350014467412\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7559350014467412\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7565275145927821\tExtraTreesRegressor(RobustScaler(Normalizer(input_matrix, Normalizer__norm=max)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 60, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 89, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _mate_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 56, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.7642169415744056\tExtraTreesRegressor(MinMaxScaler(RobustScaler(Normalizer(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), Normalizer__norm=max))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.7642169415744056\tExtraTreesRegressor(MinMaxScaler(RobustScaler(Normalizer(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), Normalizer__norm=max))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7591398771204204\tExtraTreesRegressor(MinMaxScaler(Normalizer(ZeroCount(input_matrix), Normalizer__norm=max)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.7642169415744056\tExtraTreesRegressor(MinMaxScaler(RobustScaler(Normalizer(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), Normalizer__norm=max))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.01000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7591398771204204\tExtraTreesRegressor(Normalizer(ZeroCount(input_matrix), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.7642169415744056\tExtraTreesRegressor(MinMaxScaler(RobustScaler(Normalizer(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), Normalizer__norm=max))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7591398771204204\tExtraTreesRegressor(Normalizer(ZeroCount(input_matrix), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7642894768038775\tExtraTreesRegressor(MinMaxScaler(PolynomialFeatures(Normalizer(input_matrix, Normalizer__norm=max), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by Normalizer..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7565275145927821\tExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7618500105551667\tExtraTreesRegressor(RobustScaler(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7642894768038775\tExtraTreesRegressor(MinMaxScaler(PolynomialFeatures(Normalizer(input_matrix, Normalizer__norm=max), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 61, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7642894768038775\tExtraTreesRegressor(MinMaxScaler(PolynomialFeatures(Normalizer(input_matrix, Normalizer__norm=max), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 81, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7642894768038775\tExtraTreesRegressor(MinMaxScaler(PolynomialFeatures(Normalizer(input_matrix, Normalizer__norm=max), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 59, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7642894768038775\tExtraTreesRegressor(MinMaxScaler(PolynomialFeatures(Normalizer(input_matrix, Normalizer__norm=max), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7642894768038775\tExtraTreesRegressor(MinMaxScaler(PolynomialFeatures(Normalizer(input_matrix, Normalizer__norm=max), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by Normalizer..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 76, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by Normalizer..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7647980606465548\tExtraTreesRegressor(Normalizer(CombineDFs(input_matrix, Normalizer(input_matrix, Normalizer__norm=max)), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7647980606465548\tExtraTreesRegressor(Normalizer(CombineDFs(input_matrix, Normalizer(input_matrix, Normalizer__norm=max)), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7651902988991874\tExtraTreesRegressor(RobustScaler(Normalizer(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), Normalizer__norm=max)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 95, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7647980606465548\tExtraTreesRegressor(Normalizer(CombineDFs(input_matrix, Normalizer(input_matrix, Normalizer__norm=max)), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.785636237330727\tExtraTreesRegressor(DecisionTreeRegressor(MinMaxScaler(Normalizer(input_matrix, Normalizer__norm=max)), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=20), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7647980606465548\tExtraTreesRegressor(Normalizer(CombineDFs(input_matrix, Normalizer(input_matrix, Normalizer__norm=max)), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.785636237330727\tExtraTreesRegressor(DecisionTreeRegressor(MinMaxScaler(Normalizer(input_matrix, Normalizer__norm=max)), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=20), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 86, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 60, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7647980606465548\tExtraTreesRegressor(Normalizer(CombineDFs(input_matrix, Normalizer(input_matrix, Normalizer__norm=max)), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.785636237330727\tExtraTreesRegressor(DecisionTreeRegressor(MinMaxScaler(Normalizer(input_matrix, Normalizer__norm=max)), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=20), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 78, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7647980606465548\tExtraTreesRegressor(Normalizer(CombineDFs(input_matrix, Normalizer(input_matrix, Normalizer__norm=max)), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.785636237330727\tExtraTreesRegressor(DecisionTreeRegressor(MinMaxScaler(Normalizer(input_matrix, Normalizer__norm=max)), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=20), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7755351618740816\tExtraTreesRegressor(Normalizer(CombineDFs(input_matrix, Normalizer(input_matrix, Normalizer__norm=max)), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.785636237330727\tExtraTreesRegressor(DecisionTreeRegressor(MinMaxScaler(Normalizer(input_matrix, Normalizer__norm=max)), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=20), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 60, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7755351618740816\tExtraTreesRegressor(Normalizer(CombineDFs(input_matrix, Normalizer(input_matrix, Normalizer__norm=max)), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.785636237330727\tExtraTreesRegressor(DecisionTreeRegressor(MinMaxScaler(Normalizer(input_matrix, Normalizer__norm=max)), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=20), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 59, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.785636237330727\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=20), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 99, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 100, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 54, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 51 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.785636237330727\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=20), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 52 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.785636237330727\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=20), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 53 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.785636237330727\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=20), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 99, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 70, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 54 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.785636237330727\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=20), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7917556257470629\tExtraTreesRegressor(DecisionTreeRegressor(MinMaxScaler(Normalizer(input_matrix, Normalizer__norm=max)), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 74, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 55 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.785636237330727\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=20), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7917556257470629\tExtraTreesRegressor(DecisionTreeRegressor(MinMaxScaler(Normalizer(input_matrix, Normalizer__norm=max)), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 55, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 56 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7917556257470629\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 57 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7917556257470629\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.7994178906826247\tExtraTreesRegressor(Normalizer(ExtraTreesRegressor(SGDRegressor(Normalizer(input_matrix, Normalizer__norm=max), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 58 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7917556257470629\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.7994178906826247\tExtraTreesRegressor(Normalizer(ExtraTreesRegressor(SGDRegressor(Normalizer(input_matrix, Normalizer__norm=max), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 59 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7917556257470629\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.7994178906826247\tExtraTreesRegressor(Normalizer(ExtraTreesRegressor(SGDRegressor(Normalizer(input_matrix, Normalizer__norm=max), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.01000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 60 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7927454132706163\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=3, DecisionTreeRegressor__min_samples_split=6), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7994178906826247\tExtraTreesRegressor(ExtraTreesRegressor(SGDRegressor(Normalizer(input_matrix, Normalizer__norm=max), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 75, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 61 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7927454132706163\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=3, DecisionTreeRegressor__min_samples_split=6), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7994178906826247\tExtraTreesRegressor(ExtraTreesRegressor(SGDRegressor(Normalizer(input_matrix, Normalizer__norm=max), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 62 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7927454132706163\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=3, DecisionTreeRegressor__min_samples_split=6), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7994178906826247\tExtraTreesRegressor(ExtraTreesRegressor(SGDRegressor(Normalizer(input_matrix, Normalizer__norm=max), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 63 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7927454132706163\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=3, DecisionTreeRegressor__min_samples_split=6), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7994178906826247\tExtraTreesRegressor(ExtraTreesRegressor(SGDRegressor(Normalizer(input_matrix, Normalizer__norm=max), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.8002384411002158\tExtraTreesRegressor(Normalizer(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=18, ExtraTreesRegressor__n_estimators=100), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 64 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7927454132706163\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=3, DecisionTreeRegressor__min_samples_split=6), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7994178906826247\tExtraTreesRegressor(ExtraTreesRegressor(SGDRegressor(Normalizer(input_matrix, Normalizer__norm=max), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.8002384411002158\tExtraTreesRegressor(Normalizer(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=18, ExtraTreesRegressor__n_estimators=100), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 100, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 65 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7927454132706163\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=3, DecisionTreeRegressor__min_samples_split=6), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7994178906826247\tExtraTreesRegressor(ExtraTreesRegressor(SGDRegressor(Normalizer(input_matrix, Normalizer__norm=max), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.8002384411002158\tExtraTreesRegressor(Normalizer(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=18, ExtraTreesRegressor__n_estimators=100), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 66 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7927454132706163\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=3, DecisionTreeRegressor__min_samples_split=6), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7994178906826247\tExtraTreesRegressor(ExtraTreesRegressor(SGDRegressor(Normalizer(input_matrix, Normalizer__norm=max), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.8002384411002158\tExtraTreesRegressor(Normalizer(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=18, ExtraTreesRegressor__n_estimators=100), Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 67 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7927454132706163\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=3, DecisionTreeRegressor__min_samples_split=6), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.8002384411002158\tExtraTreesRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=18, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 68 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7927454132706163\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=3, DecisionTreeRegressor__min_samples_split=6), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.8002384411002158\tExtraTreesRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=18, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 56, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 90, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 69 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7927454132706163\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=3, DecisionTreeRegressor__min_samples_split=6), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.8002384411002158\tExtraTreesRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=18, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 55, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 70 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7927454132706163\tExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=3, DecisionTreeRegressor__min_samples_split=6), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.8002384411002158\tExtraTreesRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=18, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 63, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 71 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.809527469754092\tExtraTreesRegressor(OneHotEncoder(ExtraTreesRegressor(SGDRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=10), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeRegressor..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 72 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.809527469754092\tExtraTreesRegressor(OneHotEncoder(ExtraTreesRegressor(SGDRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=10), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 73 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7618500105551667\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.809527469754092\tExtraTreesRegressor(OneHotEncoder(ExtraTreesRegressor(SGDRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=13, DecisionTreeRegressor__min_samples_split=10), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 74 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7631117355888498\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.8124056870513503\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=18, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 75 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7631117355888498\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.8124056870513503\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=18, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 62, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 76 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7631117355888498\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.8124056870513503\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=18, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 77 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7631117355888498\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.8124056870513503\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=18, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 78 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7631117355888498\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.8124056870513503\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=18, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 79 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7631117355888498\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.8124056870513503\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=18, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 72, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 80 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7631117355888498\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.8124056870513503\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=18, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.8139227242430224\tExtraTreesRegressor(OneHotEncoder(ExtraTreesRegressor(SGDRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=3), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 100, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 61, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 81 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7631117355888498\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.8124056870513503\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=18, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.8139227242430224\tExtraTreesRegressor(OneHotEncoder(ExtraTreesRegressor(SGDRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=3), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 73, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 65, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _mate_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _mate_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 82 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7631117355888498\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.815080490474416\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 83 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7631117355888498\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.815080490474416\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 83, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 84 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7631117355888498\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.815080490474416\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 74, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 82, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.01000.\n",
      "\n",
      "Generation 85 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7631117355888498\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.815080490474416\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 86 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7631117355888498\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.815080490474416\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 85, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 69, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.\n",
      "\n",
      "Generation 87 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7631117355888498\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.815080490474416\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 81, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 88 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7631117355888498\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.815080490474416\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 89 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7631117355888498\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.815080490474416\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.8179924738162991\tExtraTreesRegressor(MinMaxScaler(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(MinMaxScaler(Normalizer(input_matrix, Normalizer__norm=max)), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=18, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 No feature in X meets the variance threshold 0.20000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 59, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 77, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 90 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7631117355888498\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.815080490474416\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.8157722289091721\tExtraTreesRegressor(MinMaxScaler(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=7), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=12, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.8179924738162991\tExtraTreesRegressor(MinMaxScaler(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(MinMaxScaler(Normalizer(input_matrix, Normalizer__norm=max)), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=18, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 91 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7631117355888498\tExtraTreesRegressor(CombineDFs(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.8040642722911089\tExtraTreesRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=15), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=17, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.815080490474416\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.8157722289091721\tExtraTreesRegressor(MinMaxScaler(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=7), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=12, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-7\t0.8179924738162991\tExtraTreesRegressor(MinMaxScaler(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(MinMaxScaler(Normalizer(input_matrix, Normalizer__norm=max)), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=18, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 95, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 92 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7636055596942845\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.8040642722911089\tExtraTreesRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=15), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=17, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.815080490474416\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=19, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t0.8179924738162991\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(MinMaxScaler(Normalizer(input_matrix, Normalizer__norm=max)), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=18, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 93 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7636055596942845\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.8040642722911089\tExtraTreesRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=15), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=17, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.8201970900938044\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=5), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 92, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 59, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 94 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7636055596942845\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.8040642722911089\tExtraTreesRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=15), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=17, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.8201970900938044\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=5), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 78, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 95 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7636055596942845\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.8040642722911089\tExtraTreesRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=15), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=17, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.8201970900938044\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=5), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 85, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 96 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7636055596942845\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.8040642722911089\tExtraTreesRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=15), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=17, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.8201970900938044\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=5), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 92, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 97 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7636055596942845\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.8040642722911089\tExtraTreesRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=15), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=17, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.8201970900938044\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=5), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 98 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7636055596942845\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.8040642722911089\tExtraTreesRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=15), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=17, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.8201970900938044\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=5), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 81, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 71, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 99 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7636055596942845\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.8040642722911089\tExtraTreesRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=15), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=17, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.8201970900938044\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=5), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 94, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 62, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 100 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7225763040495495\tExtraTreesRegressor(CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7636055596942845\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=8, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.8016379384643489\tExtraTreesRegressor(ExtraTreesRegressor(Normalizer(input_matrix, Normalizer__norm=max), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.8040642722911089\tExtraTreesRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=15), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=17, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t0.8201970900938044\tExtraTreesRegressor(DecisionTreeRegressor(ExtraTreesRegressor(DecisionTreeRegressor(Normalizer(input_matrix, Normalizer__norm=max), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=5), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=14, DecisionTreeRegressor__min_samples_split=3), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "training r2 is :  0.9759165489159869\n",
      "testing r2 is :  0.6309067531969847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=100, population_size=200, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_combined_MYL_12.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f70a99e6-34cd-447b-adff-859f12c60815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/2400 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _mate_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5889045049425722\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6008168143030513\tExtraTreesRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.5, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5889045049425722\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6008168143030513\tExtraTreesRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.5, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5889045049425722\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6100698440754012\tExtraTreesRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=30, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6099738599937761\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=8, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6263886658490353\tAdaBoostRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 69, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 75, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6099738599937761\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=8, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6263886658490353\tAdaBoostRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "training r2 is :  0.9236129002653765\n",
      "testing r2 is :  0.23849355968235786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=5, population_size=400, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_combined_MYL_13.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d045962-9651-483f-a84d-79ee186814a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/4400 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _mate_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5889045049425722\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6008168143030513\tExtraTreesRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.5, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5889045049425722\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6008168143030513\tExtraTreesRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.5, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5889045049425722\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6100698440754012\tExtraTreesRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=30, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6099738599937761\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=8, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6263886658490353\tAdaBoostRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 69, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 75, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6099738599937761\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=8, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6263886658490353\tAdaBoostRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:30:48] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 69, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6314900269733645\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6419296004768439\tAdaBoostRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6314900269733645\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6876636740831557\tAdaBoostRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 96, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6314900269733645\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6876636740831557\tAdaBoostRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 99, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:43:09] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6485184199402505\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6896507240680316\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [23:46:35] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6687816808033098\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6896507240680316\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "training r2 is :  0.9595090088166542\n",
      "testing r2 is :  -0.3743385412116269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=10, population_size=400, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_combined_MYL_14.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0cbbafbc-ea14-44e8-ac70-e4202c58a39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/20400 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _mate_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5889045049425722\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6008168143030513\tExtraTreesRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.5, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5889045049425722\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6008168143030513\tExtraTreesRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.5, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5889045049425722\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6100698440754012\tExtraTreesRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=30, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6099738599937761\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=8, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6263886658490353\tAdaBoostRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 69, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 75, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6099738599937761\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=8, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6263886658490353\tAdaBoostRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:10:07] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 69, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6314900269733645\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6419296004768439\tAdaBoostRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6314900269733645\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6876636740831557\tAdaBoostRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 96, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6314900269733645\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6876636740831557\tAdaBoostRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 99, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:22:26] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6485184199402505\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6896507240680316\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [00:25:51] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6687816808033098\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6896507240680316\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 90, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 58, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 68, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6687816808033098\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6896507240680316\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.694902261922206\tExtraTreesRegressor(ZeroCount(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6687816808033098\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6896507240680316\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.694902261922206\tExtraTreesRegressor(ZeroCount(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6687816808033098\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6896507240680316\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.694902261922206\tExtraTreesRegressor(ZeroCount(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:39:11] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6687816808033098\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6914157781790449\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.694902261922206\tExtraTreesRegressor(ZeroCount(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6687816808033098\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.7002869188785099\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:45:35] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6687816808033098\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.7002869188785099\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:49:23] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6687816808033098\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.7246417605405978\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6687816808033098\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.7246417605405978\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:56:14] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:56:33] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 80, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:56:49] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6845719374329496\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7246417605405978\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7025453011381159\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7246417605405978\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.73153937165503\tXGBRegressor(XGBRegressor(MaxAbsScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7025453011381159\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7246417605405978\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.73153937165503\tXGBRegressor(XGBRegressor(MaxAbsScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:08:31] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7025453011381159\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7246417605405978\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.73153937165503\tXGBRegressor(XGBRegressor(MaxAbsScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 53, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7246417605405978\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.73153937165503\tXGBRegressor(XGBRegressor(MaxAbsScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:17:32] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7246417605405978\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.73153937165503\tXGBRegressor(XGBRegressor(MaxAbsScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7246417605405978\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.73153937165503\tXGBRegressor(XGBRegressor(MaxAbsScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.73153937165503\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7418051100500999\tExtraTreesRegressor(ExtraTreesRegressor(MaxAbsScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.73153937165503\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7418051100500999\tExtraTreesRegressor(ExtraTreesRegressor(MaxAbsScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 97, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7333776884204501\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7418051100500999\tExtraTreesRegressor(ExtraTreesRegressor(MaxAbsScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7333776884204501\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7418051100500999\tExtraTreesRegressor(ExtraTreesRegressor(MaxAbsScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:45:22] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7333776884204501\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7418051100500999\tExtraTreesRegressor(ExtraTreesRegressor(MaxAbsScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:49:59] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7452684260357939\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7590731422078744\tExtraTreesRegressor(AdaBoostRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 78, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 97, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 76, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 65, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 70, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.767664364358388\tExtraTreesRegressor(OneHotEncoder(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:31:02] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:31:24] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 66, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.767664364358388\tExtraTreesRegressor(OneHotEncoder(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.767664364358388\tExtraTreesRegressor(OneHotEncoder(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.767664364358388\tExtraTreesRegressor(OneHotEncoder(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:46:22] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7764061792654358\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 73, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:52:31] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7764061792654358\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 59, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7764061792654358\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 100, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 62, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7764061792654358\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 58, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _mate_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7764061792654358\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7764061792654358\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [03:17:03] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7683070515839261\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.8, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7764061792654358\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 82, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7683070515839261\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.8, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7764061792654358\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "training r2 is :  0.9759165489159869\n",
      "testing r2 is :  -0.7220646594808895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=50, population_size=400, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_combined_MYL_15.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c152a5d0-187c-465e-b10d-a61aa80eade6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/40400 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _mate_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5889045049425722\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6008168143030513\tExtraTreesRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.5, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5889045049425722\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6008168143030513\tExtraTreesRegressor(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.5, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5889045049425722\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.6100698440754012\tExtraTreesRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=30, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=uniform), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6099738599937761\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=8, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6263886658490353\tAdaBoostRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 69, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 75, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6099738599937761\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=8, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6263886658490353\tAdaBoostRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.01, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [03:46:05] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 69, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6314900269733645\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6419296004768439\tAdaBoostRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6314900269733645\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6876636740831557\tAdaBoostRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 96, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6314900269733645\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6876636740831557\tAdaBoostRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.15000000000000002, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 99, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [03:58:24] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6485184199402505\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6896507240680316\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [04:01:49] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6687816808033098\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6896507240680316\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 90, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 58, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 68, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6687816808033098\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6896507240680316\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.694902261922206\tExtraTreesRegressor(ZeroCount(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6687816808033098\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6896507240680316\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.694902261922206\tExtraTreesRegressor(ZeroCount(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6687816808033098\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6896507240680316\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.694902261922206\tExtraTreesRegressor(ZeroCount(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:15:09] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6687816808033098\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.6914157781790449\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.694902261922206\tExtraTreesRegressor(ZeroCount(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6687816808033098\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.7002869188785099\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:21:34] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6687816808033098\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.7002869188785099\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:25:21] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6687816808033098\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.7246417605405978\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6687816808033098\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.7246417605405978\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:32:13] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:32:32] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 80, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:32:48] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6845719374329496\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7246417605405978\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7025453011381159\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7246417605405978\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.73153937165503\tXGBRegressor(XGBRegressor(MaxAbsScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7025453011381159\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7246417605405978\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.73153937165503\tXGBRegressor(XGBRegressor(MaxAbsScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:44:30] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7025453011381159\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7246417605405978\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.73153937165503\tXGBRegressor(XGBRegressor(MaxAbsScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 53, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7246417605405978\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.73153937165503\tXGBRegressor(XGBRegressor(MaxAbsScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:53:31] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7246417605405978\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.73153937165503\tXGBRegressor(XGBRegressor(MaxAbsScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7246417605405978\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.73153937165503\tXGBRegressor(XGBRegressor(MaxAbsScaler(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.73153937165503\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7418051100500999\tExtraTreesRegressor(ExtraTreesRegressor(MaxAbsScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.73153937165503\tXGBRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8, XGBRegressor__verbosity=0), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7418051100500999\tExtraTreesRegressor(ExtraTreesRegressor(MaxAbsScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 97, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7333776884204501\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7418051100500999\tExtraTreesRegressor(ExtraTreesRegressor(MaxAbsScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7333776884204501\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7418051100500999\tExtraTreesRegressor(ExtraTreesRegressor(MaxAbsScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [05:21:22] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7333776884204501\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7418051100500999\tExtraTreesRegressor(ExtraTreesRegressor(MaxAbsScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [05:26:00] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7452684260357939\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7590731422078744\tExtraTreesRegressor(AdaBoostRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 78, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 97, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 76, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 65, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 70, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.767664364358388\tExtraTreesRegressor(OneHotEncoder(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [06:07:03] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [06:07:25] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 66, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.767664364358388\tExtraTreesRegressor(OneHotEncoder(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.767664364358388\tExtraTreesRegressor(OneHotEncoder(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.767664364358388\tExtraTreesRegressor(OneHotEncoder(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [06:22:26] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7764061792654358\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 73, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [06:28:37] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7764061792654358\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 59, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7764061792654358\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 100, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 62, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7764061792654358\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 58, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _mate_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7078767615083088\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7764061792654358\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.761929739016089\tXGBRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=10, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.7764061792654358\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [06:53:08] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7683070515839261\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.8, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7764061792654358\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 82, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7683070515839261\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.8, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7764061792654358\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 51 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7683070515839261\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.8, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7764061792654358\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 90, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 52 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7683070515839261\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.8, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7764061792654358\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 53 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7683070515839261\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.8, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7764061792654358\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 93, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 97, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 54 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7683070515839261\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.8, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7764061792654358\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 55 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7683070515839261\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.8, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7771046870994109\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 56 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7683070515839261\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.8, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7771046870994109\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=53), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.7799996760409398\tExtraTreesRegressor(MinMaxScaler(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=60)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 54, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [07:30:59] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [07:31:24] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 57 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7683070515839261\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.8, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7799996760409398\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=59), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _mate_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 58 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7683070515839261\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.8, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7799996760409398\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=59), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 64, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 59 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7683070515839261\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.8, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7799996760409398\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=59), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 61, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _mate_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 60 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7683070515839261\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.8, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _mate_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 61 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 73, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 62 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _mate_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 62, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 63 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 86, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 75, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 63, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 64 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 65 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 85, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 66 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 67 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 68 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 89, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 69 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 70 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 71 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 83, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Expected n_neighbors <= n_samples_fit, but n_neighbors = 65, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 72 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 89, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 58, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 73 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 74 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 75 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 76 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7092393241289575\tExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 96, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 77 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 78 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 79 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 74, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 61, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 80 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 81 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _mate_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _mate_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [10:01:33] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 82 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 65, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 86, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 83 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 84 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 54, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 85 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 89, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 61, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 86 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 87 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 88 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 89 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 90 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 91 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 92 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 86, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 66, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 93 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 94 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 95 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 60, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 96 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 81, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 97 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.8004478416927207\tExtraTreesRegressor(OneHotEncoder(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=47), OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 93, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 54, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 98 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.8004478416927207\tExtraTreesRegressor(OneHotEncoder(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=47), OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 64, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 99 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.8004478416927207\tExtraTreesRegressor(OneHotEncoder(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=47), OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'huber', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'huber', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 100 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7098392693371451\tExtraTreesRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7709191354782632\tExtraTreesRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=8, GradientBoostingRegressor__min_samples_split=13, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.7958462245691078\tExtraTreesRegressor(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=44), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t0.8004478416927207\tExtraTreesRegressor(OneHotEncoder(SelectPercentile(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), SelectPercentile__percentile=47), OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=1.0, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "training r2 is :  0.9759165489159869\n",
      "testing r2 is :  -0.5624120501646857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=100, population_size=400, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_P3HT_pipeline_combined_MYL_16.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4754f5d4-d9a5-4ef5-8c05-36b7ca46a27e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
