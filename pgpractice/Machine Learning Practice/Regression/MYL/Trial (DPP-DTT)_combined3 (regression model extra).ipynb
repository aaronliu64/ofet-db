{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d81314c0",
   "metadata": {},
   "source": [
    "# Regression modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ce2aac",
   "metadata": {},
   "source": [
    "## 1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afad0db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c24bd022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>solution_concentration</th>\n",
       "      <th>polymer_mw</th>\n",
       "      <th>polymer_mn</th>\n",
       "      <th>polymer_dispersity</th>\n",
       "      <th>hole_mobility</th>\n",
       "      <th>solution_treatment</th>\n",
       "      <th>substrate_pretreatment</th>\n",
       "      <th>post_process</th>\n",
       "      <th>channel_width</th>\n",
       "      <th>channel_length</th>\n",
       "      <th>film_deposition_type_spin</th>\n",
       "      <th>dielectric_material_SiO2</th>\n",
       "      <th>electrode_configuration_BGBC</th>\n",
       "      <th>electrode_configuration_BGTC</th>\n",
       "      <th>electrode_configuration_TGBC</th>\n",
       "      <th>gate_material_Other</th>\n",
       "      <th>film_deposition_type_MGC</th>\n",
       "      <th>dielectric_material_other</th>\n",
       "      <th>solvent_boiling_point</th>\n",
       "      <th>delta_d</th>\n",
       "      <th>delta_p</th>\n",
       "      <th>delta_h</th>\n",
       "      <th>blend_conjugated_polymer</th>\n",
       "      <th>insulating_polymer</th>\n",
       "      <th>substrate_pretreat_sam</th>\n",
       "      <th>substrate_pretreat_plasma</th>\n",
       "      <th>substrate_pretreat_uv_ozone</th>\n",
       "      <th>solution_treatment_poor_solvent</th>\n",
       "      <th>solution_treatment_aging</th>\n",
       "      <th>solution_treatment_sonication</th>\n",
       "      <th>solution_treatment_mixing</th>\n",
       "      <th>solution_treatment_mixing_multiple</th>\n",
       "      <th>solution_treatment_uv_irradiation</th>\n",
       "      <th>post_process_annealing</th>\n",
       "      <th>post_process_drying</th>\n",
       "      <th>post_process_chemical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.00</td>\n",
       "      <td>299.00</td>\n",
       "      <td>90.000</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.00</td>\n",
       "      <td>299.00</td>\n",
       "      <td>90.000</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.00</td>\n",
       "      <td>299.00</td>\n",
       "      <td>90.000</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.00</td>\n",
       "      <td>299.00</td>\n",
       "      <td>90.000</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.00</td>\n",
       "      <td>299.00</td>\n",
       "      <td>90.000</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1.860000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.00</td>\n",
       "      <td>299.00</td>\n",
       "      <td>90.000</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.00</td>\n",
       "      <td>299.00</td>\n",
       "      <td>90.000</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.00</td>\n",
       "      <td>299.00</td>\n",
       "      <td>90.000</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.00</td>\n",
       "      <td>299.00</td>\n",
       "      <td>90.000</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1.970000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.00</td>\n",
       "      <td>299.00</td>\n",
       "      <td>90.000</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.00</td>\n",
       "      <td>299.00</td>\n",
       "      <td>90.000</td>\n",
       "      <td>3.32</td>\n",
       "      <td>2.040000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.20</td>\n",
       "      <td>74.900</td>\n",
       "      <td>3.90</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.20</td>\n",
       "      <td>74.900</td>\n",
       "      <td>3.90</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.20</td>\n",
       "      <td>74.900</td>\n",
       "      <td>3.90</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.20</td>\n",
       "      <td>74.900</td>\n",
       "      <td>3.90</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.20</td>\n",
       "      <td>74.900</td>\n",
       "      <td>3.90</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.20</td>\n",
       "      <td>74.900</td>\n",
       "      <td>3.90</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.20</td>\n",
       "      <td>74.900</td>\n",
       "      <td>3.90</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.20</td>\n",
       "      <td>74.900</td>\n",
       "      <td>3.90</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.20</td>\n",
       "      <td>74.900</td>\n",
       "      <td>3.90</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.00</td>\n",
       "      <td>501.00</td>\n",
       "      <td>110.000</td>\n",
       "      <td>4.55</td>\n",
       "      <td>6.850000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4000</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.00</td>\n",
       "      <td>501.00</td>\n",
       "      <td>110.000</td>\n",
       "      <td>4.55</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4000</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.00</td>\n",
       "      <td>501.00</td>\n",
       "      <td>110.000</td>\n",
       "      <td>4.55</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1400</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.00</td>\n",
       "      <td>501.00</td>\n",
       "      <td>110.000</td>\n",
       "      <td>4.55</td>\n",
       "      <td>7.950000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1400</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.00</td>\n",
       "      <td>501.00</td>\n",
       "      <td>110.000</td>\n",
       "      <td>4.55</td>\n",
       "      <td>7.850000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1400</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.00</td>\n",
       "      <td>501.00</td>\n",
       "      <td>110.000</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1400</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.00</td>\n",
       "      <td>501.00</td>\n",
       "      <td>110.000</td>\n",
       "      <td>4.55</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1400</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.00</td>\n",
       "      <td>501.00</td>\n",
       "      <td>110.000</td>\n",
       "      <td>4.55</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1400</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.00</td>\n",
       "      <td>501.00</td>\n",
       "      <td>110.000</td>\n",
       "      <td>4.55</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4000</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.00</td>\n",
       "      <td>501.00</td>\n",
       "      <td>110.000</td>\n",
       "      <td>4.55</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1400</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.00</td>\n",
       "      <td>501.00</td>\n",
       "      <td>110.000</td>\n",
       "      <td>4.55</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7000</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.00</td>\n",
       "      <td>199.00</td>\n",
       "      <td>55.000</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.1</td>\n",
       "      <td>9.39</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.00</td>\n",
       "      <td>199.00</td>\n",
       "      <td>55.000</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.390000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.1</td>\n",
       "      <td>9.39</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4.00</td>\n",
       "      <td>199.00</td>\n",
       "      <td>55.000</td>\n",
       "      <td>3.62</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.1</td>\n",
       "      <td>9.39</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4.00</td>\n",
       "      <td>199.00</td>\n",
       "      <td>55.000</td>\n",
       "      <td>3.62</td>\n",
       "      <td>6.760000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.1</td>\n",
       "      <td>9.39</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4.00</td>\n",
       "      <td>199.00</td>\n",
       "      <td>55.000</td>\n",
       "      <td>3.62</td>\n",
       "      <td>2.570000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.1</td>\n",
       "      <td>9.39</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4.00</td>\n",
       "      <td>199.00</td>\n",
       "      <td>55.000</td>\n",
       "      <td>3.62</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.1</td>\n",
       "      <td>9.39</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4.00</td>\n",
       "      <td>199.00</td>\n",
       "      <td>55.000</td>\n",
       "      <td>3.62</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.1</td>\n",
       "      <td>9.39</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4.00</td>\n",
       "      <td>199.00</td>\n",
       "      <td>55.000</td>\n",
       "      <td>3.62</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.1</td>\n",
       "      <td>9.39</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3.00</td>\n",
       "      <td>290.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.1</td>\n",
       "      <td>9.39</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3.00</td>\n",
       "      <td>290.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.181200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.1</td>\n",
       "      <td>9.39</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>7.00</td>\n",
       "      <td>290.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.090100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.1</td>\n",
       "      <td>9.39</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>7.00</td>\n",
       "      <td>290.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.1</td>\n",
       "      <td>9.39</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>6.50</td>\n",
       "      <td>100.00</td>\n",
       "      <td>67.000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3000</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.60</td>\n",
       "      <td>279.00</td>\n",
       "      <td>77.000</td>\n",
       "      <td>3.65</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.60</td>\n",
       "      <td>279.00</td>\n",
       "      <td>77.000</td>\n",
       "      <td>3.65</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.50</td>\n",
       "      <td>102.00</td>\n",
       "      <td>43.000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.75</td>\n",
       "      <td>102.00</td>\n",
       "      <td>43.000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.00</td>\n",
       "      <td>102.00</td>\n",
       "      <td>43.000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2.00</td>\n",
       "      <td>102.00</td>\n",
       "      <td>43.000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5.00</td>\n",
       "      <td>102.00</td>\n",
       "      <td>43.000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.50</td>\n",
       "      <td>102.00</td>\n",
       "      <td>43.000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.75</td>\n",
       "      <td>102.00</td>\n",
       "      <td>43.000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.00</td>\n",
       "      <td>102.00</td>\n",
       "      <td>43.000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2.00</td>\n",
       "      <td>102.00</td>\n",
       "      <td>43.000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>5.00</td>\n",
       "      <td>102.00</td>\n",
       "      <td>43.000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>5.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>7.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>8.50</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>10.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>12.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>14.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>16.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>18.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>25.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>5.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>7.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>8.50</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>10.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>12.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>14.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>16.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>18.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>25.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>3.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>5.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>7.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>8.50</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>10.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>12.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>14.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>16.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>18.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>25.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>3.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>5.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>7.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>10.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>12.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>14.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>18.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>25.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>3.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.336000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>4.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>6.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.442000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>8.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.397000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5.00</td>\n",
       "      <td>255.00</td>\n",
       "      <td>94.400</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8800</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.1</td>\n",
       "      <td>9.39</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>10.00</td>\n",
       "      <td>193.50</td>\n",
       "      <td>50.000</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>180.1</td>\n",
       "      <td>9.39</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>10.00</td>\n",
       "      <td>193.50</td>\n",
       "      <td>50.000</td>\n",
       "      <td>3.87</td>\n",
       "      <td>1.230000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>180.1</td>\n",
       "      <td>9.39</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>10.00</td>\n",
       "      <td>193.50</td>\n",
       "      <td>50.000</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>180.1</td>\n",
       "      <td>9.39</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>10.00</td>\n",
       "      <td>193.50</td>\n",
       "      <td>50.000</td>\n",
       "      <td>3.87</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>180.1</td>\n",
       "      <td>9.39</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>8.00</td>\n",
       "      <td>54.79</td>\n",
       "      <td>20.052</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4400</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>8.00</td>\n",
       "      <td>54.79</td>\n",
       "      <td>20.052</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4400</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>12.00</td>\n",
       "      <td>54.79</td>\n",
       "      <td>20.052</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4400</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>12.00</td>\n",
       "      <td>54.79</td>\n",
       "      <td>20.052</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4400</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>27.00</td>\n",
       "      <td>54.79</td>\n",
       "      <td>20.052</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4400</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>27.00</td>\n",
       "      <td>54.79</td>\n",
       "      <td>20.052</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4400</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>4.00</td>\n",
       "      <td>250.00</td>\n",
       "      <td>68.000</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.1</td>\n",
       "      <td>9.39</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.042816</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>6.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.113931</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>10.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.061766</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>5.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>5.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>5.00</td>\n",
       "      <td>110.00</td>\n",
       "      <td>44.000</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>5.00</td>\n",
       "      <td>152.00</td>\n",
       "      <td>55.000</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.104000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>5.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.187177</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>5.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.190529</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>5.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.180283</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.159097</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>5.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.129223</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.143917</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.176246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.174302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.092925</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.074403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.124165</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.370376</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.319754</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.257338</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>5.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>143.000</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.193502</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.012535</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>4.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.038333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>6.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.101182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>7.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.198601</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>8.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.163842</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>10.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.170187</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2.00</td>\n",
       "      <td>110.00</td>\n",
       "      <td>44.000</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>4.00</td>\n",
       "      <td>110.00</td>\n",
       "      <td>44.000</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.007139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.00</td>\n",
       "      <td>110.00</td>\n",
       "      <td>44.000</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.037412</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>8.00</td>\n",
       "      <td>110.00</td>\n",
       "      <td>44.000</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.072672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>9.00</td>\n",
       "      <td>110.00</td>\n",
       "      <td>44.000</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.063779</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>10.00</td>\n",
       "      <td>110.00</td>\n",
       "      <td>44.000</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.027642</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>12.00</td>\n",
       "      <td>110.00</td>\n",
       "      <td>44.000</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.046912</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.011943</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>4.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.042120</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.148004</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>7.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.111808</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>8.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.163626</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>10.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>66.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.141648</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     solution_concentration  polymer_mw  polymer_mn  polymer_dispersity  \\\n",
       "0                      4.00      299.00      90.000                3.32   \n",
       "1                      4.00      299.00      90.000                3.32   \n",
       "2                      4.00      299.00      90.000                3.32   \n",
       "3                      4.00      299.00      90.000                3.32   \n",
       "4                      4.00      299.00      90.000                3.32   \n",
       "5                      4.00      299.00      90.000                3.32   \n",
       "6                      4.00      299.00      90.000                3.32   \n",
       "7                      4.00      299.00      90.000                3.32   \n",
       "8                      4.00      299.00      90.000                3.32   \n",
       "9                      4.00      299.00      90.000                3.32   \n",
       "10                     4.00      299.00      90.000                3.32   \n",
       "11                     5.00      292.20      74.900                3.90   \n",
       "12                     5.00      292.20      74.900                3.90   \n",
       "13                     5.00      292.20      74.900                3.90   \n",
       "14                     5.00      292.20      74.900                3.90   \n",
       "15                     5.00      292.20      74.900                3.90   \n",
       "16                     5.00      292.20      74.900                3.90   \n",
       "17                     5.00      292.20      74.900                3.90   \n",
       "18                     5.00      292.20      74.900                3.90   \n",
       "19                     5.00      292.20      74.900                3.90   \n",
       "20                     5.00      501.00     110.000                4.55   \n",
       "21                     5.00      501.00     110.000                4.55   \n",
       "22                     5.00      501.00     110.000                4.55   \n",
       "23                     5.00      501.00     110.000                4.55   \n",
       "24                     5.00      501.00     110.000                4.55   \n",
       "25                     5.00      501.00     110.000                4.55   \n",
       "26                     5.00      501.00     110.000                4.55   \n",
       "27                     5.00      501.00     110.000                4.55   \n",
       "28                     5.00      501.00     110.000                4.55   \n",
       "29                     5.00      501.00     110.000                4.55   \n",
       "30                     5.00      501.00     110.000                4.55   \n",
       "31                     4.00      199.00      55.000                3.62   \n",
       "32                     4.00      199.00      55.000                3.62   \n",
       "33                     4.00      199.00      55.000                3.62   \n",
       "34                     4.00      199.00      55.000                3.62   \n",
       "35                     4.00      199.00      55.000                3.62   \n",
       "36                     4.00      199.00      55.000                3.62   \n",
       "37                     4.00      199.00      55.000                3.62   \n",
       "38                     4.00      199.00      55.000                3.62   \n",
       "39                     3.00      290.00     143.000                2.03   \n",
       "40                     3.00      290.00     143.000                2.03   \n",
       "41                     7.00      290.00     143.000                2.03   \n",
       "42                     7.00      290.00     143.000                2.03   \n",
       "43                     6.50      100.00      67.000                3.00   \n",
       "44                     1.60      279.00      77.000                3.65   \n",
       "45                     1.60      279.00      77.000                3.65   \n",
       "46                     0.50      102.00      43.000                2.00   \n",
       "47                     0.75      102.00      43.000                2.00   \n",
       "48                     1.00      102.00      43.000                2.00   \n",
       "49                     2.00      102.00      43.000                2.00   \n",
       "50                     5.00      102.00      43.000                2.00   \n",
       "51                     0.50      102.00      43.000                2.00   \n",
       "52                     0.75      102.00      43.000                2.00   \n",
       "53                     1.00      102.00      43.000                2.00   \n",
       "54                     2.00      102.00      43.000                2.00   \n",
       "55                     5.00      102.00      43.000                2.00   \n",
       "56                     5.00      104.00      20.000                5.20   \n",
       "57                     7.00      104.00      20.000                5.20   \n",
       "58                     8.50      104.00      20.000                5.20   \n",
       "59                    10.00      104.00      20.000                5.20   \n",
       "60                    12.00      104.00      20.000                5.20   \n",
       "61                    14.00      104.00      20.000                5.20   \n",
       "62                    16.00      104.00      20.000                5.20   \n",
       "63                    18.00      104.00      20.000                5.20   \n",
       "64                    25.00      104.00      20.000                5.20   \n",
       "65                     5.00      104.00      20.000                5.20   \n",
       "66                     7.00      104.00      20.000                5.20   \n",
       "67                     8.50      104.00      20.000                5.20   \n",
       "68                    10.00      104.00      20.000                5.20   \n",
       "69                    12.00      104.00      20.000                5.20   \n",
       "70                    14.00      104.00      20.000                5.20   \n",
       "71                    16.00      104.00      20.000                5.20   \n",
       "72                    18.00      104.00      20.000                5.20   \n",
       "73                    25.00      104.00      20.000                5.20   \n",
       "74                     3.00      104.00      20.000                5.20   \n",
       "75                     5.00      104.00      20.000                5.20   \n",
       "76                     7.00      104.00      20.000                5.20   \n",
       "77                     8.50      104.00      20.000                5.20   \n",
       "78                    10.00      104.00      20.000                5.20   \n",
       "79                    12.00      104.00      20.000                5.20   \n",
       "80                    14.00      104.00      20.000                5.20   \n",
       "81                    16.00      104.00      20.000                5.20   \n",
       "82                    18.00      104.00      20.000                5.20   \n",
       "83                    25.00      104.00      20.000                5.20   \n",
       "84                     3.00      104.00      20.000                5.20   \n",
       "85                     5.00      104.00      20.000                5.20   \n",
       "86                     7.00      104.00      20.000                5.20   \n",
       "87                    10.00      104.00      20.000                5.20   \n",
       "88                    12.00      104.00      20.000                5.20   \n",
       "89                    14.00      104.00      20.000                5.20   \n",
       "90                    18.00      104.00      20.000                5.20   \n",
       "91                    25.00      104.00      20.000                5.20   \n",
       "92                     3.00      292.00     143.000                2.03   \n",
       "93                     4.00      292.00     143.000                2.03   \n",
       "94                     5.00      292.00     143.000                2.03   \n",
       "95                     6.00      292.00     143.000                2.03   \n",
       "96                     8.00      292.00     143.000                2.03   \n",
       "97                     5.00      255.00      94.400                2.70   \n",
       "98                    10.00      193.50      50.000                3.87   \n",
       "99                    10.00      193.50      50.000                3.87   \n",
       "100                   10.00      193.50      50.000                3.87   \n",
       "101                   10.00      193.50      50.000                3.87   \n",
       "102                    8.00       54.79      20.052                2.73   \n",
       "103                    8.00       54.79      20.052                2.73   \n",
       "104                   12.00       54.79      20.052                2.73   \n",
       "105                   12.00       54.79      20.052                2.73   \n",
       "106                   27.00       54.79      20.052                2.73   \n",
       "107                   27.00       54.79      20.052                2.73   \n",
       "108                    4.00      250.00      68.000                3.67   \n",
       "109                    2.00      204.00      66.000                3.10   \n",
       "110                    6.00      204.00      66.000                3.10   \n",
       "111                   10.00      204.00      66.000                3.10   \n",
       "112                    5.00      204.00      66.000                3.10   \n",
       "113                    5.00      292.00     143.000                2.03   \n",
       "114                    5.00      204.00      66.000                3.10   \n",
       "115                    5.00      110.00      44.000                2.50   \n",
       "116                    5.00      152.00      55.000                2.76   \n",
       "117                    5.00      292.00     143.000                2.03   \n",
       "118                    5.00      204.00      66.000                3.10   \n",
       "119                    5.00      204.00      66.000                3.10   \n",
       "120                    5.00      204.00      66.000                3.10   \n",
       "121                    5.00      204.00      66.000                3.10   \n",
       "122                    5.00      204.00      66.000                3.10   \n",
       "123                    5.00      292.00     143.000                2.03   \n",
       "124                    5.00      292.00     143.000                2.03   \n",
       "125                    5.00      292.00     143.000                2.03   \n",
       "126                    5.00      292.00     143.000                2.03   \n",
       "127                    5.00      292.00     143.000                2.03   \n",
       "128                    5.00      292.00     143.000                2.03   \n",
       "129                    5.00      292.00     143.000                2.03   \n",
       "130                    5.00      292.00     143.000                2.03   \n",
       "131                    5.00      292.00     143.000                2.03   \n",
       "132                    5.00      292.00     143.000                2.03   \n",
       "133                    2.00      204.00      66.000                3.10   \n",
       "134                    4.00      204.00      66.000                3.10   \n",
       "135                    6.00      204.00      66.000                3.10   \n",
       "136                    7.00      204.00      66.000                3.10   \n",
       "137                    8.00      204.00      66.000                3.10   \n",
       "138                   10.00      204.00      66.000                3.10   \n",
       "139                    2.00      110.00      44.000                2.50   \n",
       "140                    4.00      110.00      44.000                2.50   \n",
       "141                    6.00      110.00      44.000                2.50   \n",
       "142                    8.00      110.00      44.000                2.50   \n",
       "143                    9.00      110.00      44.000                2.50   \n",
       "144                   10.00      110.00      44.000                2.50   \n",
       "145                   12.00      110.00      44.000                2.50   \n",
       "146                    2.00      204.00      66.000                3.10   \n",
       "147                    4.00      204.00      66.000                3.10   \n",
       "148                    6.00      204.00      66.000                3.10   \n",
       "149                    7.00      204.00      66.000                3.10   \n",
       "150                    8.00      204.00      66.000                3.10   \n",
       "151                   10.00      204.00      66.000                3.10   \n",
       "\n",
       "     hole_mobility  solution_treatment  substrate_pretreatment  post_process  \\\n",
       "0         0.110000                   0                       0             1   \n",
       "1         0.290000                   0                       1             1   \n",
       "2         0.230000                   0                       1             1   \n",
       "3         0.730000                   0                       1             1   \n",
       "4         1.860000                   0                       1             1   \n",
       "5         0.210000                   0                       1             1   \n",
       "6         0.340000                   0                       1             1   \n",
       "7         0.240000                   0                       1             1   \n",
       "8         1.970000                   0                       1             1   \n",
       "9         0.690000                   0                       1             1   \n",
       "10        2.040000                   0                       1             1   \n",
       "11        0.810000                   0                       0             1   \n",
       "12        1.530000                   0                       0             1   \n",
       "13        0.900000                   0                       0             1   \n",
       "14        1.100000                   0                       0             1   \n",
       "15        1.250000                   0                       0             1   \n",
       "16        1.300000                   0                       0             1   \n",
       "17        1.250000                   0                       0             1   \n",
       "18        0.900000                   0                       0             1   \n",
       "19        0.600000                   0                       0             1   \n",
       "20        6.850000                   0                       1             1   \n",
       "21        7.250000                   0                       1             1   \n",
       "22        5.750000                   0                       1             1   \n",
       "23        7.950000                   0                       1             1   \n",
       "24        7.850000                   0                       1             1   \n",
       "25        0.150000                   0                       1             1   \n",
       "26        2.100000                   0                       1             1   \n",
       "27        9.000000                   0                       1             1   \n",
       "28        4.400000                   0                       1             1   \n",
       "29        2.650000                   0                       1             1   \n",
       "30        3.500000                   0                       1             1   \n",
       "31        1.160000                   0                       1             1   \n",
       "32        3.390000                   0                       1             1   \n",
       "33        5.800000                   0                       1             1   \n",
       "34        6.760000                   0                       1             1   \n",
       "35        2.570000                   0                       1             1   \n",
       "36        6.000000                   0                       1             1   \n",
       "37        5.000000                   0                       1             1   \n",
       "38        4.000000                   0                       1             1   \n",
       "39        0.048400                   0                       1             1   \n",
       "40        0.181200                   0                       1             1   \n",
       "41        0.090100                   0                       1             1   \n",
       "42        0.064100                   0                       1             1   \n",
       "43        0.200000                   0                       0             1   \n",
       "44        0.090000                   0                       1             1   \n",
       "45        0.140000                   1                       1             1   \n",
       "46        0.000150                   0                       1             1   \n",
       "47        0.020000                   0                       1             1   \n",
       "48        0.060000                   0                       1             1   \n",
       "49        0.150000                   0                       1             1   \n",
       "50        0.450000                   0                       1             1   \n",
       "51        0.000450                   0                       1             1   \n",
       "52        0.450000                   0                       1             1   \n",
       "53        0.200000                   0                       1             1   \n",
       "54        0.800000                   0                       1             1   \n",
       "55        0.500000                   0                       1             1   \n",
       "56        0.100000                   0                       1             1   \n",
       "57        0.170000                   0                       1             1   \n",
       "58        0.150000                   0                       1             1   \n",
       "59        0.280000                   0                       1             1   \n",
       "60        0.200000                   0                       1             1   \n",
       "61        0.290000                   0                       1             1   \n",
       "62        0.325000                   0                       1             1   \n",
       "63        0.360000                   0                       1             1   \n",
       "64        0.400000                   0                       1             1   \n",
       "65        0.180000                   0                       1             1   \n",
       "66        0.280000                   0                       1             1   \n",
       "67        0.230000                   0                       1             1   \n",
       "68        0.315000                   0                       1             1   \n",
       "69        0.180000                   0                       1             1   \n",
       "70        0.280000                   0                       1             1   \n",
       "71        0.300000                   0                       1             1   \n",
       "72        0.280000                   0                       1             1   \n",
       "73        0.275000                   0                       1             1   \n",
       "74        0.080000                   0                       1             1   \n",
       "75        0.125000                   0                       1             1   \n",
       "76        0.080000                   0                       1             1   \n",
       "77        0.080000                   0                       1             1   \n",
       "78        0.080000                   0                       1             1   \n",
       "79        0.130000                   0                       1             1   \n",
       "80        0.125000                   0                       1             1   \n",
       "81        0.050000                   0                       1             1   \n",
       "82        0.075000                   0                       1             1   \n",
       "83        0.095000                   0                       1             1   \n",
       "84        0.055000                   0                       1             1   \n",
       "85        0.325000                   0                       1             1   \n",
       "86        0.300000                   0                       1             1   \n",
       "87        0.350000                   0                       1             1   \n",
       "88        0.380000                   0                       1             1   \n",
       "89        0.425000                   0                       1             1   \n",
       "90        0.400000                   0                       1             1   \n",
       "91        0.300000                   0                       1             1   \n",
       "92        0.336000                   0                       1             1   \n",
       "93        0.401000                   0                       1             1   \n",
       "94        0.560000                   0                       1             1   \n",
       "95        0.442000                   0                       1             1   \n",
       "96        0.397000                   0                       1             1   \n",
       "97        0.800000                   0                       1             0   \n",
       "98        0.470000                   0                       1             1   \n",
       "99        1.230000                   0                       1             1   \n",
       "100       0.930000                   0                       1             1   \n",
       "101       1.360000                   0                       0             1   \n",
       "102       0.160000                   0                       1             0   \n",
       "103       0.320000                   0                       1             0   \n",
       "104       0.240000                   0                       1             0   \n",
       "105       0.270000                   0                       1             0   \n",
       "106       0.250000                   0                       1             0   \n",
       "107       0.200000                   0                       1             0   \n",
       "108       0.800000                   0                       1             1   \n",
       "109       0.042816                   0                       0             1   \n",
       "110       0.113931                   0                       0             1   \n",
       "111       0.061766                   0                       0             1   \n",
       "112       0.220000                   0                       0             1   \n",
       "113       0.105000                   0                       0             1   \n",
       "114       0.077000                   0                       0             1   \n",
       "115       0.052000                   0                       0             1   \n",
       "116       0.047000                   0                       0             1   \n",
       "117       0.104000                   0                       0             1   \n",
       "118       0.187177                   0                       1             1   \n",
       "119       0.190529                   0                       1             1   \n",
       "120       0.180283                   0                       1             1   \n",
       "121       0.159097                   0                       1             1   \n",
       "122       0.129223                   0                       1             1   \n",
       "123       0.143917                   0                       0             1   \n",
       "124       0.176246                   0                       0             1   \n",
       "125       0.174302                   0                       0             1   \n",
       "126       0.092925                   0                       0             1   \n",
       "127       0.074403                   0                       0             1   \n",
       "128       0.124165                   0                       1             1   \n",
       "129       0.370376                   0                       1             1   \n",
       "130       0.319754                   0                       1             1   \n",
       "131       0.257338                   0                       1             1   \n",
       "132       0.193502                   0                       1             1   \n",
       "133       0.012535                   0                       0             1   \n",
       "134       0.038333                   0                       0             1   \n",
       "135       0.101182                   0                       0             1   \n",
       "136       0.198601                   0                       0             1   \n",
       "137       0.163842                   0                       0             1   \n",
       "138       0.170187                   0                       0             1   \n",
       "139       0.000412                   0                       0             1   \n",
       "140       0.007139                   0                       0             1   \n",
       "141       0.037412                   0                       0             1   \n",
       "142       0.072672                   0                       0             1   \n",
       "143       0.063779                   0                       0             1   \n",
       "144       0.027642                   0                       0             1   \n",
       "145       0.046912                   0                       0             1   \n",
       "146       0.011943                   0                       1             1   \n",
       "147       0.042120                   0                       1             1   \n",
       "148       0.148004                   0                       1             1   \n",
       "149       0.111808                   0                       1             1   \n",
       "150       0.163626                   0                       1             1   \n",
       "151       0.141648                   0                       1             1   \n",
       "\n",
       "     channel_width  channel_length  film_deposition_type_spin  \\\n",
       "0             1500              80                          1   \n",
       "1             1500              80                          1   \n",
       "2             1500              80                          1   \n",
       "3             1500              80                          1   \n",
       "4             1500              80                          1   \n",
       "5             1500              80                          1   \n",
       "6             1500              80                          1   \n",
       "7             1500              80                          1   \n",
       "8             1500              80                          1   \n",
       "9             1500              80                          1   \n",
       "10            1500              80                          1   \n",
       "11            2000              50                          1   \n",
       "12            2000              50                          1   \n",
       "13            2000              50                          1   \n",
       "14            2000              50                          1   \n",
       "15            2000              50                          1   \n",
       "16            2000              50                          1   \n",
       "17            2000              50                          1   \n",
       "18            2000              50                          1   \n",
       "19            2000              50                          1   \n",
       "20            4000             100                          1   \n",
       "21            4000             125                          1   \n",
       "22            1400              30                          1   \n",
       "23            1400              40                          1   \n",
       "24            1400              50                          1   \n",
       "25            1400              50                          1   \n",
       "26            1400              50                          1   \n",
       "27            1400              50                          1   \n",
       "28            4000             125                          0   \n",
       "29            1400             125                          0   \n",
       "30            7000              70                          1   \n",
       "31            1500              80                          1   \n",
       "32            1500              80                          1   \n",
       "33            1500              80                          1   \n",
       "34            1500              80                          1   \n",
       "35            1500              80                          1   \n",
       "36            1500              80                          1   \n",
       "37            1500              80                          1   \n",
       "38            1500              80                          1   \n",
       "39            1000              30                          1   \n",
       "40            1000              30                          1   \n",
       "41            1000              30                          1   \n",
       "42            1000              30                          1   \n",
       "43            3000             120                          1   \n",
       "44            1000             100                          1   \n",
       "45            1000             100                          1   \n",
       "46             800              40                          0   \n",
       "47             800              40                          0   \n",
       "48             800              40                          0   \n",
       "49             800              40                          0   \n",
       "50             800              40                          0   \n",
       "51             800              40                          0   \n",
       "52             800              40                          0   \n",
       "53             800              40                          0   \n",
       "54             800              40                          0   \n",
       "55             800              40                          0   \n",
       "56            4500              70                          0   \n",
       "57            4500              70                          0   \n",
       "58            4500              70                          0   \n",
       "59            4500              70                          0   \n",
       "60            4500              70                          0   \n",
       "61            4500              70                          0   \n",
       "62            4500              70                          0   \n",
       "63            4500              70                          0   \n",
       "64            4500              70                          0   \n",
       "65            4500              70                          0   \n",
       "66            4500              70                          0   \n",
       "67            4500              70                          0   \n",
       "68            4500              70                          0   \n",
       "69            4500              70                          0   \n",
       "70            4500              70                          0   \n",
       "71            4500              70                          0   \n",
       "72            4500              70                          0   \n",
       "73            4500              70                          0   \n",
       "74            4500              70                          0   \n",
       "75            4500              70                          0   \n",
       "76            4500              70                          0   \n",
       "77            4500              70                          0   \n",
       "78            4500              70                          0   \n",
       "79            4500              70                          0   \n",
       "80            4500              70                          0   \n",
       "81            4500              70                          0   \n",
       "82            4500              70                          0   \n",
       "83            4500              70                          0   \n",
       "84            4500              70                          0   \n",
       "85            4500              70                          0   \n",
       "86            4500              70                          0   \n",
       "87            4500              70                          0   \n",
       "88            4500              70                          0   \n",
       "89            4500              70                          0   \n",
       "90            4500              70                          0   \n",
       "91            4500              70                          0   \n",
       "92            2000              50                          0   \n",
       "93            2000              50                          0   \n",
       "94            2000              50                          0   \n",
       "95            2000              50                          0   \n",
       "96            2000              50                          0   \n",
       "97            8800              80                          1   \n",
       "98           10000              20                          1   \n",
       "99           10000              20                          1   \n",
       "100          10000              20                          1   \n",
       "101          10000              20                          1   \n",
       "102           4400              52                          0   \n",
       "103           4400              52                          0   \n",
       "104           4400              52                          0   \n",
       "105           4400              52                          0   \n",
       "106           4400              52                          0   \n",
       "107           4400              52                          0   \n",
       "108           1500              80                          1   \n",
       "109           2000              50                          0   \n",
       "110           2000              50                          0   \n",
       "111           2000              50                          0   \n",
       "112           2000              50                          0   \n",
       "113           2000              50                          0   \n",
       "114           2000              50                          0   \n",
       "115           2000              50                          0   \n",
       "116           2000              50                          0   \n",
       "117           2000              50                          0   \n",
       "118           2000              50                          0   \n",
       "119           2000              50                          0   \n",
       "120           2000              50                          0   \n",
       "121           2000              50                          0   \n",
       "122           2000              50                          0   \n",
       "123           2000              50                          0   \n",
       "124           2000              50                          0   \n",
       "125           2000              50                          0   \n",
       "126           2000              50                          0   \n",
       "127           2000              50                          0   \n",
       "128           2000              50                          0   \n",
       "129           2000              50                          0   \n",
       "130           2000              50                          0   \n",
       "131           2000              50                          0   \n",
       "132           2000              50                          0   \n",
       "133           2000              50                          0   \n",
       "134           2000              50                          0   \n",
       "135           2000              50                          0   \n",
       "136           2000              50                          0   \n",
       "137           2000              50                          0   \n",
       "138           2000              50                          0   \n",
       "139           2000              50                          0   \n",
       "140           2000              50                          0   \n",
       "141           2000              50                          0   \n",
       "142           2000              50                          0   \n",
       "143           2000              50                          0   \n",
       "144           2000              50                          0   \n",
       "145           2000              50                          0   \n",
       "146           2000              50                          0   \n",
       "147           2000              50                          0   \n",
       "148           2000              50                          0   \n",
       "149           2000              50                          0   \n",
       "150           2000              50                          0   \n",
       "151           2000              50                          0   \n",
       "\n",
       "     dielectric_material_SiO2  electrode_configuration_BGBC  \\\n",
       "0                           1                             0   \n",
       "1                           1                             0   \n",
       "2                           1                             0   \n",
       "3                           1                             0   \n",
       "4                           1                             0   \n",
       "5                           1                             0   \n",
       "6                           1                             0   \n",
       "7                           1                             0   \n",
       "8                           1                             0   \n",
       "9                           1                             0   \n",
       "10                          1                             0   \n",
       "11                          1                             1   \n",
       "12                          1                             1   \n",
       "13                          1                             1   \n",
       "14                          1                             1   \n",
       "15                          1                             1   \n",
       "16                          1                             1   \n",
       "17                          1                             1   \n",
       "18                          1                             1   \n",
       "19                          1                             1   \n",
       "20                          1                             0   \n",
       "21                          1                             0   \n",
       "22                          1                             1   \n",
       "23                          1                             1   \n",
       "24                          1                             1   \n",
       "25                          1                             1   \n",
       "26                          1                             1   \n",
       "27                          1                             1   \n",
       "28                          1                             0   \n",
       "29                          1                             0   \n",
       "30                          0                             1   \n",
       "31                          1                             0   \n",
       "32                          1                             0   \n",
       "33                          1                             0   \n",
       "34                          1                             0   \n",
       "35                          1                             0   \n",
       "36                          1                             0   \n",
       "37                          1                             0   \n",
       "38                          1                             0   \n",
       "39                          1                             0   \n",
       "40                          1                             0   \n",
       "41                          1                             0   \n",
       "42                          1                             0   \n",
       "43                          0                             0   \n",
       "44                          1                             1   \n",
       "45                          1                             1   \n",
       "46                          1                             0   \n",
       "47                          1                             0   \n",
       "48                          1                             0   \n",
       "49                          1                             0   \n",
       "50                          1                             0   \n",
       "51                          1                             0   \n",
       "52                          1                             0   \n",
       "53                          1                             0   \n",
       "54                          1                             0   \n",
       "55                          1                             0   \n",
       "56                          1                             0   \n",
       "57                          1                             0   \n",
       "58                          1                             0   \n",
       "59                          1                             0   \n",
       "60                          1                             0   \n",
       "61                          1                             0   \n",
       "62                          1                             0   \n",
       "63                          1                             0   \n",
       "64                          1                             0   \n",
       "65                          1                             0   \n",
       "66                          1                             0   \n",
       "67                          1                             0   \n",
       "68                          1                             0   \n",
       "69                          1                             0   \n",
       "70                          1                             0   \n",
       "71                          1                             0   \n",
       "72                          1                             0   \n",
       "73                          1                             0   \n",
       "74                          0                             0   \n",
       "75                          0                             0   \n",
       "76                          0                             0   \n",
       "77                          0                             0   \n",
       "78                          0                             0   \n",
       "79                          0                             0   \n",
       "80                          0                             0   \n",
       "81                          0                             0   \n",
       "82                          0                             0   \n",
       "83                          0                             0   \n",
       "84                          0                             0   \n",
       "85                          0                             0   \n",
       "86                          0                             0   \n",
       "87                          0                             0   \n",
       "88                          0                             0   \n",
       "89                          0                             0   \n",
       "90                          0                             0   \n",
       "91                          0                             0   \n",
       "92                          1                             1   \n",
       "93                          1                             1   \n",
       "94                          1                             1   \n",
       "95                          1                             1   \n",
       "96                          1                             1   \n",
       "97                          1                             1   \n",
       "98                          0                             0   \n",
       "99                          0                             0   \n",
       "100                         0                             0   \n",
       "101                         0                             0   \n",
       "102                         1                             0   \n",
       "103                         1                             0   \n",
       "104                         1                             0   \n",
       "105                         1                             0   \n",
       "106                         1                             0   \n",
       "107                         1                             0   \n",
       "108                         1                             0   \n",
       "109                         1                             1   \n",
       "110                         1                             1   \n",
       "111                         1                             1   \n",
       "112                         1                             1   \n",
       "113                         1                             1   \n",
       "114                         1                             1   \n",
       "115                         1                             1   \n",
       "116                         1                             1   \n",
       "117                         1                             1   \n",
       "118                         1                             1   \n",
       "119                         1                             1   \n",
       "120                         1                             1   \n",
       "121                         1                             1   \n",
       "122                         1                             1   \n",
       "123                         1                             1   \n",
       "124                         1                             1   \n",
       "125                         1                             1   \n",
       "126                         1                             1   \n",
       "127                         1                             1   \n",
       "128                         1                             1   \n",
       "129                         1                             1   \n",
       "130                         1                             1   \n",
       "131                         1                             1   \n",
       "132                         1                             1   \n",
       "133                         1                             1   \n",
       "134                         1                             1   \n",
       "135                         1                             1   \n",
       "136                         1                             1   \n",
       "137                         1                             1   \n",
       "138                         1                             1   \n",
       "139                         1                             1   \n",
       "140                         1                             1   \n",
       "141                         1                             1   \n",
       "142                         1                             1   \n",
       "143                         1                             1   \n",
       "144                         1                             1   \n",
       "145                         1                             1   \n",
       "146                         1                             1   \n",
       "147                         1                             1   \n",
       "148                         1                             1   \n",
       "149                         1                             1   \n",
       "150                         1                             1   \n",
       "151                         1                             1   \n",
       "\n",
       "     electrode_configuration_BGTC  electrode_configuration_TGBC  \\\n",
       "0                               1                             0   \n",
       "1                               1                             0   \n",
       "2                               1                             0   \n",
       "3                               1                             0   \n",
       "4                               1                             0   \n",
       "5                               1                             0   \n",
       "6                               1                             0   \n",
       "7                               1                             0   \n",
       "8                               1                             0   \n",
       "9                               1                             0   \n",
       "10                              1                             0   \n",
       "11                              0                             0   \n",
       "12                              0                             0   \n",
       "13                              0                             0   \n",
       "14                              0                             0   \n",
       "15                              0                             0   \n",
       "16                              0                             0   \n",
       "17                              0                             0   \n",
       "18                              0                             0   \n",
       "19                              0                             0   \n",
       "20                              1                             0   \n",
       "21                              1                             0   \n",
       "22                              0                             0   \n",
       "23                              0                             0   \n",
       "24                              0                             0   \n",
       "25                              0                             0   \n",
       "26                              0                             0   \n",
       "27                              0                             0   \n",
       "28                              1                             0   \n",
       "29                              1                             0   \n",
       "30                              0                             0   \n",
       "31                              1                             0   \n",
       "32                              1                             0   \n",
       "33                              1                             0   \n",
       "34                              1                             0   \n",
       "35                              1                             0   \n",
       "36                              1                             0   \n",
       "37                              1                             0   \n",
       "38                              1                             0   \n",
       "39                              1                             0   \n",
       "40                              1                             0   \n",
       "41                              1                             0   \n",
       "42                              1                             0   \n",
       "43                              1                             0   \n",
       "44                              0                             0   \n",
       "45                              0                             0   \n",
       "46                              1                             0   \n",
       "47                              1                             0   \n",
       "48                              1                             0   \n",
       "49                              1                             0   \n",
       "50                              1                             0   \n",
       "51                              1                             0   \n",
       "52                              1                             0   \n",
       "53                              1                             0   \n",
       "54                              1                             0   \n",
       "55                              1                             0   \n",
       "56                              1                             0   \n",
       "57                              1                             0   \n",
       "58                              1                             0   \n",
       "59                              1                             0   \n",
       "60                              1                             0   \n",
       "61                              1                             0   \n",
       "62                              1                             0   \n",
       "63                              1                             0   \n",
       "64                              1                             0   \n",
       "65                              1                             0   \n",
       "66                              1                             0   \n",
       "67                              1                             0   \n",
       "68                              1                             0   \n",
       "69                              1                             0   \n",
       "70                              1                             0   \n",
       "71                              1                             0   \n",
       "72                              1                             0   \n",
       "73                              1                             0   \n",
       "74                              0                             1   \n",
       "75                              0                             1   \n",
       "76                              0                             1   \n",
       "77                              0                             1   \n",
       "78                              0                             1   \n",
       "79                              0                             1   \n",
       "80                              0                             1   \n",
       "81                              0                             1   \n",
       "82                              0                             1   \n",
       "83                              0                             1   \n",
       "84                              0                             1   \n",
       "85                              0                             1   \n",
       "86                              0                             1   \n",
       "87                              0                             1   \n",
       "88                              0                             1   \n",
       "89                              0                             1   \n",
       "90                              0                             1   \n",
       "91                              0                             1   \n",
       "92                              0                             0   \n",
       "93                              0                             0   \n",
       "94                              0                             0   \n",
       "95                              0                             0   \n",
       "96                              0                             0   \n",
       "97                              0                             0   \n",
       "98                              0                             1   \n",
       "99                              0                             1   \n",
       "100                             0                             1   \n",
       "101                             0                             1   \n",
       "102                             1                             0   \n",
       "103                             1                             0   \n",
       "104                             1                             0   \n",
       "105                             1                             0   \n",
       "106                             1                             0   \n",
       "107                             1                             0   \n",
       "108                             1                             0   \n",
       "109                             0                             0   \n",
       "110                             0                             0   \n",
       "111                             0                             0   \n",
       "112                             0                             0   \n",
       "113                             0                             0   \n",
       "114                             0                             0   \n",
       "115                             0                             0   \n",
       "116                             0                             0   \n",
       "117                             0                             0   \n",
       "118                             0                             0   \n",
       "119                             0                             0   \n",
       "120                             0                             0   \n",
       "121                             0                             0   \n",
       "122                             0                             0   \n",
       "123                             0                             0   \n",
       "124                             0                             0   \n",
       "125                             0                             0   \n",
       "126                             0                             0   \n",
       "127                             0                             0   \n",
       "128                             0                             0   \n",
       "129                             0                             0   \n",
       "130                             0                             0   \n",
       "131                             0                             0   \n",
       "132                             0                             0   \n",
       "133                             0                             0   \n",
       "134                             0                             0   \n",
       "135                             0                             0   \n",
       "136                             0                             0   \n",
       "137                             0                             0   \n",
       "138                             0                             0   \n",
       "139                             0                             0   \n",
       "140                             0                             0   \n",
       "141                             0                             0   \n",
       "142                             0                             0   \n",
       "143                             0                             0   \n",
       "144                             0                             0   \n",
       "145                             0                             0   \n",
       "146                             0                             0   \n",
       "147                             0                             0   \n",
       "148                             0                             0   \n",
       "149                             0                             0   \n",
       "150                             0                             0   \n",
       "151                             0                             0   \n",
       "\n",
       "     gate_material_Other  film_deposition_type_MGC  dielectric_material_other  \\\n",
       "0                      0                         0                          0   \n",
       "1                      0                         0                          0   \n",
       "2                      0                         0                          0   \n",
       "3                      0                         0                          0   \n",
       "4                      0                         0                          0   \n",
       "5                      0                         0                          0   \n",
       "6                      0                         0                          0   \n",
       "7                      0                         0                          0   \n",
       "8                      0                         0                          0   \n",
       "9                      0                         0                          0   \n",
       "10                     0                         0                          0   \n",
       "11                     0                         0                          0   \n",
       "12                     0                         0                          0   \n",
       "13                     0                         0                          0   \n",
       "14                     0                         0                          0   \n",
       "15                     0                         0                          0   \n",
       "16                     0                         0                          0   \n",
       "17                     0                         0                          0   \n",
       "18                     0                         0                          0   \n",
       "19                     0                         0                          0   \n",
       "20                     0                         0                          0   \n",
       "21                     0                         0                          0   \n",
       "22                     0                         0                          0   \n",
       "23                     0                         0                          0   \n",
       "24                     0                         0                          0   \n",
       "25                     0                         0                          0   \n",
       "26                     0                         0                          0   \n",
       "27                     0                         0                          0   \n",
       "28                     0                         1                          0   \n",
       "29                     0                         1                          0   \n",
       "30                     1                         0                          1   \n",
       "31                     0                         0                          0   \n",
       "32                     0                         0                          0   \n",
       "33                     0                         0                          0   \n",
       "34                     0                         0                          0   \n",
       "35                     0                         0                          0   \n",
       "36                     0                         0                          0   \n",
       "37                     0                         0                          0   \n",
       "38                     0                         0                          0   \n",
       "39                     0                         0                          0   \n",
       "40                     0                         0                          0   \n",
       "41                     0                         0                          0   \n",
       "42                     0                         0                          0   \n",
       "43                     0                         0                          1   \n",
       "44                     0                         0                          0   \n",
       "45                     0                         0                          0   \n",
       "46                     0                         1                          0   \n",
       "47                     0                         1                          0   \n",
       "48                     0                         1                          0   \n",
       "49                     0                         1                          0   \n",
       "50                     0                         1                          0   \n",
       "51                     0                         1                          0   \n",
       "52                     0                         1                          0   \n",
       "53                     0                         1                          0   \n",
       "54                     0                         1                          0   \n",
       "55                     0                         1                          0   \n",
       "56                     0                         1                          0   \n",
       "57                     0                         1                          0   \n",
       "58                     0                         1                          0   \n",
       "59                     0                         1                          0   \n",
       "60                     0                         1                          0   \n",
       "61                     0                         1                          0   \n",
       "62                     0                         1                          0   \n",
       "63                     0                         1                          0   \n",
       "64                     0                         1                          0   \n",
       "65                     0                         1                          0   \n",
       "66                     0                         1                          0   \n",
       "67                     0                         1                          0   \n",
       "68                     0                         1                          0   \n",
       "69                     0                         1                          0   \n",
       "70                     0                         1                          0   \n",
       "71                     0                         1                          0   \n",
       "72                     0                         1                          0   \n",
       "73                     0                         1                          0   \n",
       "74                     0                         1                          1   \n",
       "75                     0                         1                          1   \n",
       "76                     0                         1                          1   \n",
       "77                     0                         1                          1   \n",
       "78                     0                         1                          1   \n",
       "79                     0                         1                          1   \n",
       "80                     0                         1                          1   \n",
       "81                     0                         1                          1   \n",
       "82                     0                         1                          1   \n",
       "83                     0                         1                          1   \n",
       "84                     0                         1                          1   \n",
       "85                     0                         1                          1   \n",
       "86                     0                         1                          1   \n",
       "87                     0                         1                          1   \n",
       "88                     0                         1                          1   \n",
       "89                     0                         1                          1   \n",
       "90                     0                         1                          1   \n",
       "91                     0                         1                          1   \n",
       "92                     0                         1                          0   \n",
       "93                     0                         1                          0   \n",
       "94                     0                         1                          0   \n",
       "95                     0                         1                          0   \n",
       "96                     0                         1                          0   \n",
       "97                     0                         0                          0   \n",
       "98                     1                         0                          1   \n",
       "99                     1                         0                          1   \n",
       "100                    1                         0                          1   \n",
       "101                    1                         0                          1   \n",
       "102                    0                         1                          0   \n",
       "103                    0                         1                          0   \n",
       "104                    0                         1                          0   \n",
       "105                    0                         1                          0   \n",
       "106                    0                         1                          0   \n",
       "107                    0                         1                          0   \n",
       "108                    0                         0                          0   \n",
       "109                    0                         1                          0   \n",
       "110                    0                         1                          0   \n",
       "111                    0                         1                          0   \n",
       "112                    0                         1                          0   \n",
       "113                    0                         1                          0   \n",
       "114                    0                         1                          0   \n",
       "115                    0                         1                          0   \n",
       "116                    0                         1                          0   \n",
       "117                    0                         1                          0   \n",
       "118                    0                         1                          0   \n",
       "119                    0                         1                          0   \n",
       "120                    0                         1                          0   \n",
       "121                    0                         1                          0   \n",
       "122                    0                         1                          0   \n",
       "123                    0                         1                          0   \n",
       "124                    0                         1                          0   \n",
       "125                    0                         1                          0   \n",
       "126                    0                         1                          0   \n",
       "127                    0                         1                          0   \n",
       "128                    0                         1                          0   \n",
       "129                    0                         1                          0   \n",
       "130                    0                         1                          0   \n",
       "131                    0                         1                          0   \n",
       "132                    0                         1                          0   \n",
       "133                    0                         1                          0   \n",
       "134                    0                         1                          0   \n",
       "135                    0                         1                          0   \n",
       "136                    0                         1                          0   \n",
       "137                    0                         1                          0   \n",
       "138                    0                         1                          0   \n",
       "139                    0                         1                          0   \n",
       "140                    0                         1                          0   \n",
       "141                    0                         1                          0   \n",
       "142                    0                         1                          0   \n",
       "143                    0                         1                          0   \n",
       "144                    0                         1                          0   \n",
       "145                    0                         1                          0   \n",
       "146                    0                         1                          0   \n",
       "147                    0                         1                          0   \n",
       "148                    0                         1                          0   \n",
       "149                    0                         1                          0   \n",
       "150                    0                         1                          0   \n",
       "151                    0                         1                          0   \n",
       "\n",
       "     solvent_boiling_point  delta_d  delta_p  delta_h  \\\n",
       "0                    132.0     9.29     2.10     0.98   \n",
       "1                    132.0     9.29     2.10     0.98   \n",
       "2                    132.0     9.29     2.10     0.98   \n",
       "3                    132.0     9.29     2.10     0.98   \n",
       "4                    132.0     9.29     2.10     0.98   \n",
       "5                    132.0     9.29     2.10     0.98   \n",
       "6                    132.0     9.29     2.10     0.98   \n",
       "7                    132.0     9.29     2.10     0.98   \n",
       "8                    132.0     9.29     2.10     0.98   \n",
       "9                    132.0     9.29     2.10     0.98   \n",
       "10                   132.0     9.29     2.10     0.98   \n",
       "11                    62.0    17.80     3.10     5.70   \n",
       "12                    62.0    17.80     3.10     5.70   \n",
       "13                    62.0    17.80     3.10     5.70   \n",
       "14                    62.0    17.80     3.10     5.70   \n",
       "15                    62.0    17.80     3.10     5.70   \n",
       "16                    62.0    17.80     3.10     5.70   \n",
       "17                    62.0    17.80     3.10     5.70   \n",
       "18                    62.0    17.80     3.10     5.70   \n",
       "19                    62.0    17.80     3.10     5.70   \n",
       "20                   132.0     9.29     2.10     0.98   \n",
       "21                   132.0     9.29     2.10     0.98   \n",
       "22                   132.0     9.29     2.10     0.98   \n",
       "23                   132.0     9.29     2.10     0.98   \n",
       "24                   132.0     9.29     2.10     0.98   \n",
       "25                   132.0     9.29     2.10     0.98   \n",
       "26                   132.0     9.29     2.10     0.98   \n",
       "27                   132.0     9.29     2.10     0.98   \n",
       "28                   132.0     9.29     2.10     0.98   \n",
       "29                   132.0     9.29     2.10     0.98   \n",
       "30                   132.0     9.29     2.10     0.98   \n",
       "31                   180.1     9.39     3.08     1.61   \n",
       "32                   180.1     9.39     3.08     1.61   \n",
       "33                   180.1     9.39     3.08     1.61   \n",
       "34                   180.1     9.39     3.08     1.61   \n",
       "35                   180.1     9.39     3.08     1.61   \n",
       "36                   180.1     9.39     3.08     1.61   \n",
       "37                   180.1     9.39     3.08     1.61   \n",
       "38                   180.1     9.39     3.08     1.61   \n",
       "39                   180.1     9.39     3.08     1.61   \n",
       "40                   180.1     9.39     3.08     1.61   \n",
       "41                   180.1     9.39     3.08     1.61   \n",
       "42                   180.1     9.39     3.08     1.61   \n",
       "43                    62.0    17.80     3.10     5.70   \n",
       "44                    62.0    17.80     3.10     5.70   \n",
       "45                    62.0    17.80     3.10     5.70   \n",
       "46                    62.0    17.80     3.10     5.70   \n",
       "47                    62.0    17.80     3.10     5.70   \n",
       "48                    62.0    17.80     3.10     5.70   \n",
       "49                    62.0    17.80     3.10     5.70   \n",
       "50                    62.0    17.80     3.10     5.70   \n",
       "51                    62.0    17.80     3.10     5.70   \n",
       "52                    62.0    17.80     3.10     5.70   \n",
       "53                    62.0    17.80     3.10     5.70   \n",
       "54                    62.0    17.80     3.10     5.70   \n",
       "55                    62.0    17.80     3.10     5.70   \n",
       "56                    62.0    17.80     3.10     5.70   \n",
       "57                    62.0    17.80     3.10     5.70   \n",
       "58                    62.0    17.80     3.10     5.70   \n",
       "59                    62.0    17.80     3.10     5.70   \n",
       "60                    62.0    17.80     3.10     5.70   \n",
       "61                    62.0    17.80     3.10     5.70   \n",
       "62                    62.0    17.80     3.10     5.70   \n",
       "63                    62.0    17.80     3.10     5.70   \n",
       "64                    62.0    17.80     3.10     5.70   \n",
       "65                    62.0    17.80     3.10     5.70   \n",
       "66                    62.0    17.80     3.10     5.70   \n",
       "67                    62.0    17.80     3.10     5.70   \n",
       "68                    62.0    17.80     3.10     5.70   \n",
       "69                    62.0    17.80     3.10     5.70   \n",
       "70                    62.0    17.80     3.10     5.70   \n",
       "71                    62.0    17.80     3.10     5.70   \n",
       "72                    62.0    17.80     3.10     5.70   \n",
       "73                    62.0    17.80     3.10     5.70   \n",
       "74                    62.0    17.80     3.10     5.70   \n",
       "75                    62.0    17.80     3.10     5.70   \n",
       "76                    62.0    17.80     3.10     5.70   \n",
       "77                    62.0    17.80     3.10     5.70   \n",
       "78                    62.0    17.80     3.10     5.70   \n",
       "79                    62.0    17.80     3.10     5.70   \n",
       "80                    62.0    17.80     3.10     5.70   \n",
       "81                    62.0    17.80     3.10     5.70   \n",
       "82                    62.0    17.80     3.10     5.70   \n",
       "83                    62.0    17.80     3.10     5.70   \n",
       "84                    62.0    17.80     3.10     5.70   \n",
       "85                    62.0    17.80     3.10     5.70   \n",
       "86                    62.0    17.80     3.10     5.70   \n",
       "87                    62.0    17.80     3.10     5.70   \n",
       "88                    62.0    17.80     3.10     5.70   \n",
       "89                    62.0    17.80     3.10     5.70   \n",
       "90                    62.0    17.80     3.10     5.70   \n",
       "91                    62.0    17.80     3.10     5.70   \n",
       "92                   132.0     9.29     2.10     0.98   \n",
       "93                   132.0     9.29     2.10     0.98   \n",
       "94                   132.0     9.29     2.10     0.98   \n",
       "95                   132.0     9.29     2.10     0.98   \n",
       "96                   132.0     9.29     2.10     0.98   \n",
       "97                   180.1     9.39     3.08     1.61   \n",
       "98                   180.1     9.39     3.08     1.61   \n",
       "99                   180.1     9.39     3.08     1.61   \n",
       "100                  180.1     9.39     3.08     1.61   \n",
       "101                  180.1     9.39     3.08     1.61   \n",
       "102                   62.0    17.80     3.10     5.70   \n",
       "103                   62.0    17.80     3.10     5.70   \n",
       "104                   62.0    17.80     3.10     5.70   \n",
       "105                   62.0    17.80     3.10     5.70   \n",
       "106                   62.0    17.80     3.10     5.70   \n",
       "107                   62.0    17.80     3.10     5.70   \n",
       "108                  180.1     9.39     3.08     1.61   \n",
       "109                  132.0     9.29     2.10     0.98   \n",
       "110                  132.0     9.29     2.10     0.98   \n",
       "111                  132.0     9.29     2.10     0.98   \n",
       "112                  132.0     9.29     2.10     0.98   \n",
       "113                  132.0     9.29     2.10     0.98   \n",
       "114                  132.0     9.29     2.10     0.98   \n",
       "115                  132.0     9.29     2.10     0.98   \n",
       "116                  132.0     9.29     2.10     0.98   \n",
       "117                  132.0     9.29     2.10     0.98   \n",
       "118                  132.0     9.29     2.10     0.98   \n",
       "119                  132.0     9.29     2.10     0.98   \n",
       "120                  132.0     9.29     2.10     0.98   \n",
       "121                  132.0     9.29     2.10     0.98   \n",
       "122                  132.0     9.29     2.10     0.98   \n",
       "123                  132.0     9.29     2.10     0.98   \n",
       "124                  132.0     9.29     2.10     0.98   \n",
       "125                  132.0     9.29     2.10     0.98   \n",
       "126                  132.0     9.29     2.10     0.98   \n",
       "127                  132.0     9.29     2.10     0.98   \n",
       "128                  132.0     9.29     2.10     0.98   \n",
       "129                  132.0     9.29     2.10     0.98   \n",
       "130                  132.0     9.29     2.10     0.98   \n",
       "131                  132.0     9.29     2.10     0.98   \n",
       "132                  132.0     9.29     2.10     0.98   \n",
       "133                  132.0     9.29     2.10     0.98   \n",
       "134                  132.0     9.29     2.10     0.98   \n",
       "135                  132.0     9.29     2.10     0.98   \n",
       "136                  132.0     9.29     2.10     0.98   \n",
       "137                  132.0     9.29     2.10     0.98   \n",
       "138                  132.0     9.29     2.10     0.98   \n",
       "139                  132.0     9.29     2.10     0.98   \n",
       "140                  132.0     9.29     2.10     0.98   \n",
       "141                  132.0     9.29     2.10     0.98   \n",
       "142                  132.0     9.29     2.10     0.98   \n",
       "143                  132.0     9.29     2.10     0.98   \n",
       "144                  132.0     9.29     2.10     0.98   \n",
       "145                  132.0     9.29     2.10     0.98   \n",
       "146                  132.0     9.29     2.10     0.98   \n",
       "147                  132.0     9.29     2.10     0.98   \n",
       "148                  132.0     9.29     2.10     0.98   \n",
       "149                  132.0     9.29     2.10     0.98   \n",
       "150                  132.0     9.29     2.10     0.98   \n",
       "151                  132.0     9.29     2.10     0.98   \n",
       "\n",
       "     blend_conjugated_polymer  insulating_polymer  substrate_pretreat_sam  \\\n",
       "0                           0                   0                       0   \n",
       "1                           0                   0                       1   \n",
       "2                           0                   0                       1   \n",
       "3                           0                   0                       1   \n",
       "4                           0                   0                       1   \n",
       "5                           0                   0                       2   \n",
       "6                           0                   0                       2   \n",
       "7                           0                   0                       2   \n",
       "8                           0                   0                       2   \n",
       "9                           0                   0                       2   \n",
       "10                          0                   0                       2   \n",
       "11                          0                   0                       0   \n",
       "12                          0                   1                       0   \n",
       "13                          0                   1                       0   \n",
       "14                          0                   1                       0   \n",
       "15                          0                   1                       0   \n",
       "16                          0                   1                       0   \n",
       "17                          0                   1                       0   \n",
       "18                          0                   1                       0   \n",
       "19                          0                   1                       0   \n",
       "20                          0                   0                       1   \n",
       "21                          0                   0                       1   \n",
       "22                          0                   0                       1   \n",
       "23                          0                   0                       1   \n",
       "24                          0                   0                       1   \n",
       "25                          0                   0                       1   \n",
       "26                          0                   0                       1   \n",
       "27                          0                   0                       1   \n",
       "28                          0                   0                       1   \n",
       "29                          0                   0                       1   \n",
       "30                          0                   0                       1   \n",
       "31                          0                   0                       1   \n",
       "32                          0                   1                       1   \n",
       "33                          0                   1                       1   \n",
       "34                          0                   1                       1   \n",
       "35                          0                   1                       1   \n",
       "36                          0                   1                       1   \n",
       "37                          0                   1                       1   \n",
       "38                          0                   1                       1   \n",
       "39                          0                   0                       1   \n",
       "40                          0                   0                       1   \n",
       "41                          0                   0                       1   \n",
       "42                          0                   0                       1   \n",
       "43                          0                   0                       0   \n",
       "44                          0                   0                       1   \n",
       "45                          0                   0                       1   \n",
       "46                          0                   0                       1   \n",
       "47                          0                   0                       1   \n",
       "48                          0                   0                       1   \n",
       "49                          0                   0                       1   \n",
       "50                          0                   0                       1   \n",
       "51                          0                   0                       1   \n",
       "52                          0                   0                       1   \n",
       "53                          0                   0                       1   \n",
       "54                          0                   0                       1   \n",
       "55                          0                   0                       1   \n",
       "56                          0                   0                       1   \n",
       "57                          0                   0                       1   \n",
       "58                          0                   0                       1   \n",
       "59                          0                   0                       1   \n",
       "60                          0                   0                       1   \n",
       "61                          0                   0                       1   \n",
       "62                          0                   0                       1   \n",
       "63                          0                   0                       1   \n",
       "64                          0                   0                       1   \n",
       "65                          0                   0                       1   \n",
       "66                          0                   0                       1   \n",
       "67                          0                   0                       1   \n",
       "68                          0                   0                       1   \n",
       "69                          0                   0                       1   \n",
       "70                          0                   0                       1   \n",
       "71                          0                   0                       1   \n",
       "72                          0                   0                       1   \n",
       "73                          0                   0                       1   \n",
       "74                          0                   0                       1   \n",
       "75                          0                   0                       1   \n",
       "76                          0                   0                       1   \n",
       "77                          0                   0                       1   \n",
       "78                          0                   0                       1   \n",
       "79                          0                   0                       1   \n",
       "80                          0                   0                       1   \n",
       "81                          0                   0                       1   \n",
       "82                          0                   0                       1   \n",
       "83                          0                   0                       1   \n",
       "84                          0                   0                       1   \n",
       "85                          0                   0                       1   \n",
       "86                          0                   0                       1   \n",
       "87                          0                   0                       1   \n",
       "88                          0                   0                       1   \n",
       "89                          0                   0                       1   \n",
       "90                          0                   0                       1   \n",
       "91                          0                   0                       1   \n",
       "92                          0                   0                       1   \n",
       "93                          0                   0                       1   \n",
       "94                          0                   0                       1   \n",
       "95                          0                   0                       1   \n",
       "96                          0                   0                       1   \n",
       "97                          0                   0                       1   \n",
       "98                          0                   0                       0   \n",
       "99                          0                   0                       0   \n",
       "100                         0                   0                       0   \n",
       "101                         0                   0                       0   \n",
       "102                         0                   0                       1   \n",
       "103                         0                   0                       1   \n",
       "104                         0                   0                       1   \n",
       "105                         0                   0                       1   \n",
       "106                         0                   0                       1   \n",
       "107                         0                   0                       1   \n",
       "108                         0                   0                       1   \n",
       "109                         0                   0                       0   \n",
       "110                         0                   0                       0   \n",
       "111                         0                   0                       0   \n",
       "112                         0                   0                       0   \n",
       "113                         0                   0                       0   \n",
       "114                         0                   0                       0   \n",
       "115                         0                   0                       0   \n",
       "116                         0                   0                       0   \n",
       "117                         0                   0                       0   \n",
       "118                         0                   1                       1   \n",
       "119                         0                   1                       1   \n",
       "120                         0                   1                       1   \n",
       "121                         0                   1                       1   \n",
       "122                         0                   0                       1   \n",
       "123                         0                   1                       0   \n",
       "124                         0                   1                       0   \n",
       "125                         0                   1                       0   \n",
       "126                         0                   1                       0   \n",
       "127                         0                   0                       0   \n",
       "128                         0                   1                       1   \n",
       "129                         0                   1                       1   \n",
       "130                         0                   1                       1   \n",
       "131                         0                   1                       1   \n",
       "132                         0                   0                       1   \n",
       "133                         0                   0                       0   \n",
       "134                         0                   0                       0   \n",
       "135                         0                   0                       0   \n",
       "136                         0                   0                       0   \n",
       "137                         0                   0                       0   \n",
       "138                         0                   0                       0   \n",
       "139                         0                   0                       0   \n",
       "140                         0                   0                       0   \n",
       "141                         0                   0                       0   \n",
       "142                         0                   0                       0   \n",
       "143                         0                   0                       0   \n",
       "144                         0                   0                       0   \n",
       "145                         0                   0                       0   \n",
       "146                         0                   0                       1   \n",
       "147                         0                   0                       1   \n",
       "148                         0                   0                       1   \n",
       "149                         0                   0                       1   \n",
       "150                         0                   0                       1   \n",
       "151                         0                   0                       1   \n",
       "\n",
       "     substrate_pretreat_plasma  substrate_pretreat_uv_ozone  \\\n",
       "0                            0                            0   \n",
       "1                            0                            0   \n",
       "2                            0                            0   \n",
       "3                            0                            0   \n",
       "4                            0                            0   \n",
       "5                            0                            0   \n",
       "6                            0                            0   \n",
       "7                            0                            0   \n",
       "8                            0                            0   \n",
       "9                            0                            0   \n",
       "10                           0                            0   \n",
       "11                           0                            0   \n",
       "12                           0                            0   \n",
       "13                           0                            0   \n",
       "14                           0                            0   \n",
       "15                           0                            0   \n",
       "16                           0                            0   \n",
       "17                           0                            0   \n",
       "18                           0                            0   \n",
       "19                           0                            0   \n",
       "20                           0                            0   \n",
       "21                           0                            0   \n",
       "22                           0                            0   \n",
       "23                           0                            0   \n",
       "24                           0                            0   \n",
       "25                           0                            0   \n",
       "26                           0                            0   \n",
       "27                           0                            0   \n",
       "28                           0                            0   \n",
       "29                           0                            0   \n",
       "30                           0                            0   \n",
       "31                           0                            0   \n",
       "32                           0                            0   \n",
       "33                           0                            0   \n",
       "34                           0                            0   \n",
       "35                           0                            0   \n",
       "36                           0                            0   \n",
       "37                           0                            0   \n",
       "38                           0                            0   \n",
       "39                           0                            0   \n",
       "40                           0                            0   \n",
       "41                           0                            0   \n",
       "42                           0                            0   \n",
       "43                           0                            0   \n",
       "44                           0                            0   \n",
       "45                           0                            0   \n",
       "46                           0                            0   \n",
       "47                           0                            0   \n",
       "48                           0                            0   \n",
       "49                           0                            0   \n",
       "50                           0                            0   \n",
       "51                           0                            0   \n",
       "52                           0                            0   \n",
       "53                           0                            0   \n",
       "54                           0                            0   \n",
       "55                           0                            0   \n",
       "56                           0                            0   \n",
       "57                           0                            0   \n",
       "58                           0                            0   \n",
       "59                           0                            0   \n",
       "60                           0                            0   \n",
       "61                           0                            0   \n",
       "62                           0                            0   \n",
       "63                           0                            0   \n",
       "64                           0                            0   \n",
       "65                           0                            0   \n",
       "66                           0                            0   \n",
       "67                           0                            0   \n",
       "68                           0                            0   \n",
       "69                           0                            0   \n",
       "70                           0                            0   \n",
       "71                           0                            0   \n",
       "72                           0                            0   \n",
       "73                           0                            0   \n",
       "74                           0                            0   \n",
       "75                           0                            0   \n",
       "76                           0                            0   \n",
       "77                           0                            0   \n",
       "78                           0                            0   \n",
       "79                           0                            0   \n",
       "80                           0                            0   \n",
       "81                           0                            0   \n",
       "82                           0                            0   \n",
       "83                           0                            0   \n",
       "84                           0                            0   \n",
       "85                           0                            0   \n",
       "86                           0                            0   \n",
       "87                           0                            0   \n",
       "88                           0                            0   \n",
       "89                           0                            0   \n",
       "90                           0                            0   \n",
       "91                           0                            0   \n",
       "92                           0                            0   \n",
       "93                           0                            0   \n",
       "94                           0                            0   \n",
       "95                           0                            0   \n",
       "96                           0                            0   \n",
       "97                           0                            0   \n",
       "98                           0                            0   \n",
       "99                           0                            0   \n",
       "100                          0                            0   \n",
       "101                          0                            0   \n",
       "102                          0                            0   \n",
       "103                          0                            0   \n",
       "104                          0                            0   \n",
       "105                          0                            0   \n",
       "106                          0                            0   \n",
       "107                          0                            0   \n",
       "108                          0                            0   \n",
       "109                          0                            0   \n",
       "110                          0                            0   \n",
       "111                          0                            0   \n",
       "112                          0                            0   \n",
       "113                          0                            0   \n",
       "114                          0                            0   \n",
       "115                          0                            0   \n",
       "116                          0                            0   \n",
       "117                          0                            0   \n",
       "118                          0                            0   \n",
       "119                          0                            0   \n",
       "120                          0                            0   \n",
       "121                          0                            0   \n",
       "122                          0                            0   \n",
       "123                          0                            0   \n",
       "124                          0                            0   \n",
       "125                          0                            0   \n",
       "126                          0                            0   \n",
       "127                          0                            0   \n",
       "128                          0                            0   \n",
       "129                          0                            0   \n",
       "130                          0                            0   \n",
       "131                          0                            0   \n",
       "132                          0                            0   \n",
       "133                          0                            0   \n",
       "134                          0                            0   \n",
       "135                          0                            0   \n",
       "136                          0                            0   \n",
       "137                          0                            0   \n",
       "138                          0                            0   \n",
       "139                          0                            0   \n",
       "140                          0                            0   \n",
       "141                          0                            0   \n",
       "142                          0                            0   \n",
       "143                          0                            0   \n",
       "144                          0                            0   \n",
       "145                          0                            0   \n",
       "146                          0                            0   \n",
       "147                          0                            0   \n",
       "148                          0                            0   \n",
       "149                          0                            0   \n",
       "150                          0                            0   \n",
       "151                          0                            0   \n",
       "\n",
       "     solution_treatment_poor_solvent  solution_treatment_aging  \\\n",
       "0                                  0                         0   \n",
       "1                                  0                         0   \n",
       "2                                  0                         0   \n",
       "3                                  0                         0   \n",
       "4                                  0                         0   \n",
       "5                                  0                         0   \n",
       "6                                  0                         0   \n",
       "7                                  0                         0   \n",
       "8                                  0                         0   \n",
       "9                                  0                         0   \n",
       "10                                 0                         0   \n",
       "11                                 0                         0   \n",
       "12                                 0                         0   \n",
       "13                                 0                         0   \n",
       "14                                 0                         0   \n",
       "15                                 0                         0   \n",
       "16                                 0                         0   \n",
       "17                                 0                         0   \n",
       "18                                 0                         0   \n",
       "19                                 0                         0   \n",
       "20                                 0                         0   \n",
       "21                                 0                         0   \n",
       "22                                 0                         0   \n",
       "23                                 0                         0   \n",
       "24                                 0                         0   \n",
       "25                                 0                         0   \n",
       "26                                 0                         0   \n",
       "27                                 0                         0   \n",
       "28                                 0                         0   \n",
       "29                                 0                         0   \n",
       "30                                 0                         0   \n",
       "31                                 0                         0   \n",
       "32                                 0                         0   \n",
       "33                                 0                         0   \n",
       "34                                 0                         0   \n",
       "35                                 0                         0   \n",
       "36                                 0                         0   \n",
       "37                                 0                         0   \n",
       "38                                 0                         0   \n",
       "39                                 0                         0   \n",
       "40                                 0                         0   \n",
       "41                                 0                         0   \n",
       "42                                 0                         0   \n",
       "43                                 0                         0   \n",
       "44                                 0                         0   \n",
       "45                                 1                         0   \n",
       "46                                 0                         0   \n",
       "47                                 0                         0   \n",
       "48                                 0                         0   \n",
       "49                                 0                         0   \n",
       "50                                 0                         0   \n",
       "51                                 0                         0   \n",
       "52                                 0                         0   \n",
       "53                                 0                         0   \n",
       "54                                 0                         0   \n",
       "55                                 0                         0   \n",
       "56                                 0                         0   \n",
       "57                                 0                         0   \n",
       "58                                 0                         0   \n",
       "59                                 0                         0   \n",
       "60                                 0                         0   \n",
       "61                                 0                         0   \n",
       "62                                 0                         0   \n",
       "63                                 0                         0   \n",
       "64                                 0                         0   \n",
       "65                                 0                         0   \n",
       "66                                 0                         0   \n",
       "67                                 0                         0   \n",
       "68                                 0                         0   \n",
       "69                                 0                         0   \n",
       "70                                 0                         0   \n",
       "71                                 0                         0   \n",
       "72                                 0                         0   \n",
       "73                                 0                         0   \n",
       "74                                 0                         0   \n",
       "75                                 0                         0   \n",
       "76                                 0                         0   \n",
       "77                                 0                         0   \n",
       "78                                 0                         0   \n",
       "79                                 0                         0   \n",
       "80                                 0                         0   \n",
       "81                                 0                         0   \n",
       "82                                 0                         0   \n",
       "83                                 0                         0   \n",
       "84                                 0                         0   \n",
       "85                                 0                         0   \n",
       "86                                 0                         0   \n",
       "87                                 0                         0   \n",
       "88                                 0                         0   \n",
       "89                                 0                         0   \n",
       "90                                 0                         0   \n",
       "91                                 0                         0   \n",
       "92                                 0                         0   \n",
       "93                                 0                         0   \n",
       "94                                 0                         0   \n",
       "95                                 0                         0   \n",
       "96                                 0                         0   \n",
       "97                                 0                         0   \n",
       "98                                 0                         0   \n",
       "99                                 0                         0   \n",
       "100                                0                         0   \n",
       "101                                0                         0   \n",
       "102                                0                         0   \n",
       "103                                0                         0   \n",
       "104                                0                         0   \n",
       "105                                0                         0   \n",
       "106                                0                         0   \n",
       "107                                0                         0   \n",
       "108                                0                         0   \n",
       "109                                0                         0   \n",
       "110                                0                         0   \n",
       "111                                0                         0   \n",
       "112                                0                         0   \n",
       "113                                0                         0   \n",
       "114                                0                         0   \n",
       "115                                0                         0   \n",
       "116                                0                         0   \n",
       "117                                0                         0   \n",
       "118                                0                         0   \n",
       "119                                0                         0   \n",
       "120                                0                         0   \n",
       "121                                0                         0   \n",
       "122                                0                         0   \n",
       "123                                0                         0   \n",
       "124                                0                         0   \n",
       "125                                0                         0   \n",
       "126                                0                         0   \n",
       "127                                0                         0   \n",
       "128                                0                         0   \n",
       "129                                0                         0   \n",
       "130                                0                         0   \n",
       "131                                0                         0   \n",
       "132                                0                         0   \n",
       "133                                0                         0   \n",
       "134                                0                         0   \n",
       "135                                0                         0   \n",
       "136                                0                         0   \n",
       "137                                0                         0   \n",
       "138                                0                         0   \n",
       "139                                0                         0   \n",
       "140                                0                         0   \n",
       "141                                0                         0   \n",
       "142                                0                         0   \n",
       "143                                0                         0   \n",
       "144                                0                         0   \n",
       "145                                0                         0   \n",
       "146                                0                         0   \n",
       "147                                0                         0   \n",
       "148                                0                         0   \n",
       "149                                0                         0   \n",
       "150                                0                         0   \n",
       "151                                0                         0   \n",
       "\n",
       "     solution_treatment_sonication  solution_treatment_mixing  \\\n",
       "0                                0                          0   \n",
       "1                                0                          0   \n",
       "2                                0                          0   \n",
       "3                                0                          0   \n",
       "4                                0                          0   \n",
       "5                                0                          0   \n",
       "6                                0                          0   \n",
       "7                                0                          0   \n",
       "8                                0                          0   \n",
       "9                                0                          0   \n",
       "10                               0                          0   \n",
       "11                               0                          0   \n",
       "12                               0                          0   \n",
       "13                               0                          0   \n",
       "14                               0                          0   \n",
       "15                               0                          0   \n",
       "16                               0                          0   \n",
       "17                               0                          0   \n",
       "18                               0                          0   \n",
       "19                               0                          0   \n",
       "20                               0                          0   \n",
       "21                               0                          0   \n",
       "22                               0                          0   \n",
       "23                               0                          0   \n",
       "24                               0                          0   \n",
       "25                               0                          0   \n",
       "26                               0                          0   \n",
       "27                               0                          0   \n",
       "28                               0                          0   \n",
       "29                               0                          0   \n",
       "30                               0                          0   \n",
       "31                               0                          0   \n",
       "32                               0                          0   \n",
       "33                               0                          0   \n",
       "34                               0                          0   \n",
       "35                               0                          0   \n",
       "36                               0                          0   \n",
       "37                               0                          0   \n",
       "38                               0                          0   \n",
       "39                               0                          0   \n",
       "40                               0                          0   \n",
       "41                               0                          0   \n",
       "42                               0                          0   \n",
       "43                               0                          0   \n",
       "44                               0                          0   \n",
       "45                               0                          0   \n",
       "46                               0                          0   \n",
       "47                               0                          0   \n",
       "48                               0                          0   \n",
       "49                               0                          0   \n",
       "50                               0                          0   \n",
       "51                               0                          0   \n",
       "52                               0                          0   \n",
       "53                               0                          0   \n",
       "54                               0                          0   \n",
       "55                               0                          0   \n",
       "56                               0                          0   \n",
       "57                               0                          0   \n",
       "58                               0                          0   \n",
       "59                               0                          0   \n",
       "60                               0                          0   \n",
       "61                               0                          0   \n",
       "62                               0                          0   \n",
       "63                               0                          0   \n",
       "64                               0                          0   \n",
       "65                               0                          0   \n",
       "66                               0                          0   \n",
       "67                               0                          0   \n",
       "68                               0                          0   \n",
       "69                               0                          0   \n",
       "70                               0                          0   \n",
       "71                               0                          0   \n",
       "72                               0                          0   \n",
       "73                               0                          0   \n",
       "74                               0                          0   \n",
       "75                               0                          0   \n",
       "76                               0                          0   \n",
       "77                               0                          0   \n",
       "78                               0                          0   \n",
       "79                               0                          0   \n",
       "80                               0                          0   \n",
       "81                               0                          0   \n",
       "82                               0                          0   \n",
       "83                               0                          0   \n",
       "84                               0                          0   \n",
       "85                               0                          0   \n",
       "86                               0                          0   \n",
       "87                               0                          0   \n",
       "88                               0                          0   \n",
       "89                               0                          0   \n",
       "90                               0                          0   \n",
       "91                               0                          0   \n",
       "92                               0                          0   \n",
       "93                               0                          0   \n",
       "94                               0                          0   \n",
       "95                               0                          0   \n",
       "96                               0                          0   \n",
       "97                               0                          0   \n",
       "98                               0                          0   \n",
       "99                               0                          0   \n",
       "100                              0                          0   \n",
       "101                              0                          0   \n",
       "102                              0                          0   \n",
       "103                              0                          0   \n",
       "104                              0                          0   \n",
       "105                              0                          0   \n",
       "106                              0                          0   \n",
       "107                              0                          0   \n",
       "108                              0                          0   \n",
       "109                              0                          0   \n",
       "110                              0                          0   \n",
       "111                              0                          0   \n",
       "112                              0                          0   \n",
       "113                              0                          0   \n",
       "114                              0                          0   \n",
       "115                              0                          0   \n",
       "116                              0                          0   \n",
       "117                              0                          0   \n",
       "118                              0                          0   \n",
       "119                              0                          0   \n",
       "120                              0                          0   \n",
       "121                              0                          0   \n",
       "122                              0                          0   \n",
       "123                              0                          0   \n",
       "124                              0                          0   \n",
       "125                              0                          0   \n",
       "126                              0                          0   \n",
       "127                              0                          0   \n",
       "128                              0                          0   \n",
       "129                              0                          0   \n",
       "130                              0                          0   \n",
       "131                              0                          0   \n",
       "132                              0                          0   \n",
       "133                              0                          0   \n",
       "134                              0                          0   \n",
       "135                              0                          0   \n",
       "136                              0                          0   \n",
       "137                              0                          0   \n",
       "138                              0                          0   \n",
       "139                              0                          0   \n",
       "140                              0                          0   \n",
       "141                              0                          0   \n",
       "142                              0                          0   \n",
       "143                              0                          0   \n",
       "144                              0                          0   \n",
       "145                              0                          0   \n",
       "146                              0                          0   \n",
       "147                              0                          0   \n",
       "148                              0                          0   \n",
       "149                              0                          0   \n",
       "150                              0                          0   \n",
       "151                              0                          0   \n",
       "\n",
       "     solution_treatment_mixing_multiple  solution_treatment_uv_irradiation  \\\n",
       "0                                     0                                  0   \n",
       "1                                     0                                  0   \n",
       "2                                     0                                  0   \n",
       "3                                     0                                  0   \n",
       "4                                     0                                  0   \n",
       "5                                     0                                  0   \n",
       "6                                     0                                  0   \n",
       "7                                     0                                  0   \n",
       "8                                     0                                  0   \n",
       "9                                     0                                  0   \n",
       "10                                    0                                  0   \n",
       "11                                    0                                  0   \n",
       "12                                    0                                  0   \n",
       "13                                    0                                  0   \n",
       "14                                    0                                  0   \n",
       "15                                    0                                  0   \n",
       "16                                    0                                  0   \n",
       "17                                    0                                  0   \n",
       "18                                    0                                  0   \n",
       "19                                    0                                  0   \n",
       "20                                    0                                  0   \n",
       "21                                    0                                  0   \n",
       "22                                    0                                  0   \n",
       "23                                    0                                  0   \n",
       "24                                    0                                  0   \n",
       "25                                    0                                  0   \n",
       "26                                    0                                  0   \n",
       "27                                    0                                  0   \n",
       "28                                    0                                  0   \n",
       "29                                    0                                  0   \n",
       "30                                    0                                  0   \n",
       "31                                    0                                  0   \n",
       "32                                    0                                  0   \n",
       "33                                    0                                  0   \n",
       "34                                    0                                  0   \n",
       "35                                    0                                  0   \n",
       "36                                    0                                  0   \n",
       "37                                    0                                  0   \n",
       "38                                    0                                  0   \n",
       "39                                    0                                  0   \n",
       "40                                    0                                  0   \n",
       "41                                    0                                  0   \n",
       "42                                    0                                  0   \n",
       "43                                    0                                  0   \n",
       "44                                    0                                  0   \n",
       "45                                    0                                  0   \n",
       "46                                    0                                  0   \n",
       "47                                    0                                  0   \n",
       "48                                    0                                  0   \n",
       "49                                    0                                  0   \n",
       "50                                    0                                  0   \n",
       "51                                    0                                  0   \n",
       "52                                    0                                  0   \n",
       "53                                    0                                  0   \n",
       "54                                    0                                  0   \n",
       "55                                    0                                  0   \n",
       "56                                    0                                  0   \n",
       "57                                    0                                  0   \n",
       "58                                    0                                  0   \n",
       "59                                    0                                  0   \n",
       "60                                    0                                  0   \n",
       "61                                    0                                  0   \n",
       "62                                    0                                  0   \n",
       "63                                    0                                  0   \n",
       "64                                    0                                  0   \n",
       "65                                    0                                  0   \n",
       "66                                    0                                  0   \n",
       "67                                    0                                  0   \n",
       "68                                    0                                  0   \n",
       "69                                    0                                  0   \n",
       "70                                    0                                  0   \n",
       "71                                    0                                  0   \n",
       "72                                    0                                  0   \n",
       "73                                    0                                  0   \n",
       "74                                    0                                  0   \n",
       "75                                    0                                  0   \n",
       "76                                    0                                  0   \n",
       "77                                    0                                  0   \n",
       "78                                    0                                  0   \n",
       "79                                    0                                  0   \n",
       "80                                    0                                  0   \n",
       "81                                    0                                  0   \n",
       "82                                    0                                  0   \n",
       "83                                    0                                  0   \n",
       "84                                    0                                  0   \n",
       "85                                    0                                  0   \n",
       "86                                    0                                  0   \n",
       "87                                    0                                  0   \n",
       "88                                    0                                  0   \n",
       "89                                    0                                  0   \n",
       "90                                    0                                  0   \n",
       "91                                    0                                  0   \n",
       "92                                    0                                  0   \n",
       "93                                    0                                  0   \n",
       "94                                    0                                  0   \n",
       "95                                    0                                  0   \n",
       "96                                    0                                  0   \n",
       "97                                    0                                  0   \n",
       "98                                    0                                  0   \n",
       "99                                    0                                  0   \n",
       "100                                   0                                  0   \n",
       "101                                   0                                  0   \n",
       "102                                   0                                  0   \n",
       "103                                   0                                  0   \n",
       "104                                   0                                  0   \n",
       "105                                   0                                  0   \n",
       "106                                   0                                  0   \n",
       "107                                   0                                  0   \n",
       "108                                   0                                  0   \n",
       "109                                   0                                  0   \n",
       "110                                   0                                  0   \n",
       "111                                   0                                  0   \n",
       "112                                   0                                  0   \n",
       "113                                   0                                  0   \n",
       "114                                   0                                  0   \n",
       "115                                   0                                  0   \n",
       "116                                   0                                  0   \n",
       "117                                   0                                  0   \n",
       "118                                   0                                  0   \n",
       "119                                   0                                  0   \n",
       "120                                   0                                  0   \n",
       "121                                   0                                  0   \n",
       "122                                   0                                  0   \n",
       "123                                   0                                  0   \n",
       "124                                   0                                  0   \n",
       "125                                   0                                  0   \n",
       "126                                   0                                  0   \n",
       "127                                   0                                  0   \n",
       "128                                   0                                  0   \n",
       "129                                   0                                  0   \n",
       "130                                   0                                  0   \n",
       "131                                   0                                  0   \n",
       "132                                   0                                  0   \n",
       "133                                   0                                  0   \n",
       "134                                   0                                  0   \n",
       "135                                   0                                  0   \n",
       "136                                   0                                  0   \n",
       "137                                   0                                  0   \n",
       "138                                   0                                  0   \n",
       "139                                   0                                  0   \n",
       "140                                   0                                  0   \n",
       "141                                   0                                  0   \n",
       "142                                   0                                  0   \n",
       "143                                   0                                  0   \n",
       "144                                   0                                  0   \n",
       "145                                   0                                  0   \n",
       "146                                   0                                  0   \n",
       "147                                   0                                  0   \n",
       "148                                   0                                  0   \n",
       "149                                   0                                  0   \n",
       "150                                   0                                  0   \n",
       "151                                   0                                  0   \n",
       "\n",
       "     post_process_annealing  post_process_drying  post_process_chemical  \n",
       "0                         1                    0                      0  \n",
       "1                         1                    0                      0  \n",
       "2                         1                    0                      0  \n",
       "3                         1                    0                      0  \n",
       "4                         1                    0                      0  \n",
       "5                         1                    0                      0  \n",
       "6                         1                    0                      0  \n",
       "7                         1                    0                      0  \n",
       "8                         1                    0                      0  \n",
       "9                         1                    0                      0  \n",
       "10                        1                    0                      0  \n",
       "11                        1                    0                      0  \n",
       "12                        1                    0                      0  \n",
       "13                        1                    0                      0  \n",
       "14                        1                    0                      0  \n",
       "15                        1                    0                      0  \n",
       "16                        1                    0                      0  \n",
       "17                        1                    0                      0  \n",
       "18                        1                    0                      0  \n",
       "19                        1                    0                      0  \n",
       "20                        1                    0                      0  \n",
       "21                        1                    0                      0  \n",
       "22                        1                    0                      0  \n",
       "23                        1                    0                      0  \n",
       "24                        1                    0                      0  \n",
       "25                        1                    0                      0  \n",
       "26                        1                    0                      0  \n",
       "27                        1                    0                      0  \n",
       "28                        1                    0                      0  \n",
       "29                        1                    0                      0  \n",
       "30                        1                    0                      0  \n",
       "31                        1                    0                      0  \n",
       "32                        1                    0                      0  \n",
       "33                        1                    0                      0  \n",
       "34                        1                    0                      0  \n",
       "35                        1                    0                      0  \n",
       "36                        1                    0                      0  \n",
       "37                        1                    0                      0  \n",
       "38                        1                    0                      0  \n",
       "39                        1                    0                      0  \n",
       "40                        1                    0                      0  \n",
       "41                        1                    0                      0  \n",
       "42                        1                    0                      0  \n",
       "43                        1                    0                      0  \n",
       "44                        1                    0                      0  \n",
       "45                        1                    0                      0  \n",
       "46                        1                    0                      0  \n",
       "47                        1                    0                      0  \n",
       "48                        1                    0                      0  \n",
       "49                        1                    0                      0  \n",
       "50                        1                    0                      0  \n",
       "51                        1                    0                      0  \n",
       "52                        1                    0                      0  \n",
       "53                        1                    0                      0  \n",
       "54                        1                    0                      0  \n",
       "55                        1                    0                      0  \n",
       "56                        1                    0                      0  \n",
       "57                        1                    0                      0  \n",
       "58                        1                    0                      0  \n",
       "59                        1                    0                      0  \n",
       "60                        1                    0                      0  \n",
       "61                        1                    0                      0  \n",
       "62                        1                    0                      0  \n",
       "63                        1                    0                      0  \n",
       "64                        1                    0                      0  \n",
       "65                        1                    0                      0  \n",
       "66                        1                    0                      0  \n",
       "67                        1                    0                      0  \n",
       "68                        1                    0                      0  \n",
       "69                        1                    0                      0  \n",
       "70                        1                    0                      0  \n",
       "71                        1                    0                      0  \n",
       "72                        1                    0                      0  \n",
       "73                        1                    0                      0  \n",
       "74                        1                    0                      0  \n",
       "75                        1                    0                      0  \n",
       "76                        1                    0                      0  \n",
       "77                        1                    0                      0  \n",
       "78                        1                    0                      0  \n",
       "79                        1                    0                      0  \n",
       "80                        1                    0                      0  \n",
       "81                        1                    0                      0  \n",
       "82                        1                    0                      0  \n",
       "83                        1                    0                      0  \n",
       "84                        1                    0                      0  \n",
       "85                        1                    0                      0  \n",
       "86                        1                    0                      0  \n",
       "87                        1                    0                      0  \n",
       "88                        1                    0                      0  \n",
       "89                        1                    0                      0  \n",
       "90                        1                    0                      0  \n",
       "91                        1                    0                      0  \n",
       "92                        1                    0                      0  \n",
       "93                        1                    0                      0  \n",
       "94                        1                    0                      0  \n",
       "95                        1                    0                      0  \n",
       "96                        1                    0                      0  \n",
       "97                        0                    0                      0  \n",
       "98                        1                    0                      0  \n",
       "99                        1                    0                      0  \n",
       "100                       1                    0                      0  \n",
       "101                       1                    0                      0  \n",
       "102                       0                    0                      0  \n",
       "103                       0                    0                      0  \n",
       "104                       0                    0                      0  \n",
       "105                       0                    0                      0  \n",
       "106                       0                    0                      0  \n",
       "107                       0                    0                      0  \n",
       "108                       1                    0                      0  \n",
       "109                       1                    0                      0  \n",
       "110                       1                    0                      0  \n",
       "111                       1                    0                      0  \n",
       "112                       1                    0                      0  \n",
       "113                       1                    0                      0  \n",
       "114                       1                    0                      0  \n",
       "115                       1                    0                      0  \n",
       "116                       1                    0                      0  \n",
       "117                       1                    0                      0  \n",
       "118                       1                    0                      0  \n",
       "119                       1                    0                      0  \n",
       "120                       1                    0                      0  \n",
       "121                       1                    0                      0  \n",
       "122                       1                    0                      0  \n",
       "123                       1                    0                      0  \n",
       "124                       1                    0                      0  \n",
       "125                       1                    0                      0  \n",
       "126                       1                    0                      0  \n",
       "127                       1                    0                      0  \n",
       "128                       1                    0                      0  \n",
       "129                       1                    0                      0  \n",
       "130                       1                    0                      0  \n",
       "131                       1                    0                      0  \n",
       "132                       1                    0                      0  \n",
       "133                       1                    0                      0  \n",
       "134                       1                    0                      0  \n",
       "135                       1                    0                      0  \n",
       "136                       1                    0                      0  \n",
       "137                       1                    0                      0  \n",
       "138                       1                    0                      0  \n",
       "139                       1                    0                      0  \n",
       "140                       1                    0                      0  \n",
       "141                       1                    0                      0  \n",
       "142                       1                    0                      0  \n",
       "143                       1                    0                      0  \n",
       "144                       1                    0                      0  \n",
       "145                       1                    0                      0  \n",
       "146                       1                    0                      0  \n",
       "147                       1                    0                      0  \n",
       "148                       1                    0                      0  \n",
       "149                       1                    0                      0  \n",
       "150                       1                    0                      0  \n",
       "151                       1                    0                      0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DPP_DTT_df = pd.read_csv(\"combined3_df_DPP_DTT_step.csv\")\n",
    "DPP_DTT_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc52a206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['solution_concentration',\n",
       " 'polymer_mw',\n",
       " 'polymer_mn',\n",
       " 'polymer_dispersity',\n",
       " 'hole_mobility',\n",
       " 'solution_treatment',\n",
       " 'substrate_pretreatment',\n",
       " 'post_process',\n",
       " 'channel_width',\n",
       " 'channel_length',\n",
       " 'film_deposition_type_spin',\n",
       " 'dielectric_material_SiO2',\n",
       " 'electrode_configuration_BGBC',\n",
       " 'electrode_configuration_BGTC',\n",
       " 'electrode_configuration_TGBC',\n",
       " 'gate_material_Other',\n",
       " 'film_deposition_type_MGC',\n",
       " 'dielectric_material_other',\n",
       " 'solvent_boiling_point',\n",
       " 'delta_d',\n",
       " 'delta_p',\n",
       " 'delta_h',\n",
       " 'blend_conjugated_polymer',\n",
       " 'insulating_polymer',\n",
       " 'substrate_pretreat_sam',\n",
       " 'substrate_pretreat_plasma',\n",
       " 'substrate_pretreat_uv_ozone',\n",
       " 'solution_treatment_poor_solvent',\n",
       " 'solution_treatment_aging',\n",
       " 'solution_treatment_sonication',\n",
       " 'solution_treatment_mixing',\n",
       " 'solution_treatment_mixing_multiple',\n",
       " 'solution_treatment_uv_irradiation',\n",
       " 'post_process_annealing',\n",
       " 'post_process_drying',\n",
       " 'post_process_chemical']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DPP_DTT_df_columns = DPP_DTT_df.columns.tolist()\n",
    "DPP_DTT_df_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b4cacba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DPP_DTT_Y = DPP_DTT_df[['hole_mobility']]\n",
    "DPP_DTT_X = DPP_DTT_df.drop(labels = 'hole_mobility', axis = 1)\n",
    "DPPPDTT_X = DPP_DTT_df.drop(labels = 'polymer_dispersity', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3287c574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R score for training set: 0.680\n",
      "R score for test set: 0.177\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(DPP_DTT_X, DPP_DTT_Y, test_size = 0.2, random_state=42)\n",
    "LR_model = LinearRegression()\n",
    "\n",
    "LR_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = LR_model.predict(X_train)\n",
    "y_test_pred = LR_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"R score for training set: {r2_train:.3f}\")\n",
    "print(f\"R score for test set: {r2_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cdec9b",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d99392e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree 1: R train = 0.680, R test = 0.177\n",
      "Degree 2: R train = 0.846, R test = -49750.076\n",
      "Degree 3: R train = 0.847, R test = -586167.689\n",
      "Degree 4: R train = 0.847, R test = -86933409.926\n",
      "Degree 5: R train = 0.847, R test = -6865260879467.940\n",
      "Degree 6: R train = 0.847, R test = -7729395014.333\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store R scores\n",
    "train_r2_scores = []\n",
    "test_r2_scores = []\n",
    "degrees = range(1, 7)\n",
    "\n",
    "# Loop through degrees 1 to 6\n",
    "for degree in degrees:\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    \n",
    "    # Train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    # Predict on training and test sets\n",
    "    y_train_pred = model.predict(X_train_poly)\n",
    "    y_test_pred = model.predict(X_test_poly)\n",
    "    \n",
    "    # Calculate R scores\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Append scores to the lists\n",
    "    train_r2_scores.append(r2_train)\n",
    "    test_r2_scores.append(r2_test)\n",
    "\n",
    "# Print R scores for each degree\n",
    "for degree, r2_train, r2_test in zip(degrees, train_r2_scores, test_r2_scores):\n",
    "    print(f\"Degree {degree}: R train = {r2_train:.3f}, R test = {r2_test:.3f}\")\n",
    "\n",
    "# Plot the R scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, train_r2_scores, label='Train R', marker='o')\n",
    "plt.plot(degrees, test_r2_scores, label='Test R', marker='o')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('R Score')\n",
    "plt.title('R Score vs Polynomial Degree')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c19cd8",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dee0ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.1: R train = 0.679, R test = 0.177\n",
      "Alpha 0.2: R train = 0.677, R test = 0.182\n",
      "Alpha 0.3: R train = 0.675, R test = 0.188\n",
      "Alpha 0.4: R train = 0.673, R test = 0.192\n",
      "Alpha 0.5: R train = 0.672, R test = 0.197\n",
      "Alpha 0.6: R train = 0.670, R test = 0.200\n",
      "Alpha 0.7: R train = 0.669, R test = 0.203\n",
      "Alpha 0.8: R train = 0.668, R test = 0.205\n",
      "Alpha 0.9: R train = 0.667, R test = 0.207\n",
      "Alpha 1.0: R train = 0.666, R test = 0.209\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store R scores\n",
    "train_r2_scores = []\n",
    "test_r2_scores = []\n",
    "alphas = np.arange(0.1, 1.1, 0.1)\n",
    "\n",
    "# Loop through alpha values\n",
    "for alpha in alphas:\n",
    "    # Train Ridge Regression model\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on training and test sets\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate R scores\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Append scores to the lists\n",
    "    train_r2_scores.append(r2_train)\n",
    "    test_r2_scores.append(r2_test)\n",
    "\n",
    "# Print R scores for each alpha\n",
    "for alpha, r2_train, r2_test in zip(alphas, train_r2_scores, test_r2_scores):\n",
    "    print(f\"Alpha {alpha:.1f}: R train = {r2_train:.3f}, R test = {r2_test:.3f}\")\n",
    "\n",
    "# Plot the R scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alphas, train_r2_scores, label='Train R', marker='o')\n",
    "plt.plot(alphas, test_r2_scores, label='Test R', marker='o')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('R Score')\n",
    "plt.title('R Score vs Alpha (Ridge Regression)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9aa39",
   "metadata": {},
   "source": [
    "## Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53339a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.1: R train = 0.612, R test = 0.151\n",
      "Alpha 0.2: R train = 0.605, R test = 0.188\n",
      "Alpha 0.3: R train = 0.593, R test = 0.206\n",
      "Alpha 0.4: R train = 0.577, R test = 0.207\n",
      "Alpha 0.5: R train = 0.558, R test = 0.193\n",
      "Alpha 0.6: R train = 0.553, R test = 0.188\n",
      "Alpha 0.7: R train = 0.553, R test = 0.189\n",
      "Alpha 0.8: R train = 0.553, R test = 0.189\n",
      "Alpha 0.9: R train = 0.552, R test = 0.189\n",
      "Alpha 1.0: R train = 0.552, R test = 0.190\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store R scores\n",
    "train_r2_scores = []\n",
    "test_r2_scores = []\n",
    "alphas = np.arange(0.1, 1.1, 0.1)\n",
    "\n",
    "# Loop through alpha values\n",
    "for alpha in alphas:\n",
    "    # Train Lasso Regression model\n",
    "    model = Lasso(alpha=alpha, max_iter=10000)  # Increased max_iter to ensure convergence\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on training and test sets\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate R scores\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Append scores to the lists\n",
    "    train_r2_scores.append(r2_train)\n",
    "    test_r2_scores.append(r2_test)\n",
    "\n",
    "# Print R scores for each alpha\n",
    "for alpha, r2_train, r2_test in zip(alphas, train_r2_scores, test_r2_scores):\n",
    "    print(f\"Alpha {alpha:.1f}: R train = {r2_train:.3f}, R test = {r2_test:.3f}\")\n",
    "\n",
    "# Plot the R scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alphas, train_r2_scores, label='Train R', marker='o')\n",
    "plt.plot(alphas, test_r2_scores, label='Test R', marker='o')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('R Score')\n",
    "plt.title('R Score vs Alpha (Lasso Regression)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bca8f0",
   "metadata": {},
   "source": [
    "## Elsaticnet regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e25191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    alpha  l1_ratio  r2_train   r2_test\n",
      "0     0.1       0.1  0.638148  0.194274\n",
      "1     0.1       0.2  0.632734  0.184313\n",
      "2     0.1       0.3  0.626865  0.173431\n",
      "3     0.1       0.4  0.621188  0.158119\n",
      "4     0.1       0.5  0.616712  0.143225\n",
      "5     0.1       0.6  0.613819  0.137066\n",
      "6     0.1       0.7  0.613400  0.140305\n",
      "7     0.1       0.8  0.613106  0.143911\n",
      "8     0.1       0.9  0.612781  0.147459\n",
      "9     0.1       1.0  0.612424  0.150946\n",
      "10    0.2       0.1  0.627650  0.175656\n",
      "11    0.2       0.2  0.618880  0.159593\n",
      "12    0.2       0.3  0.613212  0.148563\n",
      "13    0.2       0.4  0.611926  0.154939\n",
      "14    0.2       0.5  0.611105  0.161103\n",
      "15    0.2       0.6  0.610159  0.167015\n",
      "16    0.2       0.7  0.609081  0.172653\n",
      "17    0.2       0.8  0.607862  0.177992\n",
      "18    0.2       0.9  0.606493  0.183008\n",
      "19    0.2       1.0  0.604964  0.187673\n",
      "20    0.3       0.1  0.620168  0.167574\n",
      "21    0.3       0.2  0.612478  0.155921\n",
      "22    0.3       0.3  0.610324  0.165726\n",
      "23    0.3       0.4  0.608642  0.174408\n",
      "24    0.3       0.5  0.606829  0.181656\n",
      "25    0.3       0.6  0.604707  0.188227\n",
      "26    0.3       0.7  0.602250  0.194041\n",
      "27    0.3       0.8  0.599425  0.199009\n",
      "28    0.3       0.9  0.596198  0.203030\n",
      "29    0.3       1.0  0.592530  0.205992\n",
      "30    0.4       0.1  0.615951  0.167965\n",
      "31    0.4       0.2  0.609936  0.167719\n",
      "32    0.4       0.3  0.607204  0.180022\n",
      "33    0.4       0.4  0.604482  0.188629\n",
      "34    0.4       0.5  0.601327  0.195708\n",
      "35    0.4       0.6  0.597580  0.201412\n",
      "36    0.4       0.7  0.593171  0.205536\n",
      "37    0.4       0.8  0.588046  0.207844\n",
      "38    0.4       0.9  0.583002  0.208247\n",
      "39    0.4       1.0  0.577167  0.206804\n",
      "40    0.5       0.1  0.612607  0.170045\n",
      "41    0.5       0.2  0.607598  0.178368\n",
      "42    0.5       0.3  0.603585  0.190681\n",
      "43    0.5       0.4  0.599628  0.198461\n",
      "44    0.5       0.5  0.594809  0.204202\n",
      "45    0.5       0.6  0.589008  0.207552\n",
      "46    0.5       0.7  0.582860  0.208226\n",
      "47    0.5       0.8  0.575920  0.206311\n",
      "48    0.5       0.9  0.567723  0.201386\n",
      "49    0.5       1.0  0.558069  0.192897\n",
      "50    0.6       0.1  0.609969  0.173167\n",
      "51    0.6       0.2  0.604987  0.186908\n",
      "52    0.6       0.3  0.599716  0.198160\n",
      "53    0.6       0.4  0.594225  0.204614\n",
      "54    0.6       0.5  0.587453  0.207935\n",
      "55    0.6       0.6  0.580254  0.207805\n",
      "56    0.6       0.7  0.571798  0.204189\n",
      "57    0.6       0.8  0.561613  0.196381\n",
      "58    0.6       0.9  0.553352  0.188214\n",
      "59    0.6       1.0  0.553221  0.188468\n",
      "60    0.7       0.1  0.607939  0.178616\n",
      "61    0.7       0.2  0.602160  0.193644\n",
      "62    0.7       0.3  0.595592  0.203279\n",
      "63    0.7       0.4  0.588391  0.207674\n",
      "64    0.7       0.5  0.580283  0.207819\n",
      "65    0.7       0.6  0.570890  0.203680\n",
      "66    0.7       0.7  0.559368  0.194354\n",
      "67    0.7       0.8  0.553306  0.188398\n",
      "68    0.7       0.9  0.553146  0.188675\n",
      "69    0.7       1.0  0.552967  0.188900\n",
      "70    0.8       0.1  0.606010  0.183883\n",
      "71    0.8       0.2  0.599164  0.198829\n",
      "72    0.8       0.3  0.591283  0.206439\n",
      "73    0.8       0.4  0.582530  0.208174\n",
      "74    0.8       0.5  0.572844  0.204886\n",
      "75    0.8       0.6  0.560753  0.195753\n",
      "76    0.8       0.7  0.553302  0.188493\n",
      "77    0.8       0.8  0.553117  0.188804\n",
      "78    0.8       0.9  0.552908  0.189046\n",
      "79    0.8       1.0  0.552675  0.189221\n",
      "80    0.9       0.1  0.604202  0.188593\n",
      "81    0.9       0.2  0.596037  0.202685\n",
      "82    0.9       0.3  0.586846  0.207969\n",
      "83    0.9       0.4  0.576948  0.206843\n",
      "84    0.9       0.5  0.565023  0.199556\n",
      "85    0.9       0.6  0.553340  0.188501\n",
      "86    0.9       0.7  0.553137  0.188862\n",
      "87    0.9       0.8  0.552902  0.189138\n",
      "88    0.9       0.9  0.552638  0.189328\n",
      "89    0.9       1.0  0.552343  0.189433\n",
      "90    1.0       0.1  0.602440  0.192689\n",
      "91    1.0       0.2  0.592811  0.205384\n",
      "92    1.0       0.3  0.582366  0.208145\n",
      "93    1.0       0.4  0.571214  0.204033\n",
      "94    1.0       0.5  0.556899  0.192174\n",
      "95    1.0       0.6  0.553203  0.188845\n",
      "96    1.0       0.7  0.552952  0.189176\n",
      "97    1.0       0.8  0.552663  0.189401\n",
      "98    1.0       0.9  0.552336  0.189521\n",
      "99    1.0       1.0  0.551972  0.189534\n",
      "Maximum Train R:\n",
      "alpha       0.100000\n",
      "l1_ratio    0.100000\n",
      "r2_train    0.638148\n",
      "r2_test     0.194274\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Maximum Test R:\n",
      "alpha       0.400000\n",
      "l1_ratio    0.900000\n",
      "r2_train    0.583002\n",
      "r2_test     0.208247\n",
      "Name: 38, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Alpha and l1_ratio ranges\n",
    "alphas = np.arange(0.1, 1.1, 0.1)\n",
    "l1_ratios = np.arange(0.1, 1.1, 0.1)\n",
    "\n",
    "# Loop through alpha and l1_ratio values\n",
    "for alpha in alphas:\n",
    "    for l1_ratio in l1_ratios:\n",
    "        # Train ElasticNet Regression model\n",
    "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=10000)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on training and test sets\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate R scores\n",
    "        r2_train = r2_score(y_train, y_train_pred)\n",
    "        r2_test = r2_score(y_test, y_test_pred)\n",
    "        \n",
    "        # Append results\n",
    "        results.append({'alpha': alpha, 'l1_ratio': l1_ratio, 'r2_train': r2_train, 'r2_test': r2_test})\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results\n",
    "print(results_df)\n",
    "\n",
    "# Find the row with the maximum R score for training\n",
    "max_train_r2 = results_df.loc[results_df['r2_train'].idxmax()]\n",
    "print(\"Maximum Train R:\")\n",
    "print(max_train_r2)\n",
    "\n",
    "# Find the row with the maximum R score for testing\n",
    "max_test_r2 = results_df.loc[results_df['r2_test'].idxmax()]\n",
    "print(\"\\nMaximum Test R:\")\n",
    "print(max_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5484c70",
   "metadata": {},
   "source": [
    "## Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91a74394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    C  epsilon  r2_train   r2_test\n",
      "0   1      0.1 -0.099158 -0.075577\n",
      "1   1      0.2 -0.077072 -0.044834\n",
      "2   1      0.3 -0.048635 -0.013645\n",
      "3   1      0.4 -0.030299  0.000551\n",
      "4   1      0.5 -0.010476  0.016364\n",
      "5   1      0.6  0.005614  0.018025\n",
      "6   1      0.7  0.017294  0.007968\n",
      "7   1      0.8  0.024041 -0.009323\n",
      "8   1      0.9  0.024753 -0.039111\n",
      "9   1      1.0  0.018660 -0.084332\n",
      "10  1      1.1  0.008660 -0.141387\n",
      "11  2      0.1 -0.076795 -0.058906\n",
      "12  2      0.2 -0.054359 -0.026748\n",
      "13  2      0.3 -0.027167 -0.003773\n",
      "14  2      0.4 -0.002237  0.018708\n",
      "15  2      0.5  0.013825  0.029208\n",
      "16  2      0.6  0.025638  0.026166\n",
      "17  2      0.7  0.036444  0.013889\n",
      "18  2      0.8  0.042567 -0.005299\n",
      "19  2      0.9  0.039893 -0.039725\n",
      "20  2      1.0  0.032226 -0.083235\n",
      "21  2      1.1  0.022048 -0.140720\n",
      "22  3      0.1 -0.066176 -0.060704\n",
      "23  3      0.2 -0.040989 -0.022018\n",
      "24  3      0.3 -0.009084  0.009222\n",
      "25  3      0.4  0.010744  0.026690\n",
      "26  3      0.5  0.030510  0.039187\n",
      "27  3      0.6  0.041586  0.035347\n",
      "28  3      0.7  0.051685  0.021517\n",
      "29  3      0.8  0.057126  0.000781\n",
      "30  3      0.9  0.054191 -0.033144\n",
      "31  3      1.0  0.044765 -0.088206\n",
      "32  3      1.1  0.033698 -0.147973\n",
      "33  4      0.1 -0.056488 -0.067663\n",
      "34  4      0.2 -0.026506 -0.014483\n",
      "35  4      0.3  0.000785  0.008360\n",
      "36  4      0.4  0.024504  0.032786\n",
      "37  4      0.5  0.043420  0.047068\n",
      "38  4      0.6  0.055217  0.041991\n",
      "39  4      0.7  0.064916  0.027090\n",
      "40  4      0.8  0.069835  0.005271\n",
      "41  4      0.9  0.064008 -0.034362\n",
      "42  4      1.0  0.057810 -0.079932\n",
      "43  4      1.1  0.046551 -0.139506\n",
      "44  5      0.1 -0.044818 -0.071464\n",
      "45  5      0.2 -0.014432 -0.011107\n",
      "46  5      0.3  0.012194  0.014568\n",
      "47  5      0.4  0.034160  0.035180\n",
      "48  5      0.5  0.052577  0.051785\n",
      "49  5      0.6  0.066634  0.048421\n",
      "50  5      0.7  0.076242  0.033596\n",
      "51  5      0.8  0.081076  0.011867\n",
      "52  5      0.9  0.075759 -0.025943\n",
      "53  5      1.0  0.065967 -0.078857\n",
      "54  5      1.1  0.054787 -0.138178\n",
      "55  6      0.1 -0.026963 -0.065877\n",
      "56  6      0.2 -0.007387 -0.015390\n",
      "57  6      0.3  0.020605  0.014358\n",
      "58  6      0.4  0.044520  0.038427\n",
      "59  6      0.5  0.061456  0.051190\n",
      "60  6      0.6  0.075190  0.050014\n",
      "61  6      0.7  0.084869  0.034811\n",
      "62  6      0.8  0.089652  0.012884\n",
      "63  6      0.9  0.087717 -0.026952\n",
      "64  6      1.0  0.078322 -0.081020\n",
      "65  6      1.1  0.066847 -0.141412\n",
      "\n",
      "Maximum Train R:\n",
      "C           6.000000\n",
      "epsilon     0.800000\n",
      "r2_train    0.089652\n",
      "r2_test     0.012884\n",
      "Name: 62, dtype: float64\n",
      "\n",
      "Maximum Test R:\n",
      "C           5.000000\n",
      "epsilon     0.500000\n",
      "r2_train    0.052577\n",
      "r2_test     0.051785\n",
      "Name: 48, dtype: float64\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DataFrame.pivot() takes 1 positional argument but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 53\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(max_test_r2)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Visualize R scores as heatmaps\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Pivot data for heatmaps\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m train_heatmap \u001b[38;5;241m=\u001b[39m results_df\u001b[38;5;241m.\u001b[39mpivot(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr2_train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m test_heatmap \u001b[38;5;241m=\u001b[39m results_df\u001b[38;5;241m.\u001b[39mpivot(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr2_test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Plot train heatmap\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: DataFrame.pivot() takes 1 positional argument but 4 were given"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Range of C and epsilon values\n",
    "C_values = np.arange(1, 7, 1)\n",
    "epsilon_values = np.arange(0.1, 1.2, 0.1)\n",
    "\n",
    "# Loop through C and epsilon values\n",
    "for C in C_values:\n",
    "    for epsilon in epsilon_values:\n",
    "        # Train SVR model\n",
    "        model = SVR(C=C, epsilon=epsilon)\n",
    "        model.fit(X_train, y_train)  # Flatten y for SVR compatibility\n",
    "        \n",
    "        # Predict on training and test sets\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate R scores\n",
    "        r2_train = r2_score(y_train, y_train_pred)\n",
    "        r2_test = r2_score(y_test, y_test_pred)\n",
    "        \n",
    "        # Append results\n",
    "        results.append({'C': C, 'epsilon': epsilon, 'r2_train': r2_train, 'r2_test': r2_test})\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results\n",
    "print(results_df)\n",
    "\n",
    "# Find the best combination for training and testing\n",
    "max_train_r2 = results_df.loc[results_df['r2_train'].idxmax()]\n",
    "max_test_r2 = results_df.loc[results_df['r2_test'].idxmax()]\n",
    "\n",
    "print(\"\\nMaximum Train R:\")\n",
    "print(max_train_r2)\n",
    "\n",
    "print(\"\\nMaximum Test R:\")\n",
    "print(max_test_r2)\n",
    "\n",
    "# Visualize R scores as heatmaps\n",
    "# Pivot data for heatmaps\n",
    "train_heatmap = results_df.pivot(\"C\", \"epsilon\", \"r2_train\")\n",
    "test_heatmap = results_df.pivot(\"C\", \"epsilon\", \"r2_test\")\n",
    "\n",
    "# Plot train heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(train_heatmap, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Train R Heatmap\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"C\")\n",
    "plt.show()\n",
    "\n",
    "# Plot test heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(test_heatmap, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Test R Heatmap\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"C\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd08a7e",
   "metadata": {},
   "source": [
    "## Decision tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1e45f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R score for training set: 0.847\n",
      "R score for test set: 0.377\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the Decision Tree Regressor\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and test sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate R scores\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the R scores\n",
    "print(f\"R score for training set: {r2_train:.3f}\")\n",
    "print(f\"R score for test set: {r2_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14feb2c8",
   "metadata": {},
   "source": [
    "## Random forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89496068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 configurations by Test R:\n",
      "    n_estimators  r2_train   r2_test\n",
      "25           350  0.840418  0.444602\n",
      "26           360  0.840314  0.442252\n",
      "24           340  0.840372  0.440142\n",
      "27           370  0.840374  0.438959\n",
      "1            110  0.839977  0.438187\n",
      "28           380  0.840225  0.437445\n",
      "21           310  0.840314  0.436502\n",
      "18           280  0.840604  0.436096\n",
      "23           330  0.840324  0.435728\n",
      "20           300  0.840493  0.435047\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Range of n_estimators\n",
    "n_estimators_range = range(100, 401, 10)\n",
    "\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "# Loop through n_estimators values\n",
    "for n_estimators in n_estimators_range:\n",
    "    # Train Random Forest Regressor\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, random_state=42)\n",
    "    model.fit(X_train, y_train.ravel())  # Flatten y for compatibility\n",
    "    \n",
    "    # Predict on training and test sets\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate R scores\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Append results\n",
    "    results.append({'n_estimators': n_estimators, 'r2_train': r2_train, 'r2_test': r2_test})\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 10 configurations by Test R:\")\n",
    "print(results_df.sort_values(by='r2_test', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa756e55",
   "metadata": {},
   "source": [
    "## Gradient bossting regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17993859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 configurations by Test R:\n",
      "    n_estimators  r2_train   r2_test\n",
      "25           350  0.840418  0.444602\n",
      "26           360  0.840314  0.442252\n",
      "24           340  0.840372  0.440142\n",
      "27           370  0.840374  0.438959\n",
      "1            110  0.839977  0.438187\n",
      "28           380  0.840225  0.437445\n",
      "21           310  0.840314  0.436502\n",
      "18           280  0.840604  0.436096\n",
      "23           330  0.840324  0.435728\n",
      "20           300  0.840493  0.435047\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Hyperparameter ranges\n",
    "n_estimators_range = range(100, 311, 10)\n",
    "learning_rate_range = np.arange(0.1, 1.1, 0.1)\n",
    "max_depth_range = range(1, 16)\n",
    "\n",
    "# Perform grid search over hyperparameters\n",
    "for n_estimators in n_estimators_range:\n",
    "    for learning_rate in learning_rate_range:\n",
    "        for max_depth in max_depth_range:\n",
    "            # Train Gradient Boosting Regressor\n",
    "            model = GradientBoostingRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                learning_rate=learning_rate,\n",
    "                max_depth=max_depth\n",
    "            )\n",
    "            model.fit(X_train, y_train.ravel())  # Flatten y for compatibility\n",
    "            \n",
    "            # Predict on training and test sets\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate R scores\n",
    "            r2_train = r2_score(y_train, y_train_pred)\n",
    "            r2_test = r2_score(y_test, y_test_pred)\n",
    "            \n",
    "            # Append results\n",
    "            results.append({\n",
    "                'n_estimators': n_estimators,\n",
    "                'learning_rate': learning_rate,\n",
    "                'max_depth': max_depth,\n",
    "                'r2_train': r2_train,\n",
    "                'r2_test': r2_test\n",
    "            })\n",
    "\n",
    "# Print the top results sorted by test R\n",
    "print(\"Top 10 configurations by Test R:\")\n",
    "print(results_df.sort_values(by='r2_test', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e7f6805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: post_process_chemical, Importance: 0.0\n",
      "Feature: substrate_pretreat_plasma, Importance: 0.0\n",
      "Feature: solution_treatment_aging, Importance: 0.0\n",
      "Feature: blend_conjugated_polymer, Importance: 0.0\n",
      "Feature: solution_treatment_uv_irradiation, Importance: 0.0\n",
      "Feature: solution_treatment_mixing_multiple, Importance: 0.0\n",
      "Feature: post_process_drying, Importance: 0.0\n",
      "Feature: solution_treatment_mixing, Importance: 0.0\n",
      "Feature: substrate_pretreat_uv_ozone, Importance: 0.0\n",
      "Feature: solution_treatment_sonication, Importance: 0.0\n",
      "Feature: gate_material_Other, Importance: 2.9711973327170317e-19\n",
      "Feature: post_process_annealing, Importance: 7.356547214009688e-08\n",
      "Feature: solution_treatment, Importance: 1.560210162521531e-07\n",
      "Feature: delta_p, Importance: 3.365513877488152e-07\n",
      "Feature: delta_h, Importance: 8.526121304700842e-07\n",
      "Feature: dielectric_material_SiO2, Importance: 2.2182448131121053e-06\n",
      "Feature: electrode_configuration_TGBC, Importance: 4.005597551139224e-06\n",
      "Feature: post_process, Importance: 5.999108443756288e-06\n",
      "Feature: solution_treatment_poor_solvent, Importance: 6.996311972375948e-06\n",
      "Feature: dielectric_material_other, Importance: 1.6082468429948245e-05\n",
      "Feature: polymer_dispersity, Importance: 1.730883337277651e-05\n",
      "Feature: delta_d, Importance: 2.0532159051557307e-05\n",
      "Feature: polymer_mn, Importance: 0.0005149613747282233\n",
      "Feature: electrode_configuration_BGBC, Importance: 0.0006869656576547412\n",
      "Feature: substrate_pretreat_sam, Importance: 0.001021922849735342\n",
      "Feature: film_deposition_type_spin, Importance: 0.00137859536480957\n",
      "Feature: substrate_pretreatment, Importance: 0.0014110293855334546\n",
      "Feature: solution_concentration, Importance: 0.0015950220276560554\n",
      "Feature: electrode_configuration_BGTC, Importance: 0.003703522947035697\n",
      "Feature: channel_width, Importance: 0.004268727121463677\n",
      "Feature: channel_length, Importance: 0.01178047803771316\n",
      "Feature: film_deposition_type_MGC, Importance: 0.03447897214893614\n",
      "Feature: solvent_boiling_point, Importance: 0.14001230918776292\n",
      "Feature: insulating_polymer, Importance: 0.14998964905596782\n",
      "Feature: polymer_mw, Importance: 0.649083283367362\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fixed_learning_rate = 0.8\n",
    "fixed_n_estimators = 210\n",
    "max_depth = 10\n",
    "\n",
    "model = GradientBoostingRegressor(n_estimators=fixed_n_estimators, learning_rate=fixed_learning_rate, max_depth=max_depth, random_state=42)\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "feature_importance = model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), np.array(X_train.columns)[sorted_idx])\n",
    "plt.title('Feature Importance (Built-in)')\n",
    "plt.show()\n",
    "\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "sorted_feature_names = np.array(X_train.columns)[sorted_idx]\n",
    "sorted_feature_importances = feature_importance[sorted_idx]\n",
    "\n",
    "for feature, importance in zip(sorted_feature_names, sorted_feature_importances):\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8cc2cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = permutation_importance(model, X_train, Y_train, n_repeats=10)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.barh(range(len(sorted_idx)), result.importances_mean[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), np.array(X_train.columns)[sorted_idx])\n",
    "plt.title('Permutation Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bee255f",
   "metadata": {},
   "source": [
    "## K-nearest Neighbors Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23645960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top configurations by Test R:\n",
      "    n_neighbors  r2_train   r2_test\n",
      "12           13  0.475839  0.328416\n",
      "11           12  0.526005  0.316301\n",
      "10           11  0.562662  0.301058\n",
      "14           15  0.410780  0.291128\n",
      "13           14  0.444165  0.290294\n",
      "9            10  0.586484  0.289605\n",
      "15           16  0.368694  0.272052\n",
      "8             9  0.630303  0.243701\n",
      "7             8  0.672682  0.157400\n",
      "6             7  0.708729  0.023895\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Range of n_neighbors\n",
    "n_neighbors_range = range(1, 17)\n",
    "\n",
    "# Loop through n_neighbors values\n",
    "for n_neighbors in n_neighbors_range:\n",
    "    # Train K-Nearest Neighbors Regressor\n",
    "    model = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "    model.fit(X_train, y_train.ravel())  # Flatten y for compatibility\n",
    "    \n",
    "    # Predict on training and test sets\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate R scores\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Append results\n",
    "    results.append({'n_neighbors': n_neighbors, 'r2_train': r2_train, 'r2_test': r2_test})\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results\n",
    "print(\"Top configurations by Test R:\")\n",
    "print(results_df.sort_values(by='r2_test', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8eb45c82-77ff-448c-bf15-259779121e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               feature    importance\n",
      "7                        channel_width  8.237159e-01\n",
      "1                           polymer_mw  1.612363e-01\n",
      "22                  insulating_polymer  2.898555e-04\n",
      "34               post_process_chemical  0.000000e+00\n",
      "31   solution_treatment_uv_irradiation  0.000000e+00\n",
      "30  solution_treatment_mixing_multiple  0.000000e+00\n",
      "29           solution_treatment_mixing  0.000000e+00\n",
      "28       solution_treatment_sonication  0.000000e+00\n",
      "27            solution_treatment_aging  0.000000e+00\n",
      "26     solution_treatment_poor_solvent  0.000000e+00\n",
      "25         substrate_pretreat_uv_ozone  0.000000e+00\n",
      "24           substrate_pretreat_plasma  0.000000e+00\n",
      "21            blend_conjugated_polymer  0.000000e+00\n",
      "20                             delta_h  0.000000e+00\n",
      "19                             delta_p  0.000000e+00\n",
      "18                             delta_d  0.000000e+00\n",
      "16           dielectric_material_other  0.000000e+00\n",
      "33                 post_process_drying  0.000000e+00\n",
      "15            film_deposition_type_MGC  0.000000e+00\n",
      "14                 gate_material_Other  0.000000e+00\n",
      "13        electrode_configuration_TGBC  0.000000e+00\n",
      "12        electrode_configuration_BGTC  0.000000e+00\n",
      "11        electrode_configuration_BGBC  0.000000e+00\n",
      "10            dielectric_material_SiO2  0.000000e+00\n",
      "9            film_deposition_type_spin  0.000000e+00\n",
      "6                         post_process  0.000000e+00\n",
      "4                   solution_treatment  0.000000e+00\n",
      "3                   polymer_dispersity  0.000000e+00\n",
      "32              post_process_annealing  0.000000e+00\n",
      "8                       channel_length -1.850372e-17\n",
      "5               substrate_pretreatment -1.762269e-03\n",
      "23              substrate_pretreat_sam -2.070706e-03\n",
      "2                           polymer_mn -2.318334e-03\n",
      "0               solution_concentration -2.475351e-03\n",
      "17               solvent_boiling_point -7.361740e-02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Train k-NN model\n",
    "knn = KNeighborsRegressor(n_neighbors=9)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate baseline performance\n",
    "baseline_r2 = r2_score(Y_test, knn.predict(X_test))\n",
    "\n",
    "# Compute permutation importance\n",
    "perm_importance = permutation_importance(knn, X_test, Y_test, n_repeats=30, random_state=42)\n",
    "\n",
    "# Display importance scores\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': DPP_DTT_X.columns,\n",
    "    'importance': perm_importance.importances_mean\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba5bdc62-a675-45e8-808b-ab09547e0a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 configurations by Test R:\n",
      "      n_estimators  learning_rate  max_depth  r2_train   r2_test\n",
      "706            140            0.5          3  0.846910  0.447590\n",
      "3426           310            0.5          3  0.846911  0.447546\n",
      "2626           260            0.5          3  0.846911  0.447546\n",
      "2946           280            0.5          3  0.846911  0.447546\n",
      "2786           270            0.5          3  0.846911  0.447546\n",
      "3266           300            0.5          3  0.846911  0.447546\n",
      "3106           290            0.5          3  0.846911  0.447546\n",
      "2466           250            0.5          3  0.846911  0.447546\n",
      "2306           240            0.5          3  0.846911  0.447546\n",
      "2146           230            0.5          3  0.846911  0.447546\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Hyperparameter ranges\n",
    "n_estimators_range = range(100, 311, 10)\n",
    "learning_rate_range = np.arange(0.1, 1.1, 0.1)\n",
    "max_depth_range = range(1, 17)\n",
    "\n",
    "# Perform grid search over hyperparameters\n",
    "for n_estimators in n_estimators_range:\n",
    "    for learning_rate in learning_rate_range:\n",
    "        for max_depth in max_depth_range:\n",
    "            # Train XGBoost Regressor\n",
    "            model = XGBRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                learning_rate=learning_rate,\n",
    "                max_depth=max_depth,\n",
    "                random_state=42\n",
    "            )\n",
    "            model.fit(X_train, y_train.ravel())  # Flatten y for compatibility\n",
    "            \n",
    "            # Predict on training and test sets\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate R scores\n",
    "            r2_train = r2_score(y_train, y_train_pred)\n",
    "            r2_test = r2_score(y_test, y_test_pred)\n",
    "            \n",
    "            # Append results\n",
    "            results.append({\n",
    "                'n_estimators': n_estimators,\n",
    "                'learning_rate': learning_rate,\n",
    "                'max_depth': max_depth,\n",
    "                'r2_train': r2_train,\n",
    "                'r2_test': r2_test\n",
    "            })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the top 10 configurations sorted by test R\n",
    "print(\"Top 10 configurations by Test R:\")\n",
    "print(results_df.sort_values(by='r2_test', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2497010-8b66-4455-8830-f11bcbe7d560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-1.2.7-cp311-cp311-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting graphviz (from catboost)\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\myeongyeon lee\\anaconda3\\lib\\site-packages (from catboost) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\myeongyeon lee\\anaconda3\\lib\\site-packages (from catboost) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\myeongyeon lee\\anaconda3\\lib\\site-packages (from catboost) (2.1.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\myeongyeon lee\\anaconda3\\lib\\site-packages (from catboost) (1.11.4)\n",
      "Requirement already satisfied: plotly in c:\\users\\myeongyeon lee\\anaconda3\\lib\\site-packages (from catboost) (5.9.0)\n",
      "Requirement already satisfied: six in c:\\users\\myeongyeon lee\\anaconda3\\lib\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\myeongyeon lee\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\myeongyeon lee\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\myeongyeon lee\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\myeongyeon lee\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\myeongyeon lee\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\myeongyeon lee\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\myeongyeon lee\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\myeongyeon lee\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\myeongyeon lee\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\myeongyeon lee\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (3.0.9)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\myeongyeon lee\\anaconda3\\lib\\site-packages (from plotly->catboost) (8.2.2)\n",
      "Downloading catboost-1.2.7-cp311-cp311-win_amd64.whl (101.7 MB)\n",
      "   ---------------------------------------- 0.0/101.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/101.7 MB 1.6 MB/s eta 0:01:04\n",
      "   ---------------------------------------- 0.5/101.7 MB 5.9 MB/s eta 0:00:18\n",
      "   ---------------------------------------- 1.1/101.7 MB 9.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 2.2/101.7 MB 12.7 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 3.7/101.7 MB 17.1 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 6.1/101.7 MB 22.9 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 9.7/101.7 MB 31.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 15.2/101.7 MB 93.0 MB/s eta 0:00:01\n",
      "   ------- ------------------------------- 20.8/101.7 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------- ---------------------------- 26.4/101.7 MB 110.0 MB/s eta 0:00:01\n",
      "   ------------ -------------------------- 31.9/101.7 MB 110.0 MB/s eta 0:00:01\n",
      "   -------------- ------------------------ 37.1/101.7 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------------- ---------------------- 42.5/101.7 MB 108.8 MB/s eta 0:00:01\n",
      "   ------------------ -------------------- 48.0/101.7 MB 108.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------ 53.5/101.7 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------------------- ---------------- 58.8/101.7 MB 108.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 62.8/101.7 MB 93.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 67.0/101.7 MB 93.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 71.3/101.7 MB 81.8 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 75.7/101.7 MB 108.8 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 80.2/101.7 MB 110.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 84.7/101.7 MB 110.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 87.7/101.7 MB 93.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 90.9/101.7 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 93.9/101.7 MB 72.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 95.7/101.7 MB 59.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 99.0/101.7 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.7/101.7 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.7/101.7 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.7/101.7 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 101.7/101.7 MB 36.4 MB/s eta 0:00:00\n",
      "Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.1/47.1 kB ? eta 0:00:00\n",
      "Installing collected packages: graphviz, catboost\n",
      "Successfully installed catboost-1.2.7 graphviz-0.20.3\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4157de62-b840-45ec-81de-ccd8a4df793b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 configurations by Test R:\n",
      "      iterations  learning_rate  depth  r2_train   r2_test\n",
      "74           100            0.5     11  0.846915  0.525470\n",
      "234          110            0.5     11  0.846915  0.525372\n",
      "394          120            0.5     11  0.846915  0.525355\n",
      "554          130            0.5     11  0.846916  0.525339\n",
      "714          140            0.5     11  0.846916  0.525321\n",
      "1034         160            0.5     11  0.846916  0.525312\n",
      "874          150            0.5     11  0.846916  0.525309\n",
      "1194         170            0.5     11  0.846916  0.525306\n",
      "1514         190            0.5     11  0.846916  0.525305\n",
      "1674         200            0.5     11  0.846916  0.525304\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Hyperparameter ranges\n",
    "iterations_range = range(100, 311, 10)\n",
    "learning_rate_range = np.arange(0.1, 1.1, 0.1)\n",
    "depth_range = range(1, 17)\n",
    "\n",
    "# Perform grid search over hyperparameters\n",
    "for iterations in iterations_range:\n",
    "    for learning_rate in learning_rate_range:\n",
    "        for depth in depth_range:\n",
    "            # Train CatBoost Regressor\n",
    "            model = CatBoostRegressor(\n",
    "                iterations=iterations,\n",
    "                learning_rate=learning_rate,\n",
    "                depth=depth,\n",
    "                verbose=0,  # Suppress CatBoost output\n",
    "                random_seed=42\n",
    "            )\n",
    "            model.fit(X_train, y_train.ravel())  # Flatten y for compatibility\n",
    "            \n",
    "            # Predict on training and test sets\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate R scores\n",
    "            r2_train = r2_score(y_train, y_train_pred)\n",
    "            r2_test = r2_score(y_test, y_test_pred)\n",
    "            \n",
    "            # Append results\n",
    "            results.append({\n",
    "                'iterations': iterations,\n",
    "                'learning_rate': learning_rate,\n",
    "                'depth': depth,\n",
    "                'r2_train': r2_train,\n",
    "                'r2_test': r2_test\n",
    "            })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the top 10 configurations sorted by test R\n",
    "print(\"Top 10 configurations by Test R:\")\n",
    "print(results_df.sort_values(by='r2_test', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "edf3f1e3-d27b-4561-a94f-25df835f3082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 configurations by Test R:\n",
      "      n_estimators  learning_rate  max_depth  r2_train   r2_test\n",
      "0              100            0.1          1  0.826773  0.656678\n",
      "321            120            0.1          2  0.826773  0.656678\n",
      "1287           180            0.1          8  0.826773  0.656678\n",
      "1288           180            0.1          9  0.826773  0.656678\n",
      "1289           180            0.1         10  0.826773  0.656678\n",
      "1290           180            0.1         11  0.826773  0.656678\n",
      "1291           180            0.1         12  0.826773  0.656678\n",
      "1292           180            0.1         13  0.826773  0.656678\n",
      "1293           180            0.1         14  0.826773  0.656678\n",
      "1294           180            0.1         15  0.826773  0.656678\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Hyperparameter ranges\n",
    "n_estimators_range = range(100, 311, 10)\n",
    "learning_rate_range = np.arange(0.1, 1.1, 0.1)\n",
    "max_depth_range = range(1, 17)\n",
    "\n",
    "# Perform grid search over hyperparameters\n",
    "for n_estimators in n_estimators_range:\n",
    "    for learning_rate in learning_rate_range:\n",
    "        for max_depth in max_depth_range:\n",
    "            # Define the AdaBoost model with a DecisionTreeRegressor as the base learner\n",
    "            model = AdaBoostRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                learning_rate=learning_rate,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            # Train the model\n",
    "            model.fit(X_train, y_train.ravel())\n",
    "            \n",
    "            # Predict on training and test sets\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate R scores\n",
    "            r2_train = r2_score(y_train, y_train_pred)\n",
    "            r2_test = r2_score(y_test, y_test_pred)\n",
    "            \n",
    "            # Append results\n",
    "            results.append({\n",
    "                'n_estimators': n_estimators,\n",
    "                'learning_rate': learning_rate,\n",
    "                'max_depth': max_depth,\n",
    "                'r2_train': r2_train,\n",
    "                'r2_test': r2_test\n",
    "            })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the top 10 configurations sorted by test R\n",
    "print(\"Top 10 configurations by Test R:\")\n",
    "print(results_df.sort_values(by='r2_test', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f0b3c9",
   "metadata": {},
   "source": [
    "## Neural network regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5015a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4563141348491596, 0.4512385718898606, 0.514026941821037, 0.48041994028008883, 0.47340875673610094, 0.48444537487605466, 0.5095554880518873, 0.4278810328150038, 0.49849533037040306, 0.5170664953926447, 0.44302302894416, 0.4359583584057556, 0.458715247101559, 0.512019635492127, 0.43791165126992415, 0.5188474833489467, 0.5193438550653573, 0.42598358512374324, 0.47276298590799104, 0.43169136841252065, 0.4355932130664284, 0.4622165032711264, 0.48194562034611954, 0.48066657197926965, 0.4963967184821759, 0.4995051195227279, 0.4830822544066412, 0.4852486434092461, 0.48203696729494205, 0.4566716744367336, 0.495712481359024, 0.47531175722914487, 0.4664073783313669, 0.508000321501453, 0.4671210387956226, 0.4585484146719736, 0.4824162648842527, 0.4922770310449036, 0.4807545913738773]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "hidden_layer_sizes_range = range(50, 440, 10)\n",
    "\n",
    "mse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for size in hidden_layer_sizes_range:\n",
    "    mlp_regressor = MLPRegressor(hidden_layer_sizes=(size,), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
    "    mlp_regressor.fit(X_train_scaled, Y_train)\n",
    "    Y_pred = mlp_regressor.predict(X_test_scaled)\n",
    "    mse_scores.append(mean_squared_error(Y_test, Y_pred))\n",
    "    r2_scores.append(r2_score(Y_test, Y_pred))\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(hidden_layer_sizes_range, mse_scores, marker='o', linestyle='-', color='red')\n",
    "plt.title('Hidden Layer Size vs MSE')\n",
    "plt.xlabel('Hidden Layer Size')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(hidden_layer_sizes_range, r2_scores, marker='o', linestyle='-', color='blue')\n",
    "plt.title('Hidden Layer Size vs R^2 Score')\n",
    "plt.xlabel('Hidden Layer Size')\n",
    "plt.ylabel('R^2 Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82bcc279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\tpot\\builtins\\__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 217, in _count_physical_cores\n",
      "    raise ValueError(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7769413159434508\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.787147077203104\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7769413159434508\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.787147077203104\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7900797580810028\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=14, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7900797580810028\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=14, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8016713722446168\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7900797580810028\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=14, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8016713722446168\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "training r2 is :  0.8177079730466406\n",
      "testing r2 is :  -0.1353021068148943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "tpot = TPOTRegressor(generations=5, population_size=50, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "50\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_DPP_DTT_pipeline_combined_MYL_1.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0bacc2-94fb-472d-8ed3-aacbc69859df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/550 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7769413159434508\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.787147077203104\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7769413159434508\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.787147077203104\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7900797580810028\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=14, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7900797580810028\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=14, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8016713722446168\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7900797580810028\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=14, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8016713722446168\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7947368530400933\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8016713722446168\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7947368530400933\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8016713722446168\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "training r2 is :  0.8119986949477322\n",
      "testing r2 is :  -0.6965666819727867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "tpot = TPOTRegressor(generations=10, population_size=50, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "50\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_DPP_DTT_pipeline_combined_MYL_2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38b1ca86-bb1b-4025-908d-f6fcbeed5cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/2550 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7769413159434508\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.787147077203104\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7769413159434508\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.787147077203104\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7900797580810028\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=14, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7900797580810028\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=14, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8016713722446168\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7900797580810028\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=14, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8016713722446168\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7947368530400933\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8016713722446168\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7947368530400933\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8016713722446168\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8038258735939978\tGradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8038258735939978\tGradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 76, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8041969646615759\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8041969646615759\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8041969646615759\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8041969646615759\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8041969646615759\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8041969646615759\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8041969646615759\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8041969646615759\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 53, n_samples_fit = 50, n_samples = 50.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 64, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "training r2 is :  0.8113526434636955\n",
      "testing r2 is :  -0.24805799454170407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "tpot = TPOTRegressor(generations=50, population_size=50, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "50\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_DPP_DTT_pipeline_combined_MYL_3.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca4c1391-b7b7-401e-89f3-460021175c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/5050 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7769413159434508\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.787147077203104\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7769413159434508\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.787147077203104\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7900797580810028\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=14, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7900797580810028\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=14, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8016713722446168\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7900797580810028\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=14, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8016713722446168\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7947368530400933\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8016713722446168\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7947368530400933\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8016713722446168\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8038258735939978\tGradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8038258735939978\tGradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8027102941366092\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 76, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8041969646615759\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8041969646615759\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8041969646615759\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8041969646615759\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8041969646615759\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8041969646615759\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8041969646615759\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8041969646615759\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 53, n_samples_fit = 50, n_samples = 50.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 64, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 51 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 83, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 52 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 53 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 54 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 55 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 56 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 57 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 58 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 59 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 60 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 61 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 62 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 63 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 64 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 65 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 66 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 67 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 68 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 69 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 70 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 71 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 72 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 73 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 74 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 75 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 76 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 77 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 78 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 64, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 79 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 80 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 81 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 82 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 83 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 84 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 85 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 86 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 87 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 96, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 88 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 95, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 89 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 90 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 91 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 56, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 96, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 92 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 93 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 94 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 95 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 96 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 97 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 98 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 99 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 100 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8092598315261587\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.8104122200156045\tGradientBoostingRegressor(ZeroCount(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "training r2 is :  0.8113526434636955\n",
      "testing r2 is :  -0.24805799454170407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "tpot = TPOTRegressor(generations=100, population_size=50, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "50\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_DPP_DTT_pipeline_combined_MYL_4.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cd001ab-803d-40d4-b315-8d78eac400f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/600 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7815851459598231\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7884780083266888\tRandomForestRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=False, LinearSVR__epsilon=1.0, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7815851459598231\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7884780083266888\tRandomForestRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=False, LinearSVR__epsilon=1.0, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7815851459598231\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7884780083266888\tRandomForestRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=False, LinearSVR__epsilon=1.0, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:43:43] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "training r2 is :  0.8353172196284316\n",
      "testing r2 is :  0.06956258467380683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "tpot = TPOTRegressor(generations=5, population_size=100, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "50\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_DPP_DTT_pipeline_combined_MYL_5.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2d71c91-fa5a-4b50-850b-301d4720375a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/1100 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7815851459598231\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7884780083266888\tRandomForestRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=False, LinearSVR__epsilon=1.0, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7815851459598231\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7884780083266888\tRandomForestRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=False, LinearSVR__epsilon=1.0, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7815851459598231\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7884780083266888\tRandomForestRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=False, LinearSVR__epsilon=1.0, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:48:31] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "training r2 is :  0.8353172196284316\n",
      "testing r2 is :  0.06956258467380683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "tpot = TPOTRegressor(generations=10, population_size=100, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "50\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_DPP_DTT_pipeline_combined_MYL_6.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1b158b6-dc34-4d6a-9b61-86d399f7af50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/5100 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7815851459598231\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7884780083266888\tRandomForestRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=False, LinearSVR__epsilon=1.0, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7815851459598231\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7884780083266888\tRandomForestRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=False, LinearSVR__epsilon=1.0, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7815851459598231\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7884780083266888\tRandomForestRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=False, LinearSVR__epsilon=1.0, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [18:57:22] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 68, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 97, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 70, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 55, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 69, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:17:33] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:18:05] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:18:32] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:19:23] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:19:26] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:19:27] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 58, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:21:16] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 92, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 91, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 92, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:23:37] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 65, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 63, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:24:11] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "training r2 is :  0.8353172196284316\n",
      "testing r2 is :  0.06956258467380683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "tpot = TPOTRegressor(generations=50, population_size=100, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "50\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_DPP_DTT_pipeline_combined_MYL_7.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d68997d-0186-4701-b5f8-52c230184ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/10100 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7815851459598231\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7884780083266888\tRandomForestRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=False, LinearSVR__epsilon=1.0, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7815851459598231\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7884780083266888\tRandomForestRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=False, LinearSVR__epsilon=1.0, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7815851459598231\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.7500000000000001, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.7884780083266888\tRandomForestRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=False, LinearSVR__epsilon=1.0, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=3, RandomForestRegressor__min_samples_split=17, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:28:53] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 68, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 97, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 70, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 55, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 69, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:49:06] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:49:37] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:50:04] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:50:55] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:50:57] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:50:59] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 58, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:52:47] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 92, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 91, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 92, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:55:05] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 65, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 63, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:55:38] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 51 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:56:41] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 52 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 53 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 89, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 54 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [19:58:15] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 62, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 55 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 56 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 57 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 91, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 58 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 59 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 60 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [20:01:32] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 61 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 62 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 91, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 63 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 64 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 65 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 66 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 67 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 61, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 68 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 69 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [20:07:23] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 70 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 71 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 72 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 91, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 83, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 73 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 74 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 75 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 76 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 77 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 78 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 79 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 80 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 81 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8085568158324389\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8086131895180448\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=6, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-3\t0.8195442031429614\tGradientBoostingRegressor(RobustScaler(RobustScaler(input_matrix)), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 82 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8141622021458467\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-3\t0.8195442031429614\tGradientBoostingRegressor(RobustScaler(RobustScaler(input_matrix)), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 83 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8141622021458467\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8195442031429614\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 84 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8141622021458467\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8195442031429614\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 85 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8141622021458467\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8195442031429614\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 58, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 86 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8141622021458467\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.8195442031429614\tGradientBoostingRegressor(RobustScaler(input_matrix), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 87 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8141622021458467\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.828782522027832\tGradientBoostingRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.045), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 88 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8141622021458467\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.828782522027832\tGradientBoostingRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.045), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 89 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8141622021458467\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.828782522027832\tGradientBoostingRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.045), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 90 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8141622021458467\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.828782522027832\tGradientBoostingRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.045), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 91 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8141622021458467\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.828782522027832\tGradientBoostingRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.045), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 92 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8141622021458467\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.828782522027832\tGradientBoostingRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.045), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 93 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8141622021458467\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.828782522027832\tGradientBoostingRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.045), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-4\t0.8323091369170212\tGradientBoostingRegressor(SelectFwe(ExtraTreesRegressor(RobustScaler(input_matrix), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=19, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), SelectFwe__alpha=0.045), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 94 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8141622021458467\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-2\t0.828782522027832\tGradientBoostingRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.045), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-4\t0.8323091369170212\tGradientBoostingRegressor(SelectFwe(ExtraTreesRegressor(RobustScaler(input_matrix), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=19, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), SelectFwe__alpha=0.045), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 95 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8149281966187623\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001)\n",
      "\n",
      "-2\t0.828782522027832\tGradientBoostingRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.045), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-4\t0.8323091369170212\tGradientBoostingRegressor(SelectFwe(ExtraTreesRegressor(RobustScaler(input_matrix), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=19, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), SelectFwe__alpha=0.045), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 96 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8149281966187623\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001)\n",
      "\n",
      "-2\t0.828782522027832\tGradientBoostingRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.045), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-4\t0.8323091369170212\tGradientBoostingRegressor(SelectFwe(ExtraTreesRegressor(RobustScaler(input_matrix), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=19, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), SelectFwe__alpha=0.045), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 97 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8149281966187623\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001)\n",
      "\n",
      "-2\t0.828782522027832\tGradientBoostingRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.045), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-3\t0.8344084427732501\tGradientBoostingRegressor(ZeroCount(SelectFwe(input_matrix, SelectFwe__alpha=0.045)), GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 98 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8149281966187623\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001)\n",
      "\n",
      "-2\t0.828782522027832\tGradientBoostingRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.045), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-3\t0.8344084427732501\tGradientBoostingRegressor(ZeroCount(SelectFwe(input_matrix, SelectFwe__alpha=0.045)), GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 99 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8149281966187623\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001)\n",
      "\n",
      "-2\t0.830735578982778\tGradientBoostingRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.045), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-3\t0.8344084427732501\tGradientBoostingRegressor(ZeroCount(SelectFwe(input_matrix, SelectFwe__alpha=0.045)), GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 100 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8170893095210404\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.8500000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9500000000000001)\n",
      "\n",
      "-2\t0.830735578982778\tGradientBoostingRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.045), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "\n",
      "-3\t0.8344084427732501\tGradientBoostingRegressor(ZeroCount(SelectFwe(input_matrix, SelectFwe__alpha=0.045)), GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.7500000000000001)\n",
      "training r2 is :  0.8052333870297373\n",
      "testing r2 is :  -0.16287664213454423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "tpot = TPOTRegressor(generations=100, population_size=100, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "50\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_DPP_DTT_pipeline_combined_MYL_8.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "960addb2-b220-4ea5-be0d-dfc96be40dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/1200 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 84, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7904550298806474\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7904550298806474\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 82, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [21:08:36] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7928136611268377\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 86, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7928136611268377\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7928136611268377\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "training r2 is :  0.8217235507444598\n",
      "testing r2 is :  0.37544683378716615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "tpot = TPOTRegressor(generations=5, population_size=200, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "50\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_DPP_DTT_pipeline_combined_MYL_9.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e59689d8-c6bd-4802-8bb7-6f035bd0b9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/2200 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 84, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7904550298806474\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7904550298806474\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 82, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [21:17:26] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7928136611268377\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 86, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7928136611268377\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7928136611268377\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7931901467920942\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7937169053993018\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 52, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 71, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "training r2 is :  0.8256146420845443\n",
      "testing r2 is :  0.0385538443822806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "tpot = TPOTRegressor(generations=10, population_size=200, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "50\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_DPP_DTT_pipeline_combined_MYL_10.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd185b47-80e6-48d2-9311-2799bc5e5fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/10200 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 84, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7904550298806474\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7904550298806474\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 82, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [21:34:40] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7928136611268377\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 86, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7928136611268377\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7928136611268377\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7931901467920942\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7937169053993018\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 52, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 71, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 62, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8130984089847242\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8130984089847242\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8130984089847242\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [22:03:02] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8130984089847242\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8143349681672349\tXGBRegressor(MinMaxScaler(SelectFwe(input_matrix, SelectFwe__alpha=0.009000000000000001)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8130984089847242\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8143349681672349\tXGBRegressor(MinMaxScaler(SelectFwe(input_matrix, SelectFwe__alpha=0.009000000000000001)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8130984089847242\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8143349681672349\tXGBRegressor(MinMaxScaler(SelectFwe(input_matrix, SelectFwe__alpha=0.009000000000000001)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8143349681672349\tXGBRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.009000000000000001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [22:08:05] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 75, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8243861716068931\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.9500000000000001, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 [22:09:14] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [22:09:20] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8243861716068931\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.9500000000000001, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [22:10:24] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 94, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [22:10:34] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8248747089361086\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8248747089361086\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [22:13:23] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8248747089361086\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8249939920725845\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8249939920725845\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [22:17:15] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 76, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8312510486490179\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 73, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8312510486490179\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [22:19:56] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8312510486490179\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8323581587336639\tXGBRegressor(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8312510486490179\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8323581587336639\tXGBRegressor(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8312510486490179\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8323581587336639\tXGBRegressor(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8312510486490179\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8323581587336639\tXGBRegressor(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 69, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [22:34:24] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [22:34:39] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 97, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [22:39:01] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [22:41:30] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [22:43:14] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [22:45:29] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [22:54:38] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "training r2 is :  0.8318322621300552\n",
      "testing r2 is :  -0.06572137406273781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "tpot = TPOTRegressor(generations=50, population_size=200, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "50\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_DPP_DTT_pipeline_combined_MYL_11.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbada7a9-8b56-4b40-befe-779da6e7fe3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/20200 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 84, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7904550298806474\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7904550298806474\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 82, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:00:54] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7928136611268377\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 86, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7928136611268377\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7928136611268377\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7931901467920942\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7937169053993018\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7500000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 52, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 71, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 62, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8130984089847242\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8130984089847242\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8130984089847242\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:29:44] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8130984089847242\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8143349681672349\tXGBRegressor(MinMaxScaler(SelectFwe(input_matrix, SelectFwe__alpha=0.009000000000000001)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8130984089847242\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8143349681672349\tXGBRegressor(MinMaxScaler(SelectFwe(input_matrix, SelectFwe__alpha=0.009000000000000001)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8130984089847242\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8143349681672349\tXGBRegressor(MinMaxScaler(SelectFwe(input_matrix, SelectFwe__alpha=0.009000000000000001)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8143349681672349\tXGBRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.009000000000000001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:34:50] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 75, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8243861716068931\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.9500000000000001, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 [23:36:00] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:36:05] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8243861716068931\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.9500000000000001, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:37:10] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 94, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:37:19] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8248747089361086\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8248747089361086\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:40:05] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8248747089361086\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8249939920725845\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8249939920725845\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [23:43:48] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 76, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8312510486490179\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 73, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8312510486490179\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [23:46:25] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8312510486490179\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8323581587336639\tXGBRegressor(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8312510486490179\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8323581587336639\tXGBRegressor(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8312510486490179\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8323581587336639\tXGBRegressor(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8312510486490179\tXGBRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8323581587336639\tXGBRegressor(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 69, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:00:21] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:00:37] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 97, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:04:50] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:07:17] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:08:56] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:11:05] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:19:55] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 88, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 51 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.833346861880278\tXGBRegressor(CombineDFs(SelectPercentile(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 52 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.833346861880278\tXGBRegressor(CombineDFs(SelectPercentile(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 53 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.833346861880278\tXGBRegressor(CombineDFs(SelectPercentile(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:26:40] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 65, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 54 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.833346861880278\tXGBRegressor(CombineDFs(SelectPercentile(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 55 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.833346861880278\tXGBRegressor(CombineDFs(SelectPercentile(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 56 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.833346861880278\tXGBRegressor(CombineDFs(SelectPercentile(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:32:56] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:33:13] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 57 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.833346861880278\tXGBRegressor(CombineDFs(SelectPercentile(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 58 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.833346861880278\tXGBRegressor(CombineDFs(SelectPercentile(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 59 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.833346861880278\tXGBRegressor(CombineDFs(SelectPercentile(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 60 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.833346861880278\tXGBRegressor(CombineDFs(SelectPercentile(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 100, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 61, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 61 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.833346861880278\tXGBRegressor(CombineDFs(SelectPercentile(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "\n",
      "Generation 62 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.833346861880278\tXGBRegressor(CombineDFs(SelectPercentile(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:45:29] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:45:36] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 63 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.833346861880278\tXGBRegressor(CombineDFs(SelectPercentile(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 71, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 64 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.833346861880278\tXGBRegressor(CombineDFs(SelectPercentile(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 65 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.833346861880278\tXGBRegressor(CombineDFs(SelectPercentile(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [00:51:14] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 66 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.833346861880278\tXGBRegressor(CombineDFs(SelectPercentile(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 83, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:53:21] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "\n",
      "Generation 67 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.833346861880278\tXGBRegressor(CombineDFs(SelectPercentile(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [00:55:09] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 68 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 87, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 58, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 69 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 70 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:01:32] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 71 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 70, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:03:53] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 72 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 73 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 61, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 74 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 75 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:13:52] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 90, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 76 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:16:41] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 77 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:19:49] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 78 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:22:16] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 58, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 79 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:25:29] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 80 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 69, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:28:39] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 81 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 82 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 76, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [01:33:55] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 83 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 80, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 84 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:40:11] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 85 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 86 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:45:25] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 87 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 88 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 89 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:53:43] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 63, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 90 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 61, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 75, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 91 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [01:58:55] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 92 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [02:01:25] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 76, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "\n",
      "Generation 93 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:04:13] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [02:04:17] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 94 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 76, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:07:02] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 95 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 96 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 97 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:14:40] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 76, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [02:15:11] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 98 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 99 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=6 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 100 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8080430766816141\tGradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)\n",
      "\n",
      "-2\t0.8323581587336639\tXGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8368584914775237\tXGBRegressor(CombineDFs(SelectPercentile(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), SelectPercentile__percentile=68), input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "training r2 is :  0.8318322621300552\n",
      "testing r2 is :  -0.06572137406273781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "tpot = TPOTRegressor(generations=100, population_size=200, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "50\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_DPP_DTT_pipeline_combined_MYL_12.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8bca549-ad59-497f-9241-b44c2c60ac73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/2400 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7904550298806474\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.8027829875948325\tKNeighborsRegressor(SelectPercentile(input_matrix, SelectPercentile__percentile=10), KNeighborsRegressor__n_neighbors=49, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7904550298806474\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.8027829875948325\tKNeighborsRegressor(SelectPercentile(input_matrix, SelectPercentile__percentile=10), KNeighborsRegressor__n_neighbors=49, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 75, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7928136611268377\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.8027829875948325\tKNeighborsRegressor(SelectPercentile(input_matrix, SelectPercentile__percentile=10), KNeighborsRegressor__n_neighbors=49, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 78, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7928136611268377\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.8027829875948325\tKNeighborsRegressor(SelectPercentile(input_matrix, SelectPercentile__percentile=10), KNeighborsRegressor__n_neighbors=49, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 71, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "training r2 is :  0.8353172196284316\n",
      "testing r2 is :  0.06956258467380683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "tpot = TPOTRegressor(generations=5, population_size=400, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "50\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_DPP_DTT_pipeline_combined_MYL_13.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93ee9699-0a75-42af-a61d-346869d5a2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/4400 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7904550298806474\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.8027829875948325\tKNeighborsRegressor(SelectPercentile(input_matrix, SelectPercentile__percentile=10), KNeighborsRegressor__n_neighbors=49, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7904550298806474\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.8027829875948325\tKNeighborsRegressor(SelectPercentile(input_matrix, SelectPercentile__percentile=10), KNeighborsRegressor__n_neighbors=49, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 75, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7928136611268377\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.8027829875948325\tKNeighborsRegressor(SelectPercentile(input_matrix, SelectPercentile__percentile=10), KNeighborsRegressor__n_neighbors=49, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 78, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7928136611268377\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.8027829875948325\tKNeighborsRegressor(SelectPercentile(input_matrix, SelectPercentile__percentile=10), KNeighborsRegressor__n_neighbors=49, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 71, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [03:00:40] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 59, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 71, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 88, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8122382145920259\tXGBRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.018000000000000002), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8122382145920259\tXGBRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.018000000000000002), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "training r2 is :  0.8096736442342823\n",
      "testing r2 is :  0.0875372768651339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "tpot = TPOTRegressor(generations=10, population_size=400, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "50\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_DPP_DTT_pipeline_combined_MYL_14.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5355b83d-aeb5-457a-8e0d-572eb35a5dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/20400 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7904550298806474\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.8027829875948325\tKNeighborsRegressor(SelectPercentile(input_matrix, SelectPercentile__percentile=10), KNeighborsRegressor__n_neighbors=49, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7904550298806474\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.8027829875948325\tKNeighborsRegressor(SelectPercentile(input_matrix, SelectPercentile__percentile=10), KNeighborsRegressor__n_neighbors=49, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 75, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7928136611268377\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.8027829875948325\tKNeighborsRegressor(SelectPercentile(input_matrix, SelectPercentile__percentile=10), KNeighborsRegressor__n_neighbors=49, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 78, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7928136611268377\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.8027829875948325\tKNeighborsRegressor(SelectPercentile(input_matrix, SelectPercentile__percentile=10), KNeighborsRegressor__n_neighbors=49, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 71, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [03:32:57] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 59, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 71, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 88, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8122382145920259\tXGBRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.018000000000000002), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8122382145920259\tXGBRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.018000000000000002), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8122382145920259\tXGBRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.018000000000000002), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 98, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.821563051810444\tXGBRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.011), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.821563051810444\tXGBRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.011), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 94, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.821563051810444\tXGBRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.011), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.821563051810444\tXGBRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.011), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:03:01] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [04:03:01] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 85, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.821563051810444\tXGBRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.011), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8270350064511014\tXGBRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:09:21] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [04:09:33] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 [04:09:33] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8270350064511014\tXGBRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [04:11:48] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 63, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:12:01] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8270350064511014\tXGBRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:16:58] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:17:01] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:19:15] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 75, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:19:24] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:19:28] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 56, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:21:45] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:24:04] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:24:20] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:26:23] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:26:26] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 58, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:28:42] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 66, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 94, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:34:23] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 90, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 96, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _mate_operator: num_test=0 [04:40:31] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 89, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:44:17] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8391255251457312\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:50:44] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:50:51] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 82, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [04:51:23] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 96, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [05:14:57] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 71, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 illegal value in 4th argument of internal gesdd.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [05:29:52] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 illegal value in 4th argument of internal gesdd.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [05:39:22] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 81, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 82, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 97, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.842526311085862\tXGBRegressor(ZeroCount(GradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.842526311085862\tXGBRegressor(ZeroCount(GradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 64, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 85, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 98, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 58, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.842526311085862\tXGBRegressor(ZeroCount(GradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "training r2 is :  0.8291537062975324\n",
      "testing r2 is :  -0.12546975589547293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "tpot = TPOTRegressor(generations=50, population_size=400, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "50\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_DPP_DTT_pipeline_combined_MYL_15.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0182c64-7f6d-4bd6-b067-e4225e353cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/40400 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7904550298806474\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.8027829875948325\tKNeighborsRegressor(SelectPercentile(input_matrix, SelectPercentile__percentile=10), KNeighborsRegressor__n_neighbors=49, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7904550298806474\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.8027829875948325\tKNeighborsRegressor(SelectPercentile(input_matrix, SelectPercentile__percentile=10), KNeighborsRegressor__n_neighbors=49, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 75, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7928136611268377\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.8027829875948325\tKNeighborsRegressor(SelectPercentile(input_matrix, SelectPercentile__percentile=10), KNeighborsRegressor__n_neighbors=49, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 78, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7928136611268377\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.8027829875948325\tKNeighborsRegressor(SelectPercentile(input_matrix, SelectPercentile__percentile=10), KNeighborsRegressor__n_neighbors=49, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 71, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [06:45:22] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 59, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 71, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8031018897023069\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 88, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8122382145920259\tXGBRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.018000000000000002), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8122382145920259\tXGBRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.018000000000000002), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8122382145920259\tXGBRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.018000000000000002), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 98, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.821563051810444\tXGBRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.011), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.821563051810444\tXGBRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.011), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 94, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.821563051810444\tXGBRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.011), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.821563051810444\tXGBRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.011), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [07:16:21] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [07:16:21] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 85, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.821563051810444\tXGBRegressor(SelectFwe(input_matrix, SelectFwe__alpha=0.011), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8270350064511014\tXGBRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 51, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [07:22:51] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [07:23:03] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 [07:23:03] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8270350064511014\tXGBRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [07:25:26] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 63, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [07:25:39] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8270350064511014\tXGBRegressor(AdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [07:30:49] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [07:30:51] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8074819452865837\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [07:33:12] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 75, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [07:33:21] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [07:33:26] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 56, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [07:35:51] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [07:38:16] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [07:38:32] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [07:40:40] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 67, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [07:40:43] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 58, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [07:43:03] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 66, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 94, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [07:48:57] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 90, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 96, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by _RidgeGCV..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _mate_operator: num_test=0 [07:55:21] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8277766069130006\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 89, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [07:59:11] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8391255251457312\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.7000000000000001, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [08:05:52] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [08:05:58] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 82, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [08:06:33] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 96, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [08:30:48] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 71, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 illegal value in 4th argument of internal gesdd.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [08:45:59] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 illegal value in 4th argument of internal gesdd.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [08:55:39] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 81, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 82, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 97, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.842526311085862\tXGBRegressor(ZeroCount(GradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.842526311085862\tXGBRegressor(ZeroCount(GradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 64, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 85, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 98, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 58, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.842526311085862\tXGBRegressor(ZeroCount(GradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 51 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.842526311085862\tXGBRegressor(ZeroCount(GradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 52 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.842526311085862\tXGBRegressor(ZeroCount(GradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 92, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 84, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 53 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.842526311085862\tXGBRegressor(ZeroCount(GradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [09:57:43] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 81, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 54 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.842526311085862\tXGBRegressor(ZeroCount(GradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 71, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 55 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.842526311085862\tXGBRegressor(ZeroCount(GradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [10:09:40] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [10:10:19] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 56 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.842526311085862\tXGBRegressor(ZeroCount(GradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [10:15:40] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 57 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.842526311085862\tXGBRegressor(ZeroCount(GradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 75, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 96, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 58 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.842526311085862\tXGBRegressor(ZeroCount(GradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 96, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 89, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 81, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 59 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.842526311085862\tXGBRegressor(ZeroCount(GradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 60 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.842526311085862\tXGBRegressor(ZeroCount(GradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 53, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 78, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 61 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.842526311085862\tXGBRegressor(ZeroCount(GradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [10:45:26] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 62 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.842526311085862\tXGBRegressor(ZeroCount(GradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 illegal value in 4th argument of internal gesdd.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 63 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8424440145415278\tXGBRegressor(ZeroCount(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-4\t0.842526311085862\tXGBRegressor(ZeroCount(GradientBoostingRegressor(MaxAbsScaler(input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=8, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [10:57:25] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 64 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8487965587419781\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [11:04:11] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 65 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8487965587419781\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 58, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 66 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8487965587419781\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [11:15:01] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [11:15:08] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 70, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 67 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8487965587419781\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 68 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8487965587419781\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [11:27:40] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [11:27:41] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [11:27:58] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 69 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8487965587419781\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 70, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 70 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8487965587419781\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [11:38:56] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 87, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [11:39:28] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [11:40:02] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 71 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8487965587419781\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [11:45:52] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [11:46:05] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 69, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 72 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8487965587419781\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [11:51:35] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [11:51:37] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 91, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [11:52:19] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 73 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8487965587419781\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Expected n_neighbors <= n_samples_fit, but n_neighbors = 56, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 74 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8487965587419781\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 77, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 75 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 76 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 84, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 54, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [12:16:26] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 77 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 78 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [12:27:55] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 60, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [12:29:02] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 79 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [12:34:55] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 82, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 80 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 81 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 [12:46:26] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 93, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [12:47:45] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 82 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [12:53:07] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 83 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [12:59:05] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [12:59:07] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [12:59:30] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 84 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 72, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 85 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 illegal value in 4th argument of internal gesdd.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [13:11:21] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [13:11:42] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 86 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 95, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 63, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 87 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [13:24:16] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 88 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [13:30:05] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 80, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 89 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 illegal value in 4th argument of internal gesdd.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 90 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 79, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [13:42:08] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 98, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 91 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 92 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [13:54:45] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 78, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 62, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 93 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [14:02:25] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 57, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [14:02:29] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 61, n_samples_fit = 50, n_samples = 50.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 94 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [14:08:22] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 66, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 95 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [14:15:11] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 89, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [14:15:42] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [14:15:45] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [14:15:57] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 96 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 [14:21:12] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 Expected n_neighbors <= n_samples_fit, but n_neighbors = 73, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [14:21:18] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 99, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [14:21:52] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples_fit, but n_neighbors = 96, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [14:22:34] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 97 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [14:28:37] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [14:28:42] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 98 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 [14:33:54] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples_fit, but n_neighbors = 98, n_samples_fit = 50, n_samples = 50.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 FeatureAgglomeration.__init__() got an unexpected keyword argument 'affinity'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 99 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [14:40:38] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\iterative_dmatrix.cc:202: Check failed: n_features >= 1 (0 vs. 1) : Data must has at least 1 column..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=6 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 LassoLarsCV.__init__() got an unexpected keyword argument 'normalize'.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'ls' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'absolute_error', 'quantile', 'squared_error'}. Got 'lad' instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'huber', 'squared_epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 100 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.815371713900195\tXGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3, XGBRegressor__verbosity=0)\n",
      "\n",
      "-2\t0.8394368268566845\tXGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "\n",
      "-3\t0.8505269540866391\tXGBRegressor(DecisionTreeRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45), DecisionTreeRegressor__max_depth=5, DecisionTreeRegressor__min_samples_leaf=10, DecisionTreeRegressor__min_samples_split=8), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25, XGBRegressor__verbosity=0)\n",
      "training r2 is :  0.8272943532873012\n",
      "testing r2 is :  0.0898333175026731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "tpot = TPOTRegressor(generations=100, population_size=400, verbosity=3, scoring='r2', random_state=42)\n",
    "#meaning of parameters = #http://epistasislab.github.io/tpot/api/\n",
    "\n",
    "tpot.fit(X_train, Y_train)\n",
    "50\n",
    "print('training r2 is : ', tpot.score(X_train, Y_train))\n",
    "print('testing r2 is : ',tpot.score(X_test, Y_test))\n",
    "\n",
    "tpot.export('tpot_DPP_DTT_pipeline_combined_MYL_16.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88ccef23-b493-484b-bc6f-314d7e4573ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\Myeongyeon Lee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Define a range of `n_estimators` to evaluate\n",
    "n_estimators_range = np.arange(1, 101, 1)\n",
    "\n",
    "# Initialize a list to store the R2 scores for each model\n",
    "r2_scores = []\n",
    "\n",
    "# Evaluate Extra Trees model with different `n_estimators`\n",
    "for n_estimators in n_estimators_range:\n",
    "    model = ExtraTreesRegressor(n_estimators=n_estimators, random_state=42)\n",
    "    model.fit(X_train, Y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    r2 = r2_score(Y_test, predictions)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "# Plotting n_estimators vs R2\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_estimators_range, r2_scores, marker='o', linestyle='-')\n",
    "plt.title('Effect of n_estimators on R2 Score')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('R2 Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b20f3eff-ab2c-4803-94ad-1bf6c2490014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x27363bc9b50>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a=DPP_DTT_df[['solvent_boiling_point']]\n",
    "plt.plot(a,DPP_DTT_Y, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b0733e-1635-4320-8572-7db7fab67da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
